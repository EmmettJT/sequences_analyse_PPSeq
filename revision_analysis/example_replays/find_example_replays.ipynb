{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830fe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3f71a6",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56a4ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import seaborn as sns;\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_H5_bodypart(tracking_path,video_type, tracking_point):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'task' in file:\n",
    "                back_file = pd.read_hdf(tracking_path + file)     \n",
    "                \n",
    "    # drag data out of the df\n",
    "    scorer = back_file.columns.tolist()[0][0]\n",
    "    body_part = back_file[scorer][tracking_point]\n",
    "    \n",
    "    parts=[]\n",
    "    for item in list(back_file[scorer]):\n",
    "        parts+=[item[0]]\n",
    "    print(np.unique(parts))\n",
    "    \n",
    "    # clean and interpolate frames with less than 98% confidence\n",
    "    clean_and_interpolate(body_part,0.98)\n",
    "    \n",
    "    return(body_part)\n",
    "  \n",
    "def load_H5_ports(tracking_path,video_type):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'port' in file or 'PORT' in file:\n",
    "                back_ports_file = pd.read_hdf(tracking_path + file)\n",
    "\n",
    "    ## same for the ports:\n",
    "    scorer = back_ports_file.columns.tolist()[0][0]\n",
    "        \n",
    "    if video_type == 'back':\n",
    "        port1 =back_ports_file[scorer]['port2']\n",
    "        port2 =back_ports_file[scorer]['port1']\n",
    "        port3 =back_ports_file[scorer]['port6']\n",
    "        port4 =back_ports_file[scorer]['port3']\n",
    "        port5 =back_ports_file[scorer]['port7']\n",
    "    else:\n",
    "        port1 =back_ports_file[scorer]['Port2']\n",
    "        port2 =back_ports_file[scorer]['Port1']\n",
    "        port3 =back_ports_file[scorer]['Port6']\n",
    "        port4 =back_ports_file[scorer]['Port3']\n",
    "        port5 =back_ports_file[scorer]['Port7']\n",
    "\n",
    "    clean_and_interpolate(port1,0.98)\n",
    "    clean_and_interpolate(port2,0.98)\n",
    "    clean_and_interpolate(port3,0.98)\n",
    "    clean_and_interpolate(port4,0.98)\n",
    "    clean_and_interpolate(port5,0.98)\n",
    "    \n",
    "    return(port1,port2,port3,port4,port5)\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "def clean_and_interpolate(head_centre,threshold):\n",
    "    bad_confidence_inds = np.where(head_centre.likelihood.values<threshold)[0]\n",
    "    newx = head_centre.x.values\n",
    "    newx[bad_confidence_inds] = 0\n",
    "    newy = head_centre.y.values\n",
    "    newy[bad_confidence_inds] = 0\n",
    "\n",
    "    start_value_cleanup(newx)\n",
    "    interped_x = interp_0_coords(newx)\n",
    "\n",
    "    start_value_cleanup(newy)\n",
    "    interped_y = interp_0_coords(newy)\n",
    "    \n",
    "    head_centre['interped_x'] = interped_x\n",
    "    head_centre['interped_y'] = interped_y\n",
    "    \n",
    "def start_value_cleanup(coords):\n",
    "    # This is for when the starting value of the coords == 0; interpolation will not work on these coords until the first 0 \n",
    "    #is changed. The 0 value is changed to the first non-zero value in the coords lists\n",
    "    for index, value in enumerate(coords):\n",
    "        working = 0\n",
    "        if value > 0:\n",
    "            start_value = value\n",
    "            start_index = index\n",
    "            working = 1\n",
    "            break\n",
    "    if working == 1:\n",
    "        for x in range(start_index):\n",
    "            coords[x] = start_value\n",
    "            \n",
    "def interp_0_coords(coords_list):\n",
    "    #coords_list is one if the outputs of the get_x_y_data = a list of co-ordinate points\n",
    "    for index, value in enumerate(coords_list):\n",
    "        if value == 0:\n",
    "            if coords_list[index-1] > 0:\n",
    "                value_before = coords_list[index-1]\n",
    "                interp_start_index = index-1\n",
    "                #print('interp_start_index: ', interp_start_index)\n",
    "                #print('interp_start_value: ', value_before)\n",
    "                #print('')\n",
    "\n",
    "        if index < len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                if coords_list[index+1] > 0:\n",
    "                    interp_end_index = index+1\n",
    "                    value_after = coords_list[index+1]\n",
    "                    #print('interp_end_index: ', interp_end_index)\n",
    "                    #print('interp_end_value: ', value_after)\n",
    "                    #print('')\n",
    "\n",
    "                    #now code to interpolate over the values\n",
    "                    try:\n",
    "                        interp_diff_index = interp_end_index - interp_start_index\n",
    "                    except UnboundLocalError:\n",
    "#                         print('the first value in list is 0, use the function start_value_cleanup to fix')\n",
    "                        break\n",
    "                    #print('interp_diff_index is:', interp_diff_index)\n",
    "\n",
    "                    new_values = np.linspace(value_before, value_after, interp_diff_index)\n",
    "                    #print(new_values)\n",
    "\n",
    "                    interp_index = interp_start_index+1\n",
    "                    for x in range(interp_diff_index):\n",
    "                        #print('interp_index is:', interp_index)\n",
    "                        #print('new_value should be:', new_values[x])\n",
    "                        coords_list[interp_index] = new_values[x]\n",
    "                        interp_index +=1\n",
    "        if index == len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                for x in range(30):\n",
    "                    coords_list[index-x] = coords_list[index-30]\n",
    "                    #print('')\n",
    "#     print('function exiting')\n",
    "    return(coords_list)\n",
    "\n",
    "\n",
    "# Function to find corresponding number in another column\n",
    "def find_corresponding(nums,df_dict):\n",
    "    return [df_dict[num] for num in nums]\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def load_in_paths(pp_file, PP_PATH, DAT_PATH,run_index,ignore_list):\n",
    "    mir = '_'.join(pp_file.split('_')[0:3])\n",
    "    print(str(run_index+1) + '/' + str(len(os.listdir(PP_PATH))-1) + '-------------------------------------------------------------------------')\n",
    "    print(pp_file)\n",
    "    mouse_session_recording = pp_file.split('_')[0] + '_' + pp_file.split('_')[1] + '_' + pp_file.split('_')[2] \n",
    "    skip = False\n",
    "    for item in ignore_list:\n",
    "        if item == mouse_session_recording:\n",
    "            skip = True\n",
    "\n",
    "    save_path = PP_PATH + pp_file + '\\\\_final_analysis_output\\\\'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    ## set dat_path:\n",
    "    for file_ in os.listdir(DAT_PATH):\n",
    "        if mouse_session_recording.split('_')[0] in file_:\n",
    "            if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "                dat_path = os.path.join(DAT_PATH,file_)\n",
    "    for recording in os.listdir(os.path.join(DAT_PATH,dat_path)):\n",
    "        if recording.split('_')[0][9::] == mouse_session_recording.split('_')[-1]:\n",
    "            dat_path = os.path.join(dat_path,recording)\n",
    "\n",
    "    # set tracking path\n",
    "    for file_ in os.listdir(dat_path + r\"\\video\\tracking\\\\\"):\n",
    "        if 'EJT' in mir:\n",
    "            if 'task' in file_:\n",
    "                if not 'clock' in file_:\n",
    "                    tracking_path = os.path.join(dat_path + r\"\\video\\tracking\\\\\",file_) + '\\\\'\n",
    "        else:\n",
    "            tracking_path = dat_path + r\"\\video\\tracking\\\\\"\n",
    "            \n",
    "\n",
    "    return mir,mouse_session_recording,save_path,tracking_path,dat_path\n",
    "\n",
    "def load_PPSEQ_data(PP_PATH,pp_file,dat_path,mouse_session_recording):\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "    ## LOAD \n",
    "    print(\"LOADING PPSEQ DATA\")\n",
    "    print('\\n')\n",
    "    #The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\n",
    "    assignment_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\assigment_hist_frame.csv\")\n",
    "\n",
    "    # latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\n",
    "    latent_event_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\latent_event_hist.csv\")\n",
    "\n",
    "    # seq_type_log_proportions: log p of each type of sequence at each iteration\n",
    "    seq_type_log_proportions_df = pd.read_csv(PP_PATH + pp_file + r\"\\seq_type_log_proportions.csv\")\n",
    "\n",
    "    # neuron_responses.csv: iterations x neurons by 3(number of sequences). Each neuron has three parameters per sequence to describe how it is influenced by each sequence type. \n",
    "    # Each iteration these are resampled, therefore there are number of neurons by iterations by 3 by number of sequences of these numbers.\n",
    "    neuron_response_df = pd.read_csv(PP_PATH + pp_file + r\"\\neuron_response.csv\")\n",
    "\n",
    "    masking = False\n",
    "    for dat_files in os.listdir(PP_PATH + pp_file):\n",
    "        if 'unmasked_spikes' in dat_files:\n",
    "            masking = True\n",
    "            print('masking was used')\n",
    "\n",
    "    if masking == True:\n",
    "        #log_p_hist.csv: the history of the log_p of the model\n",
    "        log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\test_log_p_hist.csv\")\n",
    "\n",
    "        unmasked_spikes_df = pd.read_csv(PP_PATH + pp_file + r\"\\unmasked_spikes.csv\")\n",
    "    else:\n",
    "        log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\log_p_hist.csv\")\n",
    "\n",
    "        spikes_file = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + mouse_session_recording + '.txt'\n",
    "        neuron_ids, spike_times= [], []\n",
    "        with open(spikes_file) as f:\n",
    "            for (i, line) in enumerate(f.readlines()):\n",
    "                neuron_id, spike_time = line.split('\\t')\n",
    "                spike_time = float(spike_time.strip())\n",
    "                neuron_id = float(neuron_id)\n",
    "                spike_times.append(spike_time)\n",
    "                neuron_ids.append(neuron_id)\n",
    "        unmasked_spikes_df = pd.DataFrame({'neuron':neuron_ids,'timestamp':spike_times}) \n",
    "        bkgd_log_proportions_array = pd.read_csv(PP_PATH + pp_file + r\"\\bkgd_log_proportions_array.csv\")\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open(PP_PATH + pp_file + r'\\config_file.json')\n",
    "    # returns JSON object as a dictionary\n",
    "    config = eval(json.load(f))\n",
    "    print(f'      done')\n",
    "\n",
    "    ## LOAD behaviour data\n",
    "    print('\\n')\n",
    "    print(\"LOADING BEHAV DATA\")\n",
    "\n",
    "    ## load in the timespan used for pppseq:\n",
    "    input_params_path = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + ('params_' + mouse_session_recording +'.json')\n",
    "    # Opening JSON file\n",
    "    f = open(input_params_path)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    input_config = json.load(f)\n",
    "    behav_time_interval_start = input_config['time_span']\n",
    "    print(f\"      A corresponding time span has been found. Time span set to {behav_time_interval_start}\")\n",
    "\n",
    "    ### load in data:\n",
    "    for sub_file in os.listdir(dat_path + '\\\\behav_sync\\\\'):\n",
    "        if 'task' in sub_file:\n",
    "            behav_sync_path = dat_path + '\\\\behav_sync\\\\' + sub_file +'\\\\'\n",
    "    behav_sync = pd.read_csv(behav_sync_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "    transitions = pd.read_csv(behav_sync_path + 'Transition_data_sync.csv')\n",
    "\n",
    "    return assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start\n",
    "\n",
    "\n",
    "\n",
    "def plot_save_log_l_curve(log_p_hist_df,save_path):\n",
    "    # find 95% of growth value and when it crossed this\n",
    "    max_ = max(log_p_hist_df.x1)\n",
    "    min_ = min(log_p_hist_df.x1)\n",
    "    growth = max_ - min_\n",
    "    _prcntile =  max_ - (0.02 * growth)\n",
    "\n",
    "    ## model log likley hood curve\n",
    "    plt.plot(log_p_hist_df.x1)\n",
    "    plt.axhline(y=_prcntile, color='r', linestyle='--')\n",
    "\n",
    "    SaveFig('log_l_curve.png',save_path)\n",
    "    \n",
    "def plot_data_raster(behav_time_interval_start, spikes_df, neuron_index, colors, save_path):\n",
    "    # calculate interval timings and end points\n",
    "    interval_lengths = []\n",
    "    for interval in behav_time_interval_start:\n",
    "        interval_lengths += [np.diff(interval)[0]]\n",
    "    total_time = sum(interval_lengths)\n",
    "    interval_end_points = np.cumsum(interval_lengths)\n",
    "\n",
    "    # Plot sequences - basic\n",
    "    timeframe = [0, total_time]\n",
    "    mask = (spikes_df.timestamp > timeframe[0]) * (spikes_df.timestamp < timeframe[-1])\n",
    "\n",
    "    # Define neuron order\n",
    "    neuron_permute_loc = np.zeros(len(neuron_index))\n",
    "    for i in range(len(neuron_index)):\n",
    "        neuron_permute_loc[i] = int(list(neuron_index).index(i))\n",
    "    neuron_order = neuron_permute_loc[(spikes_df.neuron - 1).astype(int)]\n",
    "\n",
    "    # Plotting\n",
    "    fig, [ax, ax2] = plt.subplots(2, 1, figsize=(20, 20))\n",
    "\n",
    "    # Plot background in grey\n",
    "    background_keep_mask = (spikes_df[mask].sequence_type_adjusted < 0) | (spikes_df[mask].sequence_type_adjusted >= 7.0)\n",
    "    ax.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],\n",
    "               marker='o', s=40, linewidth=0, color='lightgrey', alpha=0.3)\n",
    "    c_ = np.array(colors)[spikes_df[mask][background_keep_mask].sequence_type_adjusted.values.astype(int)]\n",
    "    ax2.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],\n",
    "                marker='o', s=40, linewidth=0, color=c_, alpha=0.3)\n",
    "    ax2.set_title('extra sequences and background only')\n",
    "\n",
    "    # Plot spikes without background\n",
    "    background_remove_mask = (spikes_df[mask].sequence_type_adjusted >= 0) * \\\n",
    "                             (spikes_df[mask].sequence_type_adjusted != 7.0) * \\\n",
    "                             (spikes_df[mask].sequence_type_adjusted != 8.0)\n",
    "    c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "    ax.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],\n",
    "               marker='o', s=40, linewidth=0, color=c_, alpha=1)\n",
    "    ax.set_title('held sequences in color and extra sequences + background in grey')\n",
    "\n",
    "    for end_p in interval_end_points:\n",
    "        ax.axvline(x=end_p, color='k')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    return interval_end_points,neuron_order\n",
    "\n",
    "\n",
    "def conactinate_nth_items(startlist):\n",
    "    concatinated_column_vectors = []\n",
    "    for c in range(len(max(startlist, key=len))):\n",
    "        column = []\n",
    "        for t in range(len(startlist)):\n",
    "            if c <= len(startlist[t])-1:\n",
    "                column = column + [startlist[t][c]]\n",
    "        concatinated_column_vectors.append(column)\n",
    "    return concatinated_column_vectors\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def split_list(numbers):\n",
    "    chunks = []\n",
    "    indices = []\n",
    "    current_chunk = []\n",
    "    current_indices = []\n",
    "    for i, num in enumerate(numbers):\n",
    "        if num == 0:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                indices.append(current_indices)\n",
    "                current_chunk = []\n",
    "                current_indices = []\n",
    "        else:\n",
    "            current_chunk.append(num)\n",
    "            current_indices.append(i)\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        indices.append(current_indices)\n",
    "    return chunks, indices\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def catagorize_seqs(real_order,num_dominant_seqs,order):\n",
    "    \n",
    "    #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "    if not len(real_order) == num_dominant_seqs:\n",
    "        dominant = real_order[0:num_dominant_seqs]\n",
    "        other_ = real_order[num_dominant_seqs::]\n",
    "    else:\n",
    "        dominant = real_order\n",
    "        other_ = []\n",
    "\n",
    "    amounts = []\n",
    "    relative_amounts = []\n",
    "    pair_outcomes = []\n",
    "    pairs = []\n",
    "    for sequence in order:\n",
    "        ordered = 0\n",
    "        reverse = 0\n",
    "        repeat = 0\n",
    "        misordered = 0\n",
    "        non_to_task = 0\n",
    "        task_to_non = 0\n",
    "        other = 0\n",
    "        for index,element in enumerate(sequence[0:-1]):\n",
    "            pair = [element,sequence[index+1]]\n",
    "            outcome = (logic_machine_for_pair_catagorisation(pair,dominant,other_))\n",
    "            if outcome == 'ordered':\n",
    "                ordered +=1\n",
    "            elif outcome == 'reverse':\n",
    "                reverse +=1\n",
    "            elif outcome == 'repeat':\n",
    "                repeat +=1\n",
    "            elif outcome == 'misordered':\n",
    "                misordered +=1\n",
    "            elif outcome == 'task to other':\n",
    "                task_to_non +=1\n",
    "            elif outcome == 'other to task':\n",
    "                non_to_task +=1\n",
    "            elif outcome == 'other':\n",
    "                other +=1\n",
    "            pairs += [pair]\n",
    "            pair_outcomes += [[outcome]]\n",
    "        pairs += [[None]]\n",
    "        pair_outcomes += [[None]]\n",
    "\n",
    "        relative_amounts += [list(np.array([ordered,reverse,repeat,misordered,non_to_task,task_to_non,other])/(len(sequence)-1))]\n",
    "        amounts += [[ordered,reverse,repeat,misordered,non_to_task,task_to_non,other]]\n",
    "        \n",
    "    return relative_amounts, amounts,pair_outcomes,pairs\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_inds_for_seq_groups(lst):\n",
    "    groups = []\n",
    "    new = True\n",
    "    for ind,item in enumerate(lst):\n",
    "        if new:\n",
    "            if item > 0:\n",
    "                start = ind\n",
    "                new = False\n",
    "        else:\n",
    "            if item == 0:\n",
    "                end = ind-1\n",
    "                groups.append((start, end))\n",
    "                new = True\n",
    "    return groups\n",
    "\n",
    "\n",
    "def plot_event_transitions(event_transitions,axes,title):\n",
    "    transitions = list(event_transitions.keys())\n",
    "    counts = list(event_transitions.values())\n",
    "\n",
    "    # Create a bar chart\n",
    "    axes.bar(range(len(transitions)), counts,alpha = 0.5)\n",
    "\n",
    "    # Set x-axis labels\n",
    "    labels = [str(transition) for transition in transitions]\n",
    "    axes.set_xticks(range(len(transitions)), labels, rotation=90)\n",
    "\n",
    "    # Set y-axis label\n",
    "    axes.set_ylabel('Normalized Occurrences %')\n",
    "\n",
    "    # Add title\n",
    "    axes.set_title('Event Transitions' + title)\n",
    "\n",
    "def count_event_transitions(event_list):\n",
    "    \n",
    "    ### change code so that it sets each dictionary with these pairs a sa starting point! \n",
    "    from itertools import product\n",
    "    numbers = range(1, 7)  # Numbers 1 to 6a\n",
    "    all_possible_pairs = list(product(numbers, repeat=2))\n",
    "\n",
    "    transitions = {key: 0 for key in all_possible_pairs}  # Create an empty dictionary with the defined keys\n",
    "    \n",
    "    total_transitions = len(event_list) - 1\n",
    "    \n",
    "    for i in range(total_transitions):\n",
    "        current_event = event_list[i]\n",
    "        next_event = event_list[i + 1]\n",
    "        transition = (current_event, next_event)\n",
    "        if transition in transitions:\n",
    "            transitions[transition] += 1\n",
    "        else:\n",
    "            transitions[transition] = 1\n",
    "    \n",
    "    normalized_transitions = {}\n",
    "    for transition, count in transitions.items():\n",
    "        normalized_count = count / total_transitions * 100\n",
    "        normalized_transitions[transition] = normalized_count\n",
    "    \n",
    "    return normalized_transitions\n",
    "\n",
    "\n",
    "def normalize_counts_to_percentages(events_dict):\n",
    "    total_count = sum(events_dict.values())\n",
    "    normalized_dict = {}\n",
    "\n",
    "    for event, count in events_dict.items():\n",
    "        percentage = (count / total_count) * 100\n",
    "        normalized_dict[event] = percentage\n",
    "\n",
    "    return normalized_dict\n",
    "\n",
    "def SaveFig_noclose(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "    \n",
    "def sortperm_neurons(bkgd_log_proportions_array,config,neuron_response_df, sequence_ordering=None, th=0.2):\n",
    "    ## this is number of neurons in total\n",
    "    N_neurons= bkgd_log_proportions_array.shape[1]\n",
    "    ## number of sequences from json file \n",
    "    n_sequences = config[\"num_sequence_types\"]\n",
    "    # the 18 neuron params for each neuron from the last iteration\n",
    "    all_final_globals = neuron_response_df.iloc[-N_neurons:]\n",
    "    # this cuts it down to just the first 6 params - i think this correspond sto the first param for each seq type? response probABILITY - ie the chance that a neuron spikes in a given latent seq \n",
    "    resp_prop = np.exp(all_final_globals.values[:, :n_sequences])#\n",
    "    # this takes the next 6 params - which i think are the offset values\n",
    "    offset = all_final_globals.values[-N_neurons:, n_sequences:2*n_sequences]\n",
    "    ## finds the max response value - ie. which seq it fits to? \n",
    "    peak_response = np.amax(resp_prop, axis=1)\n",
    "    # then threshold the reponse\n",
    "    has_response = peak_response > np.quantile(peak_response, th)\n",
    "    # I thin this is the sequence that the neuron has the max response for: ie. we are ordering them by max response \n",
    "    preferred_type = np.argmax(resp_prop, axis=1)\n",
    "    if sequence_ordering is None:\n",
    "        # order them by max reponse \n",
    "        ordered_preferred_type = preferred_type\n",
    "    else:\n",
    "        #order them differnetly \n",
    "        ordered_preferred_type = np.zeros(N_neurons)#\n",
    "        # loop through each sequence\n",
    "        for seq in range(n_sequences):\n",
    "            # where does  max repsone = user defined seque\n",
    "            seq_indices = np.where(preferred_type == sequence_ordering[seq])\n",
    "            # change order to different seq\n",
    "            ordered_preferred_type[seq_indices] = seq\n",
    "\n",
    "    # reorder the offset params according to max respsone\n",
    "    preferred_delay = offset[np.arange(N_neurons), preferred_type]\n",
    "    Z = np.stack([has_response, ordered_preferred_type+1, preferred_delay], axis=1)\n",
    "    indexes = np.lexsort((Z[:, 2], Z[:, 1], Z[:, 0]))\n",
    "    return indexes,ordered_preferred_type\n",
    "\n",
    "# Function to find corresponding number in another column\n",
    "def find_corresponding(nums):\n",
    "    return [df_dict[num] for num in nums]\n",
    "\n",
    "def split_list(nums):\n",
    "    sublists = []\n",
    "    current_sublist = [nums[0]]\n",
    "    current_element = nums[0]\n",
    "    for i in range(1,len(nums)):\n",
    "        if nums[i] == current_element:\n",
    "            current_sublist.append(nums[i])\n",
    "        else:\n",
    "            sublists.append(current_sublist)\n",
    "            current_sublist = [nums[i]]\n",
    "            current_element = nums[i]\n",
    "    sublists.append(current_sublist)\n",
    "    return sublists\n",
    "\n",
    "def conactinate_nth_items(startlist):\n",
    "    concatinated_column_vectors = []\n",
    "    for c in range(len(max(startlist, key=len))):\n",
    "        column = []\n",
    "        for t in range(len(startlist)):\n",
    "            if c <= len(startlist[t])-1:\n",
    "                column = column + [startlist[t][c]]\n",
    "        concatinated_column_vectors.append(column)\n",
    "    return concatinated_column_vectors\n",
    "\n",
    "def load_H5_bodypart(tracking_path,video_type, tracking_point):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'task' in file:\n",
    "                back_file = pd.read_hdf(tracking_path + file)     \n",
    "                \n",
    "    # drag data out of the df\n",
    "    scorer = back_file.columns.tolist()[0][0]\n",
    "    body_part = back_file[scorer][tracking_point]\n",
    "    \n",
    "    parts=[]\n",
    "    for item in list(back_file[scorer]):\n",
    "        parts+=[item[0]]\n",
    "    print(np.unique(parts))\n",
    "    \n",
    "    # clean and interpolate frames with less than 98% confidence\n",
    "    clean_and_interpolate(body_part,0.98)\n",
    "    \n",
    "    return(body_part)\n",
    "  \n",
    "def load_H5_ports(tracking_path,video_type):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'port' in file:\n",
    "                back_ports_file = pd.read_hdf(tracking_path + file)\n",
    "\n",
    "    ## same for the ports:\n",
    "    scorer = back_ports_file.columns.tolist()[0][0]\n",
    "        \n",
    "    if video_type == 'back':\n",
    "        port1 =back_ports_file[scorer]['port2']\n",
    "        port2 =back_ports_file[scorer]['port1']\n",
    "        port3 =back_ports_file[scorer]['port6']\n",
    "        port4 =back_ports_file[scorer]['port3']\n",
    "        port5 =back_ports_file[scorer]['port7']\n",
    "    else:\n",
    "        port1 =back_ports_file[scorer]['Port2']\n",
    "        port2 =back_ports_file[scorer]['Port1']\n",
    "        port3 =back_ports_file[scorer]['Port6']\n",
    "        port4 =back_ports_file[scorer]['Port3']\n",
    "        port5 =back_ports_file[scorer]['Port7']\n",
    "\n",
    "    clean_and_interpolate(port1,0.98)\n",
    "    clean_and_interpolate(port2,0.98)\n",
    "    clean_and_interpolate(port3,0.98)\n",
    "    clean_and_interpolate(port4,0.98)\n",
    "    clean_and_interpolate(port5,0.98)\n",
    "    \n",
    "    return(port1,port2,port3,port4,port5)\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "def clean_and_interpolate(head_centre,threshold):\n",
    "    bad_confidence_inds = np.where(head_centre.likelihood.values<threshold)[0]\n",
    "    newx = head_centre.x.values\n",
    "    newx[bad_confidence_inds] = 0\n",
    "    newy = head_centre.y.values\n",
    "    newy[bad_confidence_inds] = 0\n",
    "\n",
    "    start_value_cleanup(newx)\n",
    "    interped_x = interp_0_coords(newx)\n",
    "\n",
    "    start_value_cleanup(newy)\n",
    "    interped_y = interp_0_coords(newy)\n",
    "    \n",
    "    head_centre['interped_x'] = interped_x\n",
    "    head_centre['interped_y'] = interped_y\n",
    "    \n",
    "def start_value_cleanup(coords):\n",
    "    # This is for when the starting value of the coords == 0; interpolation will not work on these coords until the first 0 \n",
    "    #is changed. The 0 value is changed to the first non-zero value in the coords lists\n",
    "    for index, value in enumerate(coords):\n",
    "        working = 0\n",
    "        if value > 0:\n",
    "            start_value = value\n",
    "            start_index = index\n",
    "            working = 1\n",
    "            break\n",
    "    if working == 1:\n",
    "        for x in range(start_index):\n",
    "            coords[x] = start_value\n",
    "            \n",
    "def interp_0_coords(coords_list):\n",
    "    #coords_list is one if the outputs of the get_x_y_data = a list of co-ordinate points\n",
    "    for index, value in enumerate(coords_list):\n",
    "        if value == 0:\n",
    "            if coords_list[index-1] > 0:\n",
    "                value_before = coords_list[index-1]\n",
    "                interp_start_index = index-1\n",
    "                #print('interp_start_index: ', interp_start_index)\n",
    "                #print('interp_start_value: ', value_before)\n",
    "                #print('')\n",
    "\n",
    "        if index < len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                if coords_list[index+1] > 0:\n",
    "                    interp_end_index = index+1\n",
    "                    value_after = coords_list[index+1]\n",
    "                    #print('interp_end_index: ', interp_end_index)\n",
    "                    #print('interp_end_value: ', value_after)\n",
    "                    #print('')\n",
    "\n",
    "                    #now code to interpolate over the values\n",
    "                    try:\n",
    "                        interp_diff_index = interp_end_index - interp_start_index\n",
    "                    except UnboundLocalError:\n",
    "#                         print('the first value in list is 0, use the function start_value_cleanup to fix')\n",
    "                        break\n",
    "                    #print('interp_diff_index is:', interp_diff_index)\n",
    "\n",
    "                    new_values = np.linspace(value_before, value_after, interp_diff_index)\n",
    "                    #print(new_values)\n",
    "\n",
    "                    interp_index = interp_start_index+1\n",
    "                    for x in range(interp_diff_index):\n",
    "                        #print('interp_index is:', interp_index)\n",
    "                        #print('new_value should be:', new_values[x])\n",
    "                        coords_list[interp_index] = new_values[x]\n",
    "                        interp_index +=1\n",
    "        if index == len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                for x in range(30):\n",
    "                    coords_list[index-x] = coords_list[index-30]\n",
    "                    #print('')\n",
    "#     print('function exiting')\n",
    "    return(coords_list)\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def load_H5_bodypart_new_data(tracking_path,video_type,exclude, tracking_point):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if not exclude in file:\n",
    "                back_file = pd.read_hdf(tracking_path + file)     \n",
    "                \n",
    "    # drag data out of the df\n",
    "    scorer = back_file.columns.tolist()[0][0]\n",
    "    body_part = back_file[scorer][tracking_point]\n",
    "\n",
    "    parts=[]\n",
    "    for item in list(back_file[scorer]):\n",
    "        parts+=[item[0]]\n",
    "    print(np.unique(parts))\n",
    "\n",
    "    # clean and interpolate frames with less than 98% confidence\n",
    "    clean_and_interpolate(body_part,0.98)\n",
    "    \n",
    "    return(body_part)\n",
    "\n",
    "def load_H5_ports_newdata(tracking_path,video_type):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "                back_ports_file = pd.read_hdf(tracking_path + file)\n",
    "\n",
    "    ## same for the ports:\n",
    "    scorer = back_ports_file.columns.tolist()[0][0]\n",
    "        \n",
    "    if video_type == 'back':\n",
    "        port1 =back_ports_file[scorer]['port2']\n",
    "        port2 =back_ports_file[scorer]['port1']\n",
    "        port3 =back_ports_file[scorer]['port6']\n",
    "        port4 =back_ports_file[scorer]['port3']\n",
    "        port5 =back_ports_file[scorer]['port7']\n",
    "    else:\n",
    "        port1 =back_ports_file[scorer]['Port2']\n",
    "        port2 =back_ports_file[scorer]['Port1']\n",
    "        port3 =back_ports_file[scorer]['Port6']\n",
    "        port4 =back_ports_file[scorer]['Port3']\n",
    "        port5 =back_ports_file[scorer]['Port7']\n",
    "\n",
    "    clean_and_interpolate(port1,0.98)\n",
    "    clean_and_interpolate(port2,0.98)\n",
    "    clean_and_interpolate(port3,0.98)\n",
    "    clean_and_interpolate(port4,0.98)\n",
    "    clean_and_interpolate(port5,0.98)\n",
    "        \n",
    "    return(port1,port2,port3,port4,port5)\n",
    "\n",
    "# Function to find the closest index using np.searchsorted\n",
    "def find_closest_indices(timestamps, target_times):\n",
    "    indices = np.searchsorted(timestamps, target_times)\n",
    "    indices = np.clip(indices, 1, len(timestamps) - 1)  # Ensure indices are within bounds\n",
    "    \n",
    "    # Compare target_time with its neighbors to find the closest\n",
    "    left_indices = indices - 1\n",
    "    right_indices = indices\n",
    "    left_diffs = np.abs(timestamps[left_indices] - target_times)\n",
    "    right_diffs = np.abs(timestamps[right_indices] - target_times)\n",
    "    \n",
    "    return np.where(left_diffs <= right_diffs, left_indices, right_indices)\n",
    "\n",
    "def expand_bounding_box(points, expansion_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Expands an axis-aligned bounding box around given points by a given ratio.\n",
    "    \n",
    "    Args:\n",
    "        points (ndarray): Array of shape (n, 2) with x, y coordinates.\n",
    "        expansion_ratio (float): Proportion by which to expand the box size.\n",
    "    \n",
    "    Returns:\n",
    "        (x_min, x_max, y_min, y_max): Expanded bounding box coordinates.\n",
    "    \"\"\"\n",
    "    x_min, y_min = points.min(axis=0)\n",
    "    x_max, y_max = points.max(axis=0)\n",
    "\n",
    "    x_center = (x_max + x_min) / 2\n",
    "    y_center = (y_max + y_min) / 2\n",
    "\n",
    "    x_half_range = (x_max - x_min) / 2 * (1 + expansion_ratio)\n",
    "    y_half_range = (y_max - y_min) / 2 * (1 + expansion_ratio)\n",
    "\n",
    "    return (x_center - x_half_range, x_center + x_half_range,\n",
    "            y_center - y_half_range, y_center + y_half_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be73ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_path,'back','head_centre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db2f520b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_presleep', '2_task', '3_postsleep']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(tracking_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c5c4e",
   "metadata": {},
   "source": [
    "# find example replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd76f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '136_1_3'\n",
    "# '136_1_4'\n",
    "# '149_1_1'\n",
    "# '178_1_4'\n",
    "# '178_1_7'\n",
    "# '178_1_8'\n",
    "# '178_1_9'\n",
    "\n",
    "mouse = '136_1_3'\n",
    "\n",
    "awake_PPpath = r'Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\awake\\\\'\n",
    "sleep_PPpath = r'Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\postsleep\\\\'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef9270",
   "metadata": {},
   "source": [
    "get awake sequences and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72d5d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LOADING processed_spike_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_44400\\3686811992.py:55: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  spikes_seq_type_adjusted = pickle.load(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LOADING DLC TRACKING DATA\n",
      "back_2021-11-11T15_07_18DLC_resnet50_port-tracking_back_viewMay2shuffle1_500000.h5\n",
      "back_2021-11-11T15_07_18DLC_resnet50_task-tracking_backviewApr6shuffle1_800000.h5\n"
     ]
    },
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5D.c\", line 1044, in H5Dread\n    can't synchronously read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5D.c\", line 992, in H5D__read_api_common\n    can't read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5VLcallback.c\", line 2078, in H5VL_dataset_read\n    dataset read failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5VLcallback.c\", line 2034, in H5VL__dataset_read\n    dataset read failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5VLnative_dataset.c\", line 374, in H5VL__native_dataset_read\n    can't read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Dio.c\", line 402, in H5D__read\n    can't read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Dchunk.c\", line 2913, in H5D__chunk_read\n    unable to read raw data chunk\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Dchunk.c\", line 4528, in H5D__chunk_lock\n    unable to read raw data chunk\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Fio.c\", line 98, in H5F_shared_block_read\n    read through page buffer failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5PB.c\", line 694, in H5PB_read\n    read through metadata accumulator failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Faccum.c\", line 248, in H5F__accum_read\n    driver read request failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5FDint.c\", line 259, in H5FD_read\n    driver read request failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5FDsec2.c\", line 712, in H5FD__sec2_read\n    file read failed: time = Sat Sep 27 14:06:55 2025\n, filename = 'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\\recording3_11-11-2021\\video\\tracking\\\\2_task\\back_2021-11-11T15_07_18DLC_resnet50_task-tracking_backviewApr6shuffle1_800000.h5', file descriptor = 4, errno = 22, error message = 'Invalid argument', buf = 000001604D2400C8, total read size = 131072, bytes this sub-read = 131072, offset = 15215768\n\nEnd of HDF5 error back trace\n\nProblems reading records.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m         back_p1,back_p2,back_p3,back_p4,back_p5 \u001b[38;5;241m=\u001b[39m load_H5_ports_newdata(tracking_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPORTS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     back_head_centre \u001b[38;5;241m=\u001b[39m \u001b[43mload_H5_bodypart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracking_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mback\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhead_centre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     back_p1,back_p2,back_p3,back_p4,back_p5 \u001b[38;5;241m=\u001b[39m load_H5_ports(tracking_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mback\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m## load in the timespan used for pppseq:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 636\u001b[0m, in \u001b[0;36mload_H5_bodypart\u001b[1;34m(tracking_path, video_type, tracking_point)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m video_type \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m--> 636\u001b[0m             back_file \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracking_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# drag data out of the df\u001b[39;00m\n\u001b[0;32m    639\u001b[0m scorer \u001b[38;5;241m=\u001b[39m back_file\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:465\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey must be provided when HDF5 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile contains multiple datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m                 )\n\u001b[0;32m    464\u001b[0m         key \u001b[38;5;241m=\u001b[39m candidate_only_group\u001b[38;5;241m.\u001b[39m_v_pathname\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mLookupError\u001b[39;00m):\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, HDFStore):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# if there is an error, close the store if we opened it.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:919\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# create the iterator\u001b[39;00m\n\u001b[0;32m    906\u001b[0m it \u001b[38;5;241m=\u001b[39m TableIterator(\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    908\u001b[0m     s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    916\u001b[0m     auto_close\u001b[38;5;241m=\u001b[39mauto_close,\n\u001b[0;32m    917\u001b[0m )\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:2042\u001b[0m, in \u001b[0;36mTableIterator.get_result\u001b[1;34m(self, coordinates)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     where \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;66;03m# directly return the result\u001b[39;00m\n\u001b[1;32m-> 2042\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   2044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:903\u001b[0m, in \u001b[0;36mHDFStore.select.<locals>.func\u001b[1;34m(_start, _stop, _where)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(_start, _stop, _where):\n\u001b[1;32m--> 903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:4712\u001b[0m, in \u001b[0;36mAppendableFrameTable.read\u001b[1;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_axes():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4712\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4714\u001b[0m info \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_index_axes[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], {})\n\u001b[0;32m   4716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_index_axes)\n\u001b[0;32m   4717\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   4718\u001b[0m )\n\u001b[0;32m   4720\u001b[0m inds \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes) \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_axes[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:3895\u001b[0m, in \u001b[0;36mTable._read_axes\u001b[1;34m(self, where, start, stop)\u001b[0m\n\u001b[0;32m   3893\u001b[0m \u001b[38;5;66;03m# create the selection\u001b[39;00m\n\u001b[0;32m   3894\u001b[0m selection \u001b[38;5;241m=\u001b[39m Selection(\u001b[38;5;28mself\u001b[39m, where\u001b[38;5;241m=\u001b[39mwhere, start\u001b[38;5;241m=\u001b[39mstart, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m-> 3895\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mselection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3898\u001b[0m \u001b[38;5;66;03m# convert the data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\io\\pytables.py:5508\u001b[0m, in \u001b[0;36mSelection.select\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mread_coordinates(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates)\n\u001b[1;32m-> 5508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\tables\\table.py:1984\u001b[0m, in \u001b[0;36mTable.read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m   1981\u001b[0m start, stop, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_range(start, stop, step,\n\u001b[0;32m   1982\u001b[0m                                         warn_negstep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1984\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m internal_to_flavor(arr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflavor)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\tables\\table.py:1893\u001b[0m, in \u001b[0;36mTable._read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[38;5;66;03m# Call the routine to fill-up the resulting array\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field:\n\u001b[0;32m   1891\u001b[0m     \u001b[38;5;66;03m# This optimization works three times faster than\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m     \u001b[38;5;66;03m# the row._fill_col method (up to 170 MB/s on a pentium IV @ 2GHz)\u001b[39;00m\n\u001b[1;32m-> 1893\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[38;5;66;03m# Warning!: _read_field_name should not be used until\u001b[39;00m\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;66;03m# H5TBread_fields_name in tableextension will be finished\u001b[39;00m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;66;03m# F. Alted 2005/05/26\u001b[39;00m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;66;03m# XYX Ho implementem per a PyTables 2.0??\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m field \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1899\u001b[0m     \u001b[38;5;66;03m# For step>15, this seems to work always faster than row._fill_col.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\tables\\tableextension.pyx:646\u001b[0m, in \u001b[0;36mtables.tableextension.Table._read_records\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5D.c\", line 1044, in H5Dread\n    can't synchronously read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5D.c\", line 992, in H5D__read_api_common\n    can't read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5VLcallback.c\", line 2078, in H5VL_dataset_read\n    dataset read failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5VLcallback.c\", line 2034, in H5VL__dataset_read\n    dataset read failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5VLnative_dataset.c\", line 374, in H5VL__native_dataset_read\n    can't read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Dio.c\", line 402, in H5D__read\n    can't read data\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Dchunk.c\", line 2913, in H5D__chunk_read\n    unable to read raw data chunk\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Dchunk.c\", line 4528, in H5D__chunk_lock\n    unable to read raw data chunk\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Fio.c\", line 98, in H5F_shared_block_read\n    read through page buffer failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5PB.c\", line 694, in H5PB_read\n    read through metadata accumulator failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5Faccum.c\", line 248, in H5F__accum_read\n    driver read request failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5FDint.c\", line 259, in H5FD_read\n    driver read request failed\n  File \"C:\\b\\abs_03tw9unsqx\\croot\\hdf5_1739225563765\\work\\src\\H5FDsec2.c\", line 712, in H5FD__sec2_read\n    file read failed: time = Sat Sep 27 14:06:55 2025\n, filename = 'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\\recording3_11-11-2021\\video\\tracking\\\\2_task\\back_2021-11-11T15_07_18DLC_resnet50_task-tracking_backviewApr6shuffle1_800000.h5', file descriptor = 4, errno = 22, error message = 'Invalid argument', buf = 000001604D2400C8, total read size = 131072, bytes this sub-read = 131072, offset = 15215768\n\nEnd of HDF5 error back trace\n\nProblems reading records."
     ]
    }
   ],
   "source": [
    "PP_PATH = awake_PPpath\n",
    "mouse_session_recording =  mouse\n",
    "\n",
    "        \n",
    "        \n",
    "# DAT_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "DAT_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "\n",
    "\n",
    "## set ppseq file\n",
    "for file_ in os.listdir(PP_PATH):\n",
    "    if mouse_session_recording in file_:\n",
    "        file = file_\n",
    "        \n",
    "save_path = PP_PATH + file + '\\\\analysis_output\\\\'\n",
    "        \n",
    "## set dat_path:\n",
    "for file_ in os.listdir(DAT_PATH):\n",
    "    if mouse_session_recording.split('_')[0] in file_:\n",
    "        if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "            dat_path = os.path.join(DAT_PATH,file_)\n",
    "for recording in os.listdir(os.path.join(DAT_PATH,dat_path)):\n",
    "    if recording.split('_')[0][9::] == mouse_session_recording.split('_')[-1]:\n",
    "        dat_path = os.path.join(dat_path,recording)\n",
    "        \n",
    "# set tracking path\n",
    "for file_ in os.listdir(dat_path + r\"\\video\\tracking\\\\\"):\n",
    "    if 'seq' in mouse_session_recording or 'ap5' in mouse_session_recording:\n",
    "        tracking_path = os.path.join(dat_path + r\"\\video\\tracking\\\\\")\n",
    "    else:\n",
    "        if 'task' in file_:\n",
    "            if not 'clock' in file_:\n",
    "                tracking_path = os.path.join(dat_path + r\"\\video\\tracking\\\\\",file_) + '\\\\'\n",
    "# set video paths\n",
    "for file_ in os.listdir(dat_path + r\"\\video\\videos\\\\\"):\n",
    "    if 'task' in file_:\n",
    "        if not 'clock' in file_:\n",
    "            cam_path = os.path.join(dat_path + r\"\\video\\videos\\\\\",file_) + '\\\\' \n",
    "            for vid_file in os.listdir(cam_path):\n",
    "                if 'back' in vid_file and '.avi' in vid_file:\n",
    "                    back_cam_avi_path = os.path.join(cam_path,vid_file)\n",
    "                if 'side' in vid_file and '.avi' in vid_file:\n",
    "                    side_cam_avi_path = os.path.join(cam_path,vid_file)\n",
    "                    \n",
    "\n",
    "print('\\n')\n",
    "print(\"LOADING processed_spike_data\")\n",
    "### load in releavent things:\n",
    "\n",
    "import pickle\n",
    "with open(PP_PATH + file + \"\\\\analysis_output\\\\\" + \"latent_event_history_df_split.pickle\", \"rb\") as input_file:\n",
    "    latent_event_history_df_split = pickle.load(input_file)\n",
    "    \n",
    "with open(PP_PATH + file + \"\\\\analysis_output\\\\\" + \"spikes_seq_type_adjusted.pickle\", \"rb\") as input_file:\n",
    "    spikes_seq_type_adjusted = pickle.load(input_file)\n",
    "    \n",
    "neuron_order = np.load(PP_PATH + file + \"\\\\analysis_output\\\\\" + 'neuron_order.npy')\n",
    "\n",
    "ordered_preferred_type = np.load(PP_PATH + file + \"\\\\analysis_output\\\\\" + 'ordered_preferred_type.npy')\n",
    "\n",
    "neuron_index = np.load(PP_PATH + file + \"\\\\analysis_output\\\\\" + 'neuron_index.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Opening JSON file\n",
    "f = open(PP_PATH + file + r'\\config_file.json')\n",
    "# returns JSON object as a dictionary\n",
    "config = eval(json.load(f))\n",
    "\n",
    "\n",
    "##### load deeplabcut tracking data\n",
    "print('\\n')\n",
    "print(\"LOADING DLC TRACKING DATA\")\n",
    "\n",
    "\n",
    "if 'seq' in mouse_session_recording or 'ap5' in mouse_session_recording:\n",
    "    \n",
    "    back_head_centre = load_H5_bodypart_new_data(tracking_path,'BACK','PORTS','head_centre')\n",
    "    if 'port_centroids.txt' in os.listdir(dat_path + r\"\\video\\videos\\\\\"):\n",
    "        # load in the txt file\n",
    "        port_centroids = pd.read_csv(dat_path + r\"\\video\\videos\\port_centroids.txt\", sep=\"\\t\")\n",
    "        pc = True\n",
    "    else:\n",
    "        pc = False\n",
    "        back_p1,back_p2,back_p3,back_p4,back_p5 = load_H5_ports_newdata(tracking_path,'PORTS')\n",
    "\n",
    "else:\n",
    "    back_head_centre = load_H5_bodypart(tracking_path,'back','head_centre')\n",
    "    back_p1,back_p2,back_p3,back_p4,back_p5 = load_H5_ports(tracking_path,'back')\n",
    "\n",
    "\n",
    "## load in the timespan used for pppseq:\n",
    "input_params_path = os.path.join(PP_PATH + file,'trainingData\\\\') + ('params_' + mouse_session_recording +'.json')\n",
    "# Opening JSON file\n",
    "f = open(input_params_path)\n",
    "# returns JSON object as  a dictionary\n",
    "input_config = json.load(f)\n",
    "behav_time_interval_start = input_config['time_span'][0]\n",
    "print(f\"      A corresponding time span has been found. Time span set to {behav_time_interval_start}\")\n",
    "\n",
    "\n",
    "## LOAD behaviour data\n",
    "print('\\n')\n",
    "print(\"LOADING BEHAV DATA\")\n",
    "behav_sync = pd.read_csv(dat_path + r'\\behav_sync\\2_task\\Behav_Ephys_Camera_Sync.csv')\n",
    "transitions = pd.read_csv(dat_path + r'\\behav_sync\\2_task\\Transition_data_sync.csv')\n",
    "\n",
    "behav_mask = (behav_sync.PokeIN_EphysTime>behav_time_interval_start[0])*(behav_sync.PokeIN_EphysTime<behav_time_interval_start[1])\n",
    "poke_in_times = (behav_sync[behav_mask].PokeIN_EphysTime) - behav_time_interval_start[0]\n",
    "ports = behav_sync[behav_mask].Port\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb71e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot sequences - basic\n",
    "timeframe = [0,total_length-1]\n",
    "mask = (spikes_seq_type_adjusted.timestamp>timeframe[0])*(spikes_seq_type_adjusted.timestamp<timeframe[-1])\n",
    "\n",
    "## plotting:\n",
    "fig, ax = plt.subplots(1, 1,figsize=(20, 10))\n",
    "# plot background in grey \n",
    "background_keep_mask = spikes_seq_type_adjusted[mask].sequence_type_adjusted <= 0\n",
    "ax.scatter(spikes_seq_type_adjusted[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],marker = 'o', s=40, linewidth=0,color = 'grey' ,alpha=0.5)\n",
    "# plot spikes without background\n",
    "background_remove_mask = spikes_seq_type_adjusted[mask].sequence_type_adjusted >= 0\n",
    "c_ = np.array(colors)[spikes_seq_type_adjusted[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "# ## faster:\n",
    "ax.scatter(spikes_seq_type_adjusted[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "### add chunk lines:\n",
    "## work out ordering chunk sizes\n",
    "split_lists = split_list(ordered_preferred_type[neuron_index])\n",
    "lens = []\n",
    "for list_ in split_lists:\n",
    "    lens += [len(list_)]\n",
    "chunks = [sum(lens[0:int(np.unique(ordered_preferred_type)[-1]+1)])] + lens[int(np.unique(ordered_preferred_type)[-1]+1)::] \n",
    "## plot seq divding lines:\n",
    "for i in range(len(chunks)):\n",
    "    ax.axhline(y = np.cumsum(chunks)[i], color = 'grey', linestyle = '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248e9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out awake example with tracking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9dc937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a743df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#rem/nrem + spindles - use ejt\n",
    "# load in the replay spikes for a certain mouse \n",
    "\n",
    "# plot nice example awake replays \n",
    "\n",
    "# search for good replay \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62c2c1",
   "metadata": {},
   "source": [
    "# save this data out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd198931",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "380ac84b",
   "metadata": {},
   "source": [
    "# load it and make figure plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in, align and plot spindles. (then save this segmetn out as well )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
