{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast \n",
    "\n",
    "def assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict):\n",
    "    # if session in one of the groups (and define which)   \n",
    "    if mouse in list(expert_mice) + list(hlesion_mice) + list(learning_mice):\n",
    "        if mouse in expert_mice:\n",
    "            var_dict['expert'] += [1]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [0]               \n",
    "        elif mouse in hlesion_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [1]\n",
    "            var_dict['learning'] += [0]   \n",
    "        elif mouse in learning_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [1]   \n",
    "    return var_dict\n",
    "\n",
    "def get_time_span(dat_path,pp_file,mouse):\n",
    "    with open(dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json', 'r') as file:\n",
    "        params = json.load(file)\n",
    "    time_spans = params['time_span']\n",
    "    return time_spans\n",
    "\n",
    "def find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start):\n",
    "    current_mouse_path = []\n",
    "    for run_index,pp_file in enumerate(os.listdir(sleep_ppseq_path)):\n",
    "        if not 'sleep_time_points' in pp_file:\n",
    "            # current mouse\n",
    "            mouse = '_'.join(pp_file.split('_')[0:3])    \n",
    "\n",
    "            if mouse in useable_mirs:\n",
    "                    \n",
    "                    # asign to experimental group in var_dict\n",
    "                    var_dict = assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict)\n",
    "\n",
    "                    # load in sleep start time and time span\n",
    "                    var_dict['current_sleep_start'] += sleep_start[mouse]\n",
    "                    var_dict['time_spans'] += get_time_span(sleep_ppseq_path,pp_file,mouse)\n",
    "\n",
    "                    # set path to processed files \n",
    "                    current_mouse_path += [sleep_ppseq_path + pp_file + '\\\\analysis_output\\\\']\n",
    "                    var_dict['mirs'] += [mouse]\n",
    "    return current_mouse_path,var_dict\n",
    "\n",
    "\n",
    "def make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only):\n",
    "    ## filter this data\n",
    "    if sequential_filter == True: \n",
    "        sequential_condition = data.ordering_classification == 'sequential'\n",
    "    else:\n",
    "        sequential_condition = np.array([True]*len(data.ordering_classification))\n",
    "\n",
    "    if sleep_filters_on == True:\n",
    "        if nrem_filter == True: \n",
    "            nrem_condition = data.nrem_events == 1\n",
    "        else:\n",
    "            nrem_condition = np.array([False]*len(data.nrem_events))\n",
    "\n",
    "        if rem_filter == True: \n",
    "            rem_condition = data.rem_events == 1\n",
    "        else:\n",
    "            rem_condition = np.array([False]*len(data.rem_events))\n",
    "\n",
    "        if background_only == True:\n",
    "            rem_condition = data.rem_events == 0\n",
    "            nrem_condition = data.nrem_events == 0\n",
    "\n",
    "    else:\n",
    "        nrem_condition = np.array([True]*len(data))\n",
    "        rem_condition = np.array([True]*len(data))\n",
    "        \n",
    "    # filter is set up so that any true will carry forward \n",
    "    filter_mask = sequential_condition * (nrem_condition + rem_condition)\n",
    "        \n",
    "    return filter_mask\n",
    "\n",
    "def determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path):\n",
    "    # if sleep_filters_on is false, use all chunk time\n",
    "    if sleep_filters_on == False:\n",
    "        mins = np.diff(chunk_time)[0]\n",
    "    else:\n",
    "        # load in state times\n",
    "        rem_state_times = np.load(path + 'rem_state_times.npy')\n",
    "        nrem_state_times = np.load(path + 'nrem_state_times.npy')\n",
    "        if len(rem_state_times) > 0:\n",
    "            tot_rem = sum(np.diff(rem_state_times))[0]\n",
    "        else:\n",
    "            tot_rem = 0\n",
    "        if len(nrem_state_times) > 0:\n",
    "            tot_nrem = sum(np.diff(nrem_state_times))[0]\n",
    "        else:\n",
    "            tot_nrem = 0\n",
    "\n",
    "        # if background then use all non rem and non nrem times\n",
    "        if background_only:\n",
    "            mins = np.diff(chunk_time)[0] - (tot_rem+tot_nrem)\n",
    "        else:\n",
    "            # if both, use both \n",
    "            if nrem_filter == True and rem_filter == True:\n",
    "                mins = tot_rem+tot_nrem\n",
    "            elif nrem_filter == True and rem_filter == False:\n",
    "                mins = tot_nrem\n",
    "            elif nrem_filter == False and rem_filter == True:\n",
    "                mins = tot_rem\n",
    "    # convert to mins            \n",
    "    mins = mins/60\n",
    "    \n",
    "    return mins\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def relative_dict(input_dict):\n",
    "    total_sum = sum(input_dict.values())\n",
    "    relative_dict = {key: value / total_sum for key, value in input_dict.items()}\n",
    "    return relative_dict\n",
    "\n",
    "def refind_cluster_events(filtered_chunk_data,event_proximity_filter):\n",
    "    \n",
    "    ### ignore the origonal clusterg rosp and remake them: \n",
    "    start_times = filtered_chunk_data.first_spike_time.values\n",
    "    end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "    clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "    cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "    for index,cluster in enumerate(clustered_events):\n",
    "        for item in cluster:\n",
    "            cluster_group[item] = int(index)\n",
    "    filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "    \n",
    "    return filtered_chunk_data\n",
    "\n",
    "def create_multicluster_dataframe(filtered_chunk_data):\n",
    "    meaned_order = []\n",
    "    fs_order = []\n",
    "    event_times = []\n",
    "    count = 0\n",
    "    for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "        group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "        current_cluster = filtered_chunk_data[group_mask].copy()\n",
    "        if len(current_cluster) > 1:\n",
    "            means = []\n",
    "            event_types = []\n",
    "            fs_orders = []\n",
    "            for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                # calculate event order based on spike time weighted mean\n",
    "                means += [np.mean(ast.literal_eval(events))]\n",
    "                # calculate order based on first spike time:\n",
    "                fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "            # order by mean time:    \n",
    "            meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "            # order by first spike:\n",
    "            fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "            event_times += [fs_orders]\n",
    "\n",
    "            current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "            current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "            current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "            if count == 0:\n",
    "                multi_cluster_df = current_cluster.copy()\n",
    "            else:\n",
    "                # Concatenate the DataFrames vertically (row-wise)\n",
    "                multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                # Reset the index if needed\n",
    "                multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "            count += 1\n",
    "    return multi_cluster_df,meaned_order,fs_order\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_ordering_amounts(meaned_order,dominant,other_):\n",
    "    ordered = 0\n",
    "    misordered = 0\n",
    "    other = 0\n",
    "    for cluster in meaned_order:\n",
    "        for ind,item in enumerate(cluster):\n",
    "            if not ind == len(cluster)-1:\n",
    "                pair = [item,cluster[ind+1]]\n",
    "                outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                    ordered += 1\n",
    "                elif outcome == 'misordered':\n",
    "                    misordered += 1\n",
    "                else:\n",
    "                    other +=1\n",
    "    return ordered,misordered,other\n",
    "\n",
    "def all_motifs_proportion_coactive(multi_cluster_df):\n",
    "    motif_motif_coative_events = []\n",
    "    for seq_type in range(1,7):\n",
    "        motif_cluster_groups = multi_cluster_df[multi_cluster_df['cluster_seq_type'] == seq_type].new_cluster_group\n",
    "        if not len(motif_cluster_groups) == 0:\n",
    "            coative_motif_events = len(motif_cluster_groups)\n",
    "        else:\n",
    "            coative_motif_events = 0\n",
    "        motif_motif_coative_events += [coative_motif_events]\n",
    "    return motif_motif_coative_events\n",
    "\n",
    "def motif_by_motif_ordering(meaned_order,real_order,dominant,other_):\n",
    "\n",
    "    all_motifs_fs_task_related_ordered = []\n",
    "    all_motifs_fs_task_related_misordered = []\n",
    "    all_motifs_fs_task_related_other = []\n",
    "\n",
    "    for motif_type in range(1,7):\n",
    "        ordered = 0\n",
    "        misordered = 0\n",
    "        other = 0\n",
    "        \n",
    "        if motif_type in real_order:\n",
    "            for cluster in meaned_order:\n",
    "                for ind,item in enumerate(cluster):\n",
    "                    if not ind == len(cluster)-1:\n",
    "                        pair = [item,cluster[ind+1]]\n",
    "                        if motif_type in pair:\n",
    "                            outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                            if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                                ordered += 1\n",
    "                            elif outcome == 'misordered':\n",
    "                                misordered += 1\n",
    "                            else:\n",
    "                                other +=1  \n",
    "                                \n",
    "            all_motifs_fs_task_related_ordered += [ordered]\n",
    "            all_motifs_fs_task_related_misordered += [misordered]\n",
    "            all_motifs_fs_task_related_other += [other]\n",
    "        else:\n",
    "            all_motifs_fs_task_related_ordered += ['nan']\n",
    "            all_motifs_fs_task_related_misordered += ['nan']\n",
    "            all_motifs_fs_task_related_other += ['nan']\n",
    "\n",
    "    return all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other\n",
    "\n",
    "def coactive_rate(filtered_chunk_data):\n",
    "    # how mnay coacitve in chunk: \n",
    "    current_coactive_freqs_chunk = {}\n",
    "    for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "        num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "        if num in current_coactive_freqs_chunk:\n",
    "            current_coactive_freqs_chunk[num] += 1\n",
    "        else:\n",
    "            current_coactive_freqs_chunk[num] = 1\n",
    "            \n",
    "    # total single events that are cocaitve with at least one other\n",
    "    cocative_total = 0\n",
    "    overall_total = 0\n",
    "    for item in list(current_coactive_freqs_chunk):\n",
    "        if item > 1:\n",
    "            cocative_total += current_coactive_freqs_chunk[item]\n",
    "        overall_total += current_coactive_freqs_chunk[item] * item\n",
    "\n",
    "    # coactive_lengths (only coactive, ignore single events)\n",
    "    coactive_len_per_chunk =[]\n",
    "    for item in current_coactive_freqs_chunk:\n",
    "        if item > 1:\n",
    "            coactive_len_per_chunk += current_coactive_freqs_chunk[item] * [item]\n",
    "\n",
    "\n",
    "    return cocative_total,coactive_len_per_chunk,overall_total\n",
    "\n",
    "def empty_chunk_vars():\n",
    "    ## set chunk vars \n",
    "    chunk_vars = {\"chunk_reactivations\" : [],\n",
    "                \"chunk_mins\" : [],\n",
    "                \"chunk_motif_type_reactivations\" : [],\n",
    "                \"mean_spikes_per_event\" : [],\n",
    "                \"motif_by_motif_mean_spikes_per_event\" : [],\n",
    "                \"mean_units_per_event\" : [],\n",
    "                \"motif_by_motif_mean_units_per_event\" : [],\n",
    "                \"chunk_event_lengths\" : [],\n",
    "                \"motif_event_lenghts\" : [],\n",
    "                \"total_single_events_coacitvely_paired\" : [],\n",
    "                \"coactive_lenghts\" : [],\n",
    "                \"overall_total_coactive_or_single_cluster_events\" : [],\n",
    "                \"meaned_order_task_related_ordered\":[],\n",
    "                \"meaned_order_task_related_misordered\":[],\n",
    "                \"meaned_order_task_related_other\":[],\n",
    "                \"fs_order_task_related_ordered\":[],\n",
    "                \"fs_order_task_related_misordered\":[],\n",
    "                \"fs_order_task_related_other\":[],\n",
    "                \"normalised_task_related_total\":[],\n",
    "                \"normalised_non_task_related_total\":[],\n",
    "                \"all_motifs_total_coactive\":[],\n",
    "                \"meaned_ordering_all_motifs_task_related_ordered\":[],\n",
    "                \"meaned_ordering_all_motifs_task_related_misordered\":[],\n",
    "                \"meaned_ordering_all_motifs_task_related_other\":[],\n",
    "                \"fs_ordering_all_motifs_task_related_ordered\":[],\n",
    "                \"fs_ordering_all_motifs_task_related_misordered\":[],\n",
    "                \"fs_ordering_all_motifs_task_related_other\":[],\n",
    "                \n",
    "                \"normalised_task_related_total\":[],\n",
    "                \"normalised_non_task_related_total\":[],\n",
    "                                \n",
    "                \n",
    "\n",
    "    }\n",
    "    return chunk_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def find_data_paths(all_mice_dict,sleep_path):\n",
    "    for file in os.listdir(sleep_path):\n",
    "        if 'run' in file:\n",
    "            paths_dict = {}\n",
    "            mir = file.split('run')[0][0:-1]\n",
    "            paths_dict['sleep_path'] = sleep_path + file\n",
    "            awake_base = os.path.join(remove_last_folder(sleep_path), 'awake')\n",
    "            for awake_file in os.listdir(awake_base):\n",
    "                if mir in awake_file:\n",
    "                    paths_dict['awake_path'] = os.path.join(awake_base, awake_file)\n",
    "            try: \n",
    "                paths_dict['full_org_dat_path'] = find_organised_path('EJT' + mir,r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\")\n",
    "            except:\n",
    "                paths_dict['full_org_dat_path']  = find_organised_path(mir,r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\")\n",
    "            all_mice_dict[mir] = paths_dict\n",
    "    return all_mice_dict\n",
    "\n",
    "def remove_last_folder(path: str) -> str:\n",
    "    # 1. Normalize: collapse duplicate slashes, strip trailing ones\n",
    "    normalized = os.path.normpath(path)\n",
    "    # 2. dirname: drop the last component\n",
    "    return os.path.dirname(normalized)\n",
    "\n",
    "def find_organised_path(mir,dat_path):\n",
    "    dat_path_2 = None\n",
    "    recording = None\n",
    "    print(mir)\n",
    "    for animal_implant in os.listdir(dat_path):\n",
    "        current_m_i = '_'.join([animal_implant.split('_')[0],animal_implant.split('_')[-1][-1]])\n",
    "        mi = '_'.join(mir.split('_')[0:-1])\n",
    "        if current_m_i == mi:\n",
    "            dat_path_2 = os.path.join(dat_path,animal_implant)\n",
    "            break\n",
    "    print(dat_path_2)\n",
    "    for ind,item in enumerate([record.split('ing')[-1].split('_')[0] for record in os.listdir(dat_path_2)]):\n",
    "        if item == mir.split('_')[-1]:\n",
    "            recording = os.listdir(dat_path_2)[ind]\n",
    "    full_org_dat_path = os.path.join(dat_path_2,recording)\n",
    "    print(full_org_dat_path)\n",
    "    return full_org_dat_path\n",
    "\n",
    "def load_in_sleep_state_scoring(mouse):\n",
    "    print('---------------------')\n",
    "    print('searching for sleep state scoring')\n",
    "    # determine organised data paths for the current mouse\n",
    "    if mouse.split('_')[0].isdigit():\n",
    "        org_dat_path = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "        old_data = True\n",
    "    else:\n",
    "        org_dat_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "        old_data = False\n",
    "    org_mouse_file = None\n",
    "    for file in os.listdir(org_dat_path):\n",
    "        if mouse.split('_')[0] in file:\n",
    "            # if the implant = implant\n",
    "            if mouse.split('_')[1] == file.split('_')[-1][-1]:\n",
    "                print(f'1. mouse file found: \\033[1m{file}\\033[0m')\n",
    "                org_mouse_file = os.path.join(org_dat_path,file)\n",
    "    mouse_org_data_path = None\n",
    "    for recording in os.listdir(org_mouse_file):\n",
    "        if mouse.split('_')[-1] == recording.split('ing')[-1].split('_')[0]:\n",
    "            print(f'2. recording found: \\033[1m{recording}\\033[0m')\n",
    "            mouse_org_data_path = os.path.join(org_mouse_file,recording) + r'\\\\'\n",
    "\n",
    "    # load in sleep scoring data \n",
    "\n",
    "    sleep_state_score_path = mouse_org_data_path + r\"\\ephys\\LFP\\\\sleep_state_score\\\\\"\n",
    "    if not os.path.exists(sleep_state_score_path):\n",
    "        sleep_state_score_path =  mouse_org_data_path + '/ephys/probeA/LFP/'\n",
    "\n",
    "    if not os.path.exists(sleep_state_score_path):\n",
    "        print(f\"Sleep state score files not found for {mouse}.\")\n",
    "    else:\n",
    "        nrem_start_ends = np.load(sleep_state_score_path + \"nrem_start_ends.npy\", allow_pickle=True)\n",
    "        rem_start_ends = np.load(sleep_state_score_path + \"rem_start_ends.npy\", allow_pickle=True)\n",
    "        print (f\"\\033[1mSuccess!\\033[0m Loaded sleep state score files for mouse: {mouse}.\")\n",
    "\n",
    "    print('----------------------')\n",
    "        \n",
    "    return nrem_start_ends,rem_start_ends,mouse_org_data_path,old_data\n",
    "\n",
    "def get_chunk_state_times(rem_start_ends,chunk_time):\n",
    "    chunk_rem_times = []\n",
    "    for start,end in rem_start_ends:\n",
    "        if start >= chunk_time[0] and start <= chunk_time[1]:\n",
    "            start_chunk_rebased = start - chunk_time[0]\n",
    "            end_chunk_rebased = end - chunk_time[0]\n",
    "            # expand by 10%\n",
    "            start_chunk_rebased = start_chunk_rebased * 0.9\n",
    "            end_chunk_rebased = end_chunk_rebased * 1.1\n",
    "            # if the end stetches past the end of the chunk then just set it to the end of the chunk\n",
    "            if end_chunk_rebased > np.diff(chunk_time)[0]:\n",
    "                end_chunk_rebased = np.diff(chunk_time)[0]\n",
    "            chunk_rem_times += [[start_chunk_rebased,end_chunk_rebased]]\n",
    "    return chunk_rem_times\n",
    "\n",
    "def reactivation_per_minute(nrem_filtered_chunk_data, nrem_state_times):\n",
    "    # save out data\n",
    "    reactivations_found = len(nrem_filtered_chunk_data)\n",
    "\n",
    "    if len(nrem_state_times) > 0:\n",
    "        mins = np.diff(nrem_state_times)\n",
    "    else:\n",
    "        mins = 0\n",
    "        \n",
    "    # convert to mins            \n",
    "    mins = mins/60\n",
    "\n",
    "    if mins > 0:\n",
    "        return reactivations_found/mins\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def find_coactive_pair_rate(chunk_data_clusters,seq_id):\n",
    "    coactive_clusts = 0\n",
    "    total = 0\n",
    "    for index,item_group in enumerate(chunk_data_clusters.coactive_cluster_group):\n",
    "        if chunk_data_clusters.cluster_seq_type[index] == seq_id+1:\n",
    "            full_group = np.where(chunk_data_clusters.coactive_cluster_group == item_group)[0]\n",
    "            if len(full_group) > 1:\n",
    "                coactive_clusts += 1\n",
    "            total += 1\n",
    "    if total > 0:\n",
    "        return coactive_clusts,total\n",
    "    else:\n",
    "        return np.nan,np.nan\n",
    "    \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motif rate per min \n",
    "# motif relative event rate \n",
    "# motif relative proportion \n",
    "# motif spikes per event\n",
    "# motif units per event\n",
    "# motif event lengths\n",
    "# motif proportion coactive\n",
    "# motif proportion appearing in ordered coative pairs\n",
    "# motif overall neuron spiking consistency - are the neurons that we saw in replay on average consistently in or not in awake replay?\n",
    "# motif average warp factor\n",
    "# motif forward vs reverse replay ratio \n",
    "# motif proportion spindle linked / distance to closest spindle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EJT136_1_3\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\\recording3_11-11-2021\n",
      "EJT136_1_4\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\\recording4_12-11-2021\n",
      "EJT148_2_2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT148_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT148_implant2\\recording2_19-10-2020\n",
      "EJT149_1_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT149_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT149_implant1\\recording1_16-11-2021\n",
      "EJT178_1_4\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording4_18-03-2022\n",
      "EJT178_1_5\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording5_21-03-2022\n",
      "EJT178_1_6\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording6_29-03-2022\n",
      "EJT178_1_7\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording7_30-03-2022\n",
      "EJT178_1_8\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording8_31-03-2022\n",
      "EJT178_1_9\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording9_01-04-2022\n",
      "EJT178_2_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording1_04-04-2022\n",
      "EJT178_2_2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording2_05-04-2022\n",
      "EJT178_2_4\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording4_07-04-2022\n",
      "EJT269_1_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording1_13-05-2023\n",
      "EJT269_1_3\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording3_16-05-2023\n",
      "EJT269_1_4\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording4_18-05-2023\n",
      "EJT269_1_7\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording7_23-05-2023\n",
      "EJT270_1_3\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording3_14-05-2023\n",
      "EJT270_1_5\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording5_17-05-2023\n",
      "EJT270_1_6\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording6_19-05-2023\n",
      "EJTseq006_1_11\n",
      "None\n",
      "seq006_1_11\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\\recording11_28-11-2024\n",
      "EJT268_1_2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT268_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT268_implant1\\recording2_02-05-2023\n",
      "EJT269_1_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording1_13-05-2023\n",
      "EJT269_1_2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording2_15-05-2023\n",
      "EJT269_1_3\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording3_16-05-2023\n",
      "EJT270_1_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording1_10-05-2023\n",
      "EJT270_1_3\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording3_14-05-2023\n",
      "EJTap5R_1_1\n",
      "None\n",
      "ap5R_1_1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording1_16-11-2024\n",
      "EJTap5R_1_3\n",
      "None\n",
      "ap5R_1_3\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording3_19-11-2024\n",
      "EJTseq006_1_1\n",
      "None\n",
      "seq006_1_1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\\recording1_15-11-2024\n",
      "EJTseq006_1_4\n",
      "None\n",
      "seq006_1_4\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\\recording4_19-11-2024\n",
      "EJTseq006_1_5\n",
      "None\n",
      "seq006_1_5\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\\recording5_20-11-2024\n",
      "EJTseq006_1_6\n",
      "None\n",
      "seq006_1_6\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq006_implant1\\recording6_21-11-2024\n",
      "EJTseq007_1_1\n",
      "None\n",
      "seq007_1_1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024\n",
      "EJTseq007_1_2\n",
      "None\n",
      "seq007_1_2\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording2_19-11-2024\n",
      "EJTseq007_1_3\n",
      "None\n",
      "seq007_1_3\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording3_20-11-2024\n",
      "----------\n",
      "178_1_9\n",
      "0\n",
      "chunk1_9000to9900\n",
      "---------------------\n",
      "searching for sleep state scoring\n",
      "1. mouse file found: \u001b[1mEJT178_implant1\u001b[0m\n",
      "2. recording found: \u001b[1mrecording9_01-04-2022\u001b[0m\n",
      "\u001b[1mSuccess!\u001b[0m Loaded sleep state score files for mouse: 178_1_9.\n",
      "----------------------\n",
      "chunk2_10100to10700\n",
      "---------------------\n",
      "searching for sleep state scoring\n",
      "1. mouse file found: \u001b[1mEJT178_implant1\u001b[0m\n",
      "2. recording found: \u001b[1mrecording9_01-04-2022\u001b[0m\n",
      "\u001b[1mSuccess!\u001b[0m Loaded sleep state score files for mouse: 178_1_9.\n",
      "----------------------\n",
      "chunk3_11300to11900\n",
      "---------------------\n",
      "searching for sleep state scoring\n",
      "1. mouse file found: \u001b[1mEJT178_implant1\u001b[0m\n",
      "2. recording found: \u001b[1mrecording9_01-04-2022\u001b[0m\n",
      "\u001b[1mSuccess!\u001b[0m Loaded sleep state score files for mouse: 178_1_9.\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_42616\\3559837215.py:172: RuntimeWarning: Mean of empty slice\n",
      "  seq_event_lens += [np.nanmean([item for sublist in [item[i] for item in chunk_event_lens] for item in sublist])]\n",
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_42616\\3559837215.py:197: RuntimeWarning: Mean of empty slice\n",
      "  mean_spikes_per_seq += [np.nanmean([item for sublist in [item[i][0] for item in num_spikes_chunk] for item in sublist])]\n",
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_42616\\3559837215.py:198: RuntimeWarning: Mean of empty slice\n",
      "  mean_neurons_per_seq += [np.nanmean([item for sublist in [item[i][0] for item in num_neurons_chunk] for item in sublist])]\n"
     ]
    }
   ],
   "source": [
    "all_mice_dict = {}\n",
    "all_mice_dict = find_data_paths(all_mice_dict,'Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\postsleep\\\\')\n",
    "all_mice_dict = find_data_paths(all_mice_dict,'Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\learning\\postsleep\\\\')\n",
    "\n",
    "\n",
    "for index, mouse in enumerate(list(all_mice_dict)[9::]):\n",
    "    replay_data_found = False\n",
    "    print('----------')\n",
    "    print(mouse)\n",
    "    print(index)\n",
    "    current_mouse_sleep_path = all_mice_dict[mouse]['sleep_path']\n",
    "    if '_final_analysis_output' in os.listdir(current_mouse_sleep_path):\n",
    "        # load in sleep start time \n",
    "        sleep_time_point_df = pd.read_csv(remove_last_folder(current_mouse_sleep_path) + '\\sleep_time_points.csv')\n",
    "        current_sleep_start = sleep_time_point_df['approx_sleep_start'][np.where(sleep_time_point_df.mir == mouse)[0][0]]\n",
    "        params_file = current_mouse_sleep_path + r'\\trainingData\\\\' + 'params_' + mouse + '.json'\n",
    "        with open(params_file, 'r') as file:\n",
    "            params = json.load(file)\n",
    "        time_spans = params['time_span']\n",
    "        \n",
    "        # load sequence order\n",
    "        sequence_order = pd.read_csv(remove_last_folder(current_mouse_sleep_path) + r'\\sequence_order.csv')\n",
    "        mir_row = None\n",
    "        for ind, row in sequence_order.iterrows():\n",
    "            if row.mir in mouse:\n",
    "                mir_row = row\n",
    "        seq_order = literal_eval(mir_row.seq_order)\n",
    "        \n",
    "    else:\n",
    "        print('No replay data found for ' + mouse)\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # initialize lists to hold data across chunks \n",
    "    #1\n",
    "    chunk_rpm = []\n",
    "    #2\n",
    "    chunk_event_lens = []\n",
    "    #3\n",
    "    regression_df = pd.DataFrame({'seq_type':[],'regression_line':[],'filt_rel_spike_times':[],'slope':[],'reactivation_ID':[],'warp_factor':[],'mouse':[]})\n",
    "    #4\n",
    "    total_seqs_chunk = []\n",
    "    # 6+7\n",
    "    num_spikes_chunk = []\n",
    "    num_neurons_chunk = []\n",
    "    #8 \n",
    "    coactive_total_per_chunk = []\n",
    "    overall_total_per_chunk  = []\n",
    "    \n",
    "    current_mouse_replay_path = current_mouse_sleep_path + '\\\\_final_analysis_output'\n",
    "    ## loop across all chunk files ################################\n",
    "    for chunk_number,file in enumerate(os.listdir(current_mouse_replay_path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            current_data_path = current_mouse_replay_path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(current_data_path + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(current_data_path + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            ###### FILTERING AND MASKING ##################################################################''\n",
    "            ## filter this data for sequential ordering\n",
    "            sequential_condition = data.ordering_classification == 'sequential'\n",
    "            # filter is set up so that any true will carry forward \n",
    "            filtered_chunk_data = data[sequential_condition].reset_index()\n",
    "            \n",
    "            ## REM / NREM times only\n",
    "            # load in sleep state scoring\n",
    "            nrem_start_ends,rem_start_ends,mouse_org_data_path,old_data = load_in_sleep_state_scoring(mouse)\n",
    "            # get relevant rem/nrem times for chunk\n",
    "            chunk_nrem_times = get_chunk_state_times(nrem_start_ends,chunk_time)\n",
    "            chunk_rem_times = get_chunk_state_times(rem_start_ends,chunk_time) \n",
    "            # get spike times relative to chunk:\n",
    "            chunk_number = int(file.split('_')[0][-1])\n",
    "            chunk_start_offset = ([0]+list(np.cumsum(np.diff(time_spans))))[chunk_number-1]\n",
    "            # make relative to start of chunk\n",
    "            fs_event_times = filtered_chunk_data['first_spike_time'].values - chunk_start_offset\n",
    "            # find inds of spike times that are in nrem and rem periods:\n",
    "            idx  = []\n",
    "            for start,end in chunk_nrem_times + chunk_rem_times:\n",
    "                idx += list(np.where((fs_event_times >= start) & (fs_event_times <= end))[0])\n",
    "            # filter the data frame, only keeping the rows in idx\n",
    "            filtered_chunk_data = filtered_chunk_data.iloc[idx]\n",
    "            filtered_chunk_data = filtered_chunk_data.reset_index(drop=True)\n",
    "            # get rid of the stupid unnamed columns\n",
    "            filtered_chunk_data = filtered_chunk_data.loc[:, ~filtered_chunk_data.columns.str.startswith('Unnamed')]\n",
    "            \n",
    "            #### keep only the seq types that are in the sequence order\n",
    "            keep_inds = []\n",
    "            for i,seq_type in enumerate(filtered_chunk_data['cluster_seq_type'].values):\n",
    "                if int(seq_type)+1 in seq_order:\n",
    "                    keep_inds += [i]\n",
    "            filtered_chunk_data = filtered_chunk_data.loc[keep_inds].reset_index(drop=True)\n",
    "                        \n",
    "            # 1. REACTIVATION PER MINUTE: #######################################################################\n",
    "            seq_rpm = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_rpm += [reactivation_per_minute(seq_chunk_data, chunk_time)]\n",
    "            chunk_rpm += [seq_rpm]\n",
    "              \n",
    "            # 2. REACTIVATION EVENT LENGTH #######################################################################\n",
    "            seq_lens = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_lens += [list(seq_chunk_data.event_length.values)]\n",
    "            chunk_event_lens += [seq_lens]\n",
    "            \n",
    "            #3.4. WARP #######################################################################\n",
    "            current_data_path_temporal_structre = current_data_path + 'temporal_structure_analysis\\\\'\n",
    "            # load in regression dfs\n",
    "            df_load = pd.read_csv(current_data_path_temporal_structre+'regression_df.csv',index_col=0)\n",
    "            df_load['mouse'] = [mouse]*len(df_load)\n",
    "            #concat:\n",
    "            regression_df = pd.concat((regression_df,df_load),axis =0)\n",
    "\n",
    "            #5. RELATIVE OVERALL PROPORTION ########################################################################\n",
    "            total_seqs = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                total_seqs += [len(seq_chunk_data)]\n",
    "            total_seqs_chunk += [total_seqs]\n",
    "            \n",
    "            #6 + 7 .  SPIKES + UNITS PER EVENT ########################################################################\n",
    "            num_spikes_perseq = []\n",
    "            num_neurons_perseq = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                num_spikes = []\n",
    "                num_neurons = []\n",
    "                for i in range(len(seq_chunk_data)):\n",
    "                    num_spikes +=[len(literal_eval(seq_chunk_data['cluster_spike_times'].values[i]))]\n",
    "                    num_neurons += [len(np.unique(literal_eval(seq_chunk_data['cluster_neurons'].values[i])))]\n",
    "                num_spikes_perseq += [[num_spikes]]\n",
    "                num_neurons_perseq += [[num_neurons]]\n",
    "            num_spikes_chunk += [num_spikes_perseq]\n",
    "            num_neurons_chunk += [num_neurons_perseq]\n",
    "            \n",
    "            # 8. COACTIVE EVENTS ########################################################################\n",
    "            # filter for all task related seqs\n",
    "            task_seq_only_data = filtered_chunk_data.loc[filtered_chunk_data.cluster_seq_type.isin(np.array(seq_order) + 1)]\n",
    "            # refine the clusters\n",
    "            event_proximity_filter = 0.3 # seconds\n",
    "            chunk_data_clusters  = refind_cluster_events(task_seq_only_data,event_proximity_filter)\n",
    "\n",
    "            # for each sequence get the number of events that were paired with another, and the total number of events - for this chunk \n",
    "            coactive_total_per_seq = []\n",
    "            overall_total_per_seq = []\n",
    "            for seq_id in seq_order:\n",
    "                coacitve_tot, overall_tot = find_coactive_pair_rate(chunk_data_clusters,seq_id)\n",
    "                coactive_total_per_seq += [[coacitve_tot]]\n",
    "                overall_total_per_seq += [[overall_tot]]\n",
    "            coactive_total_per_chunk += [coactive_total_per_seq]\n",
    "            overall_total_per_chunk += [overall_total_per_seq]\n",
    "                        \n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "            \n",
    "    ## outside chunk loop\n",
    "    \n",
    "    #1. REACTIVATION PER MINUTE \n",
    "    seq_event_per_min = []\n",
    "    for i in range(len(seq_order)):\n",
    "        seq_event_per_min += [np.mean([item[i] for item in chunk_rpm])]\n",
    "    #!#\n",
    "    relative_seq_event_rate_per_min = seq_event_per_min/sum(seq_event_per_min)\n",
    "    \n",
    "    #2. REACTIVATION EVENT LENGTH\n",
    "    #!#\n",
    "    seq_event_lens = []\n",
    "    for i in range(len(seq_order)):\n",
    "        seq_event_lens += [np.nanmean([item for sublist in [item[i] for item in chunk_event_lens] for item in sublist])]\n",
    "    \n",
    "        \n",
    "    #3 + 4. WARP +forward/reverse proportion\n",
    "    forward_proportion_compared_to_reverse = []\n",
    "    regression_df.reset_index(inplace=True,drop=True)\n",
    "    for seq_id in seq_order:\n",
    "        seq_reg_data = regression_df.loc[np.where(regression_df.seq_type == seq_id+1)]\n",
    "        #!#\n",
    "        mean_warp += [np.mean(seq_reg_data['warp_factor'])]\n",
    "        forward_proportion_compared_to_reverse += [len(seq_reg_data[seq_reg_data. warp_factor > 0])/len(seq_reg_data[seq_reg_data. warp_factor < 0])]\n",
    "                \n",
    "    # 5. RELATIVE OVERALL PROPORTION\n",
    "    # motif relative proportion \n",
    "    total_seqs = []\n",
    "    \n",
    "    for i in range(len(seq_order)):\n",
    "        total_seqs += [sum([item[i] for item in total_seqs_chunk])]\n",
    "    #!#\n",
    "    relative_motif_expression_total = np.array(total_seqs)/sum(total_seqs)\n",
    "    \n",
    "    ## 6 + 7. SPIKES + UNITS PER EVENT\n",
    "    #!# #!#\n",
    "    mean_spikes_per_seq = []\n",
    "    mean_neurons_per_seq = []\n",
    "    for i in range(len(seq_order)):\n",
    "        mean_spikes_per_seq += [np.nanmean([item for sublist in [item[i][0] for item in num_spikes_chunk] for item in sublist])]\n",
    "        mean_neurons_per_seq += [np.nanmean([item for sublist in [item[i][0] for item in num_neurons_chunk] for item in sublist])]\n",
    "    \n",
    "    # 8 . COACTIVE EVENTS\n",
    "    proportion_events_coactive_per_motif = []\n",
    "    for i in range(len(seq_order)):\n",
    "        total_coactivly_paired = sum([item for sublist in [item[i] for item in coactive_total_per_chunk] for item in sublist])\n",
    "        overall_total = sum([item for sublist in [item[i] for item in overall_total_per_chunk] for item in sublist])\n",
    "        \n",
    "        if overall_total == 0:\n",
    "            coactive_proportion = np.nan\n",
    "        else:\n",
    "            coactive_proportion = total_coactivly_paired/overall_total\n",
    "        #!#\n",
    "        proportion_events_coactive_per_motif += [coactive_proportion]\n",
    "\n",
    "    break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "# motif relative event rate \n",
    "# motif event lengths\n",
    "# motif average warp factor\n",
    "# motif forward vs reverse replay ratio \n",
    "# motif relative proportion \n",
    "# motif spikes per event\n",
    "# motif units per event\n",
    "# motif proportion coactive\n",
    "\n",
    "# motif proportion appearing in ordered coative pairs\n",
    "# motif overall neuron spiking consistency - are the neurons that we saw in replay on average consistently in or not in awake replay?\n",
    "# motif proportion spindle linked / distance to closest spindle \n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are coactive clusters \n",
    "if len(chunk_data_clusters.coactive_cluster_group.unique()) < len(chunk_data_clusters):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering of coactive?\n",
    "if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "    multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "else:\n",
    "    multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "# pull out sequence order for current mouse\n",
    "seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "real_order = np.array(seq_order)+1\n",
    "\n",
    "#deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "if not len(real_order) == num_dominant_seqs:\n",
    "    dominant = list(real_order[0:num_dominant_seqs])\n",
    "    other_ = list(real_order[num_dominant_seqs::])\n",
    "else:\n",
    "    dominant = list(real_order)\n",
    "    other_ = []\n",
    "    \n",
    "# orderng amounts for mean ordering - this calculated for each pair in the chunk \n",
    "ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ordering of coactive?\n",
    "if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "    multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "else:\n",
    "    multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "# pull out sequence order for current mouse\n",
    "seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "real_order = np.array(seq_order)+1\n",
    "\n",
    "#deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "if not len(real_order) == num_dominant_seqs:\n",
    "    dominant = list(real_order[0:num_dominant_seqs])\n",
    "    other_ = list(real_order[num_dominant_seqs::])\n",
    "else:\n",
    "    dominant = list(real_order)\n",
    "    other_ = []\n",
    "    \n",
    "# orderng amounts for mean ordering - this calculated for each pair in the chunk \n",
    "ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)\n",
    "chunk_vars['meaned_order_task_related_ordered'] += [ordered]\n",
    "chunk_vars['meaned_order_task_related_misordered'] += [misordered]\n",
    "chunk_vars['meaned_order_task_related_other'] += [other]\n",
    "\n",
    "# orderng amounts for first spike ordering\n",
    "ordered,misordered,other = calculate_ordering_amounts(fs_order,dominant,other_)\n",
    "chunk_vars['fs_order_task_related_ordered'] += [ordered]\n",
    "chunk_vars['fs_order_task_related_misordered'] += [misordered]\n",
    "chunk_vars['fs_order_task_related_other'] += [other]\n",
    "### motif by motif:\n",
    "# does one motif appear more in coactive?\n",
    "if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "    all_motifs_total_coactive = all_motifs_proportion_coactive(multi_cluster_df)\n",
    "    chunk_vars['all_motifs_total_coactive'] += [all_motifs_total_coactive]\n",
    "else:\n",
    "    chunk_vars['all_motifs_total_coactive'] += [[0,0,0,0,0,0]]\n",
    "\n",
    "# does one motif appeaer more ordered? \n",
    "# # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "# meaned ordering \n",
    "all_motifs_meaned_ordering_task_related_ordered,all_motifs_meaned_ordering_task_related_misordered,all_motifs_meaned_ordering_task_related_other = motif_by_motif_ordering(meaned_order,real_order,dominant,other_)\n",
    "chunk_vars['meaned_ordering_all_motifs_task_related_ordered'] += [all_motifs_meaned_ordering_task_related_ordered]\n",
    "chunk_vars['meaned_ordering_all_motifs_task_related_misordered'] += [all_motifs_meaned_ordering_task_related_misordered]\n",
    "chunk_vars['meaned_ordering_all_motifs_task_related_other'] += [all_motifs_meaned_ordering_task_related_other]\n",
    "\n",
    "# first spike ordering \n",
    "all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other = motif_by_motif_ordering(fs_order,real_order,dominant,other_)\n",
    "chunk_vars['fs_ordering_all_motifs_task_related_ordered'] += [all_motifs_fs_task_related_ordered]\n",
    "chunk_vars['fs_ordering_all_motifs_task_related_misordered'] += [all_motifs_fs_task_related_misordered]\n",
    "chunk_vars['fs_ordering_all_motifs_task_related_other'] += [all_motifs_fs_task_related_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: 10    [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...\nName: cluster_spike_times, dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_chunk_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster_spike_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:109\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:83\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:74\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[1;32m---> 74\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:71\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     70\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string: 10    [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...\nName: cluster_spike_times, dtype: object"
     ]
    }
   ],
   "source": [
    "len(literal_eval(seq_chunk_data['cluster_spike_times']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0714285714285716, 1.18125, 1.2828282828282829, 0.6363636363636364]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_proportion_compared_to_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_reg_data[seq_reg_data. warp_factor > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_reg_data[seq_reg_data. warp_factor <= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3888888888888889"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ##################################### av. spikes involved\n",
    "            chunk_vars['mean_spikes_per_event'] += [[len(item) for item in filtered_chunk_data.cluster_spike_times]]\n",
    "            # per motif\n",
    "            motif_by_motif_mean_spikes_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_spikes_per_event += [[len(item) for item in motif_data.cluster_spike_times]]\n",
    "            chunk_vars['motif_by_motif_mean_spikes_per_event'] += [motif_by_motif_mean_spikes_per_event]  \n",
    "                    \n",
    "            ########################################## average units involved \n",
    "            chunk_vars['mean_units_per_event'] += [[len(np.unique(ast.literal_eval(item))) for item in filtered_chunk_data.cluster_neurons]]\n",
    "            motif_by_motif_mean_units_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_units_per_event += [[len(np.unique(ast.literal_eval(item))) for item in motif_data.cluster_neurons]]\n",
    "            chunk_vars['motif_by_motif_mean_units_per_event'] += [motif_by_motif_mean_units_per_event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_seq_event_rate_per_min = seq_event_per_min/sum(seq_event_per_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(nan)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_per_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_42616\\2415183231.py:3: RuntimeWarning: Mean of empty slice\n",
      "  seq_event_lens += [np.nanmean([item for sublist in [item[i] for item in chunk_event_lens] for item in sublist])]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(nan),\n",
       " np.float64(0.12021244979920481),\n",
       " np.float64(nan),\n",
       " np.float64(0.027006666666651784)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for sublist in [item[i] for item in chunk_event_lens] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_event_lens[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(nan),\n",
       " np.float64(0.12021244979920481),\n",
       " np.float64(nan),\n",
       " np.float64(0.027006666666651784)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.0531999999999754),\n",
       " np.float64(0.0142000000000166),\n",
       " np.float64(0.0228999999999928),\n",
       " np.float64(0.0275000000000034),\n",
       " np.float64(0.0101999999999975),\n",
       " np.float64(0.0584999999999809),\n",
       " np.float64(0.0175999999999874),\n",
       " np.float64(0.0537000000000205),\n",
       " np.float64(0.0224000000000046),\n",
       " np.float64(0.0378000000000042),\n",
       " np.float64(0.0768999999999664),\n",
       " np.float64(0.0511999999999943),\n",
       " np.float64(0.0912999999999897),\n",
       " np.float64(0.1779999999999972),\n",
       " np.float64(0.0810999999999921),\n",
       " np.float64(0.0736999999999739),\n",
       " np.float64(0.0464000000000055),\n",
       " np.float64(0.0212000000000216),\n",
       " np.float64(0.0372999999999592),\n",
       " np.float64(0.031499999999994),\n",
       " np.float64(0.1111000000000217),\n",
       " np.float64(0.016900000000021),\n",
       " np.float64(0.0271999999999934),\n",
       " np.float64(0.0158000000000129),\n",
       " np.float64(0.0059999999999718),\n",
       " np.float64(0.1580000000000154),\n",
       " np.float64(0.0815000000000054),\n",
       " np.float64(0.0780000000000313),\n",
       " np.float64(0.0147999999999797),\n",
       " np.float64(0.0375999999999976),\n",
       " np.float64(0.0280999999999949),\n",
       " np.float64(0.0262999999999919),\n",
       " np.float64(0.0237000000000193),\n",
       " np.float64(0.0219999999999913),\n",
       " np.float64(0.0620000000000118),\n",
       " np.float64(0.0371000000000094),\n",
       " np.float64(0.0258000000000038),\n",
       " np.float64(0.032100000000014),\n",
       " np.float64(0.1311999999999784),\n",
       " np.float64(0.1332999999999629),\n",
       " np.float64(0.0561000000000149),\n",
       " np.float64(0.2062999999999988),\n",
       " np.float64(0.1087999999999738),\n",
       " np.float64(0.0156000000000062),\n",
       " np.float64(0.1582000000000221),\n",
       " np.float64(0.1193000000000097),\n",
       " np.float64(0.1193000000000097),\n",
       " np.float64(0.1456000000000017),\n",
       " np.float64(0.010499999999979),\n",
       " np.float64(0.0058999999999969),\n",
       " np.float64(0.1538000000000465),\n",
       " np.float64(0.1433999999999855),\n",
       " np.float64(0.0838999999999714),\n",
       " np.float64(0.0867999999999824),\n",
       " np.float64(0.0520999999999958),\n",
       " np.float64(0.0274),\n",
       " np.float64(0.0435999999999694),\n",
       " np.float64(0.0596000000000458),\n",
       " np.float64(0.1582999999999401),\n",
       " np.float64(0.1046000000000049),\n",
       " np.float64(0.0854000000000496),\n",
       " np.float64(0.0730999999999539),\n",
       " np.float64(0.1480000000000245),\n",
       " np.float64(0.0208999999999832),\n",
       " np.float64(0.1388000000000602),\n",
       " np.float64(0.1023999999999887),\n",
       " np.float64(0.1317000000000234),\n",
       " np.float64(0.0965999999999667),\n",
       " np.float64(0.0236999999999625),\n",
       " np.float64(0.0248000000000274),\n",
       " np.float64(0.0311000000000376),\n",
       " np.float64(0.0658000000000811),\n",
       " np.float64(0.0878999999999905),\n",
       " np.float64(0.0965999999999667),\n",
       " np.float64(0.0670000000000072),\n",
       " np.float64(0.041300000000092),\n",
       " np.float64(0.043700000000058),\n",
       " np.float64(0.129099999999994),\n",
       " np.float64(0.0942000000000007),\n",
       " np.float64(0.1669000000000551),\n",
       " np.float64(0.0855999999999994),\n",
       " np.float64(0.0386999999999488),\n",
       " np.float64(0.0833999999999832),\n",
       " np.float64(0.5354999999999563),\n",
       " np.float64(0.0986000000000331),\n",
       " np.float64(0.1539000000000214),\n",
       " np.float64(0.0053000000000338),\n",
       " np.float64(0.0587999999997919),\n",
       " np.float64(0.0993000000000847),\n",
       " np.float64(0.0866000000000895),\n",
       " np.float64(0.0418999999999414),\n",
       " np.float64(0.1591000000000804),\n",
       " np.float64(0.256800000000112),\n",
       " np.float64(0.125300000000152),\n",
       " np.float64(0.0327999999999519),\n",
       " np.float64(0.0498999999999796),\n",
       " np.float64(0.6065000000000964),\n",
       " np.float64(0.1545999999998457),\n",
       " np.float64(0.200800000000072),\n",
       " np.float64(0.0936000000001513),\n",
       " np.float64(0.0394000000001142),\n",
       " np.float64(0.0355000000001837),\n",
       " np.float64(0.1489999999998872),\n",
       " np.float64(0.0246999999999388),\n",
       " np.float64(0.0911000000000967),\n",
       " np.float64(0.1231000000000221),\n",
       " np.float64(0.0773999999998977),\n",
       " np.float64(0.4834000000000742),\n",
       " np.float64(0.0619999999998981),\n",
       " np.float64(0.0783999999998741),\n",
       " np.float64(0.1000999999998839),\n",
       " np.float64(0.1903000000002066),\n",
       " np.float64(0.0953999999999268),\n",
       " np.float64(0.0853999999999359),\n",
       " np.float64(0.0487000000000534),\n",
       " np.float64(0.0376000000001113),\n",
       " np.float64(0.0369000000000596),\n",
       " np.float64(0.2733000000000629),\n",
       " np.float64(0.4319000000000415),\n",
       " np.float64(0.1432000000002062),\n",
       " np.float64(0.1204000000000178),\n",
       " np.float64(0.1488999999999123),\n",
       " np.float64(0.2213999999999032),\n",
       " np.float64(0.1581999999998515),\n",
       " np.float64(0.3344999999999345),\n",
       " np.float64(0.5784000000001015),\n",
       " np.float64(0.1930999999999585),\n",
       " np.float64(0.1204000000000178),\n",
       " np.float64(0.1488999999999123),\n",
       " np.float64(0.2213999999999032),\n",
       " np.float64(0.1581999999998515),\n",
       " np.float64(0.3344999999999345),\n",
       " np.float64(0.5784000000001015),\n",
       " np.float64(0.1930999999999585),\n",
       " np.float64(0.0444999999999708),\n",
       " np.float64(0.0861999999999625),\n",
       " np.float64(0.2193999999999505),\n",
       " np.float64(0.2319999999999709),\n",
       " np.float64(0.0415000000000418),\n",
       " np.float64(0.0733000000000174),\n",
       " np.float64(0.0205999999998311),\n",
       " np.float64(0.1259000000000014),\n",
       " np.float64(0.0288000000000465),\n",
       " np.float64(0.1245000000001255),\n",
       " np.float64(0.1140000000000327),\n",
       " np.float64(0.1501000000000658),\n",
       " np.float64(0.0630000000001018),\n",
       " np.float64(0.1251999999999498),\n",
       " np.float64(0.1275000000000545),\n",
       " np.float64(0.3152000000000043),\n",
       " np.float64(0.0262000000000171),\n",
       " np.float64(0.0234000000000378),\n",
       " np.float64(0.1475000000000363),\n",
       " np.float64(0.019399999999905),\n",
       " np.float64(0.0209999999999581),\n",
       " np.float64(0.2235000000000582),\n",
       " np.float64(0.3517999999999119),\n",
       " np.float64(0.3517999999999119),\n",
       " np.float64(0.501299999999901),\n",
       " np.float64(0.0361000000000331),\n",
       " np.float64(0.0237999999999374),\n",
       " np.float64(0.058999999999969),\n",
       " np.float64(0.0184999999999035),\n",
       " np.float64(0.0277000000000953),\n",
       " np.float64(0.0907999999999447),\n",
       " np.float64(0.0715999999999894),\n",
       " np.float64(0.0220999999999094),\n",
       " np.float64(0.0202999999999065),\n",
       " np.float64(0.1224999999999454),\n",
       " np.float64(0.1225000000001728),\n",
       " np.float64(0.1503999999999905),\n",
       " np.float64(0.1054000000001451),\n",
       " np.float64(0.1157000000000607),\n",
       " np.float64(0.4699000000000524),\n",
       " np.float64(0.0465000000001509),\n",
       " np.float64(0.0176000000001295),\n",
       " np.float64(0.0819000000001324),\n",
       " np.float64(0.1490999999998621),\n",
       " np.float64(0.0975000000000818),\n",
       " np.float64(0.369799999999941),\n",
       " np.float64(0.0289000000000214),\n",
       " np.float64(0.0979999999999563),\n",
       " np.float64(0.0936999999998988),\n",
       " np.float64(0.1756999999997788),\n",
       " np.float64(0.0886000000000422),\n",
       " np.float64(0.1255000000001018),\n",
       " np.float64(0.0172999999999774),\n",
       " np.float64(0.1014999999999872),\n",
       " np.float64(0.0316000000000258),\n",
       " np.float64(0.3821000000000367),\n",
       " np.float64(0.0475999999998748),\n",
       " np.float64(0.0502000000001316),\n",
       " np.float64(0.2530999999999039),\n",
       " np.float64(0.0530999999998584),\n",
       " np.float64(0.022200000000339),\n",
       " np.float64(0.1485999999999876),\n",
       " np.float64(0.1932999999999083),\n",
       " np.float64(0.0279999999997926),\n",
       " np.float64(0.1061000000004241),\n",
       " np.float64(0.1054000000001451),\n",
       " np.float64(0.1157000000000607),\n",
       " np.float64(0.4699000000000524),\n",
       " np.float64(0.0465000000001509),\n",
       " np.float64(0.0176000000001295),\n",
       " np.float64(0.0819000000001324),\n",
       " np.float64(0.1490999999998621),\n",
       " np.float64(0.0975000000000818),\n",
       " np.float64(0.369799999999941),\n",
       " np.float64(0.0289000000000214),\n",
       " np.float64(0.0979999999999563),\n",
       " np.float64(0.0936999999998988),\n",
       " np.float64(0.1756999999997788),\n",
       " np.float64(0.0886000000000422),\n",
       " np.float64(0.1255000000001018),\n",
       " np.float64(0.0172999999999774),\n",
       " np.float64(0.1014999999999872),\n",
       " np.float64(0.0316000000000258),\n",
       " np.float64(0.3821000000000367),\n",
       " np.float64(0.0475999999998748),\n",
       " np.float64(0.0502000000001316),\n",
       " np.float64(0.2530999999999039),\n",
       " np.float64(0.0530999999998584),\n",
       " np.float64(0.022200000000339),\n",
       " np.float64(0.1485999999999876),\n",
       " np.float64(0.1932999999999083),\n",
       " np.float64(0.0279999999997926),\n",
       " np.float64(0.1061000000004241),\n",
       " np.float64(0.0766000000003259),\n",
       " np.float64(0.1075000000000727),\n",
       " np.float64(0.5864999999998872),\n",
       " np.float64(0.0677000000000589),\n",
       " np.float64(0.8315999999999804),\n",
       " np.float64(0.0230999999998857),\n",
       " np.float64(0.1329000000000633),\n",
       " np.float64(0.1424000000001797),\n",
       " np.float64(0.1260999999999512),\n",
       " np.float64(0.0353000000000065),\n",
       " np.float64(0.1008999999999105),\n",
       " np.float64(0.0688999999997577),\n",
       " np.float64(0.0348999999996522),\n",
       " np.float64(0.009200000000419),\n",
       " np.float64(0.4668000000001484),\n",
       " np.float64(0.0347000000001571),\n",
       " np.float64(0.0185000000001309),\n",
       " np.float64(0.2443000000002939),\n",
       " np.float64(0.3778999999999541),\n",
       " np.float64(0.0277000000000953),\n",
       " np.float64(0.3022999999998319),\n",
       " np.float64(0.1104000000000269)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_chunk_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [np.float64(0.0531999999999754),\n",
       "  np.float64(0.0142000000000166),\n",
       "  np.float64(0.0228999999999928),\n",
       "  np.float64(0.0275000000000034),\n",
       "  np.float64(0.0101999999999975),\n",
       "  np.float64(0.0584999999999809),\n",
       "  np.float64(0.0175999999999874),\n",
       "  np.float64(0.0537000000000205),\n",
       "  np.float64(0.0224000000000046),\n",
       "  np.float64(0.0378000000000042),\n",
       "  np.float64(0.0768999999999664),\n",
       "  np.float64(0.0511999999999943),\n",
       "  np.float64(0.0912999999999897),\n",
       "  np.float64(0.1779999999999972),\n",
       "  np.float64(0.0810999999999921),\n",
       "  np.float64(0.0736999999999739),\n",
       "  np.float64(0.0464000000000055),\n",
       "  np.float64(0.0212000000000216),\n",
       "  np.float64(0.0372999999999592),\n",
       "  np.float64(0.031499999999994),\n",
       "  np.float64(0.1111000000000217),\n",
       "  np.float64(0.016900000000021),\n",
       "  np.float64(0.0271999999999934),\n",
       "  np.float64(0.0158000000000129),\n",
       "  np.float64(0.0059999999999718),\n",
       "  np.float64(0.1580000000000154),\n",
       "  np.float64(0.0815000000000054),\n",
       "  np.float64(0.0780000000000313),\n",
       "  np.float64(0.0147999999999797),\n",
       "  np.float64(0.0375999999999976),\n",
       "  np.float64(0.0280999999999949),\n",
       "  np.float64(0.0262999999999919),\n",
       "  np.float64(0.0237000000000193),\n",
       "  np.float64(0.0219999999999913),\n",
       "  np.float64(0.0620000000000118),\n",
       "  np.float64(0.0371000000000094),\n",
       "  np.float64(0.0258000000000038),\n",
       "  np.float64(0.032100000000014),\n",
       "  np.float64(0.1311999999999784),\n",
       "  np.float64(0.1332999999999629),\n",
       "  np.float64(0.0561000000000149),\n",
       "  np.float64(0.2062999999999988),\n",
       "  np.float64(0.1087999999999738),\n",
       "  np.float64(0.0156000000000062),\n",
       "  np.float64(0.1582000000000221),\n",
       "  np.float64(0.1193000000000097),\n",
       "  np.float64(0.1193000000000097),\n",
       "  np.float64(0.1456000000000017),\n",
       "  np.float64(0.010499999999979),\n",
       "  np.float64(0.0058999999999969),\n",
       "  np.float64(0.1538000000000465),\n",
       "  np.float64(0.1433999999999855),\n",
       "  np.float64(0.0838999999999714),\n",
       "  np.float64(0.0867999999999824),\n",
       "  np.float64(0.0520999999999958),\n",
       "  np.float64(0.0274),\n",
       "  np.float64(0.0435999999999694),\n",
       "  np.float64(0.0596000000000458),\n",
       "  np.float64(0.1582999999999401),\n",
       "  np.float64(0.1046000000000049),\n",
       "  np.float64(0.0854000000000496),\n",
       "  np.float64(0.0730999999999539),\n",
       "  np.float64(0.1480000000000245),\n",
       "  np.float64(0.0208999999999832),\n",
       "  np.float64(0.1388000000000602),\n",
       "  np.float64(0.1023999999999887),\n",
       "  np.float64(0.1317000000000234),\n",
       "  np.float64(0.0965999999999667),\n",
       "  np.float64(0.0236999999999625),\n",
       "  np.float64(0.0248000000000274),\n",
       "  np.float64(0.0311000000000376),\n",
       "  np.float64(0.0658000000000811),\n",
       "  np.float64(0.0878999999999905),\n",
       "  np.float64(0.0965999999999667),\n",
       "  np.float64(0.0670000000000072),\n",
       "  np.float64(0.041300000000092),\n",
       "  np.float64(0.043700000000058),\n",
       "  np.float64(0.129099999999994),\n",
       "  np.float64(0.0942000000000007),\n",
       "  np.float64(0.1669000000000551),\n",
       "  np.float64(0.0855999999999994),\n",
       "  np.float64(0.0386999999999488),\n",
       "  np.float64(0.0833999999999832)],\n",
       " [],\n",
       " [np.float64(0.0059000000000537),\n",
       "  np.float64(0.0984000000000264),\n",
       "  np.float64(0.0595000000000141),\n",
       "  np.float64(9.999999997489796e-05)]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_event_lens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_rpm\n",
    "chunk_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_type</th>\n",
       "      <th>regression_line</th>\n",
       "      <th>filt_rel_spike_times</th>\n",
       "      <th>slope</th>\n",
       "      <th>reactivation_ID</th>\n",
       "      <th>warp_factor</th>\n",
       "      <th>mouse</th>\n",
       "      <th>awake_rel_occurance_times</th>\n",
       "      <th>task_involved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4539199  0.51027892 0.4342718  0.48546027 0...</td>\n",
       "      <td>[0.0448 0.0557 0.041  0.0509 0.     0.0257 0.0...</td>\n",
       "      <td>5.170553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.443771</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.73537798 0.73537798 0.49940425 0.2811263  0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.62004643 0.39491982 0.37708801 0.23109006 0...</td>\n",
       "      <td>[0.     0.0202 0.0218 0.0349 0.0084 0.0204 0.0...</td>\n",
       "      <td>-11.144881</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.111981</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.87948413 0.30634616 0.30634616 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.86585369 0.69513829 0.3929259  0.42906835 0...</td>\n",
       "      <td>[0.     0.0222 0.0615 0.0568 0.0814 0.1038]</td>\n",
       "      <td>-7.689883</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.147243</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.87948413 0.21668602 0.23829101 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[ 0.49788529  0.20586617  0.19078031 -0.184211...</td>\n",
       "      <td>[0.     0.0271 0.0285 0.0633 0.0258 0.0096 0.0...</td>\n",
       "      <td>-10.775613</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-3.008870</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.04104176 0.         0.         0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.42389077 0.53777263 0.31965993 0.29263711 0...</td>\n",
       "      <td>[0.0059 0.     0.0113 0.0127 0.0246 0.0127 0.0...</td>\n",
       "      <td>-19.302009</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-5.389692</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.30634616 0.07390482 0.07390482 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.59548911 1.01292746 0.40594303 0.42396903 0...</td>\n",
       "      <td>[0.1806 0.5627 0.0071 0.0236 0.     0.0152 0.0...</td>\n",
       "      <td>1.092485</td>\n",
       "      <td>641.0</td>\n",
       "      <td>1.141124</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.9164957  0.9164957  0.1243793  0.02768056 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.15433603 0.91594778 0.17987242 0.16329616 0...</td>\n",
       "      <td>[0.17   0.     0.1643 0.168  0.1686 0.17   0.1...</td>\n",
       "      <td>-4.480069</td>\n",
       "      <td>644.0</td>\n",
       "      <td>-4.679531</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.45344187 0.9164957  0.02768056 0.02768056 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.5179733  0.33377115 0.86055995 0.85963789 0...</td>\n",
       "      <td>[0.8174 1.2569 0.     0.0022 0.0041 0.0065 0.1...</td>\n",
       "      <td>-0.419118</td>\n",
       "      <td>645.0</td>\n",
       "      <td>-0.437777</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.45344187 0.45344187 0.9164957  0.9164957  0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.68308293 0.43199044 0.46632788 0.2946407  0...</td>\n",
       "      <td>[0.1086 0.0384 0.048  0.     0.0222 0.0299 0.0...</td>\n",
       "      <td>3.576816</td>\n",
       "      <td>653.0</td>\n",
       "      <td>3.736063</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.9164957  0.08428787 0.02768056 0.53141627 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[ 0.94971534  0.90342477  0.87826009  0.090388...</td>\n",
       "      <td>[0.     0.0149 0.023  0.2766 0.2809 0.2902 0.3...</td>\n",
       "      <td>-3.106750</td>\n",
       "      <td>657.0</td>\n",
       "      <td>-3.245069</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.9164957  0.9164957  0.9164957  0.02768056 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      seq_type                                    regression_line  \\\n",
       "0          1.0  [0.4539199  0.51027892 0.4342718  0.48546027 0...   \n",
       "1          1.0  [0.62004643 0.39491982 0.37708801 0.23109006 0...   \n",
       "2          1.0  [0.86585369 0.69513829 0.3929259  0.42906835 0...   \n",
       "3          1.0  [ 0.49788529  0.20586617  0.19078031 -0.184211...   \n",
       "4          1.0  [0.42389077 0.53777263 0.31965993 0.29263711 0...   \n",
       "...        ...                                                ...   \n",
       "1201       6.0  [0.59548911 1.01292746 0.40594303 0.42396903 0...   \n",
       "1202       6.0  [0.15433603 0.91594778 0.17987242 0.16329616 0...   \n",
       "1203       6.0  [0.5179733  0.33377115 0.86055995 0.85963789 0...   \n",
       "1204       6.0  [0.68308293 0.43199044 0.46632788 0.2946407  0...   \n",
       "1205       6.0  [ 0.94971534  0.90342477  0.87826009  0.090388...   \n",
       "\n",
       "                                   filt_rel_spike_times      slope  \\\n",
       "0     [0.0448 0.0557 0.041  0.0509 0.     0.0257 0.0...   5.170553   \n",
       "1     [0.     0.0202 0.0218 0.0349 0.0084 0.0204 0.0... -11.144881   \n",
       "2           [0.     0.0222 0.0615 0.0568 0.0814 0.1038]  -7.689883   \n",
       "3     [0.     0.0271 0.0285 0.0633 0.0258 0.0096 0.0... -10.775613   \n",
       "4     [0.0059 0.     0.0113 0.0127 0.0246 0.0127 0.0... -19.302009   \n",
       "...                                                 ...        ...   \n",
       "1201  [0.1806 0.5627 0.0071 0.0236 0.     0.0152 0.0...   1.092485   \n",
       "1202  [0.17   0.     0.1643 0.168  0.1686 0.17   0.1...  -4.480069   \n",
       "1203  [0.8174 1.2569 0.     0.0022 0.0041 0.0065 0.1...  -0.419118   \n",
       "1204  [0.1086 0.0384 0.048  0.     0.0222 0.0299 0.0...   3.576816   \n",
       "1205  [0.     0.0149 0.023  0.2766 0.2809 0.2902 0.3...  -3.106750   \n",
       "\n",
       "      reactivation_ID  warp_factor    mouse  \\\n",
       "0                 0.0     1.443771  136_1_3   \n",
       "1                 9.0    -3.111981  136_1_3   \n",
       "2                10.0    -2.147243  136_1_3   \n",
       "3                11.0    -3.008870  136_1_3   \n",
       "4                17.0    -5.389692  136_1_3   \n",
       "...               ...          ...      ...   \n",
       "1201            641.0     1.141124  136_1_3   \n",
       "1202            644.0    -4.679531  136_1_3   \n",
       "1203            645.0    -0.437777  136_1_3   \n",
       "1204            653.0     3.736063  136_1_3   \n",
       "1205            657.0    -3.245069  136_1_3   \n",
       "\n",
       "                              awake_rel_occurance_times  task_involved  \n",
       "0     [0.73537798 0.73537798 0.49940425 0.2811263  0...            1.0  \n",
       "1     [0.87948413 0.87948413 0.30634616 0.30634616 0...            1.0  \n",
       "2     [0.87948413 0.87948413 0.21668602 0.23829101 0...            1.0  \n",
       "3     [0.87948413 0.04104176 0.         0.         0...            1.0  \n",
       "4     [0.87948413 0.30634616 0.07390482 0.07390482 0...            1.0  \n",
       "...                                                 ...            ...  \n",
       "1201  [0.9164957  0.9164957  0.1243793  0.02768056 0...            0.0  \n",
       "1202  [0.45344187 0.9164957  0.02768056 0.02768056 0...            0.0  \n",
       "1203  [0.45344187 0.45344187 0.9164957  0.9164957  0...            0.0  \n",
       "1204  [0.9164957  0.08428787 0.02768056 0.53141627 0...            0.0  \n",
       "1205  [0.9164957  0.9164957  0.9164957  0.02768056 0...            0.0  \n",
       "\n",
       "[1206 rows x 9 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.])], [array([1.])], [array([0.])], [array([0.1])]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_rpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_event_lens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.]), array([8.3]), array([0.]), array([0.4])],\n",
       " [array([0.]), array([5.85]), array([0.]), array([0.375])],\n",
       " [array([0.]), array([1.]), array([0.]), array([0.1])]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_rpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2629.961</td>\n",
       "      <td>2629.9265</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>2629.961</td>\n",
       "      <td>[2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...</td>\n",
       "      <td>[45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....</td>\n",
       "      <td>[ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_seq_type  num_spikes  num_neurons  first_spike_time  \\\n",
       "10    243                 2          20     2629.961         2629.9265   \n",
       "\n",
       "    event_length  last_spike_time  \\\n",
       "10        0.0345         2629.961   \n",
       "\n",
       "                                  cluster_spike_times  \\\n",
       "10  [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...   \n",
       "\n",
       "                                      cluster_neurons  \\\n",
       "10  [45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....   \n",
       "\n",
       "                                 spike_plotting_order  coactive_cluster_group  \\\n",
       "10  [ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...                   229.0   \n",
       "\n",
       "   ordering_classification  rem_events  nrem_events  \n",
       "10              sequential           0            0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>2596.9095</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>[2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...</td>\n",
       "      <td>[39.0, 102.0, 103.0, 104.0, 104.0, 104.0]</td>\n",
       "      <td>[84. 62. 61. 60. 60. 60.]</td>\n",
       "      <td>120.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>2610.7146</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>[2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...</td>\n",
       "      <td>[82.0, 96.0, 102.0, 104.0, 104.0, 107.0]</td>\n",
       "      <td>[33. 57. 62. 60. 60. 82.]</td>\n",
       "      <td>125.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>2650.6180</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>[2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...</td>\n",
       "      <td>[90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...</td>\n",
       "      <td>[68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>2654.9429</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>[2654.9429, 2654.9522, 2654.9514, 2654.958, 26...</td>\n",
       "      <td>[30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...</td>\n",
       "      <td>[67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]</td>\n",
       "      <td>145.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>2669.6668</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>[2669.6843, 2669.6847, 2669.678, 2669.6778, 26...</td>\n",
       "      <td>[90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...</td>\n",
       "      <td>[ 68.  58. 115.  64.  64.  56.  61.  60.]</td>\n",
       "      <td>149.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>2675.3538</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>[2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...</td>\n",
       "      <td>[30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...</td>\n",
       "      <td>[67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>2686.4794</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>[2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...</td>\n",
       "      <td>[40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...</td>\n",
       "      <td>[59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>2705.4055</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>[2705.418, 2705.4073, 2705.4135, 2705.4151, 27...</td>\n",
       "      <td>[82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...</td>\n",
       "      <td>[ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>2716.7483</td>\n",
       "      <td>0.3023</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>[2716.7483, 2716.9622, 2716.9666, 2716.986, 27...</td>\n",
       "      <td>[101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...</td>\n",
       "      <td>[56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]</td>\n",
       "      <td>166.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>2725.4187</td>\n",
       "      <td>0.1104</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>[2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...</td>\n",
       "      <td>[35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....</td>\n",
       "      <td>[ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>2629.9265</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>[2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...</td>\n",
       "      <td>[45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....</td>\n",
       "      <td>[ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>2610.2360</td>\n",
       "      <td>1.0350</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>[2610.3344, 2610.678, 2610.8732, 2611.271, 261...</td>\n",
       "      <td>[1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...</td>\n",
       "      <td>[ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>2700.6185</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>[2700.6185, 2700.685, 2700.7036, 2700.7084, 27...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...</td>\n",
       "      <td>159.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_seq_type  num_spikes  num_neurons  first_spike_time  \\\n",
       "0     120                 1           6    2596.9444         2596.9095   \n",
       "1     125                 1           6    2610.7238         2610.7146   \n",
       "2     143                 1          29    2651.0848         2650.6180   \n",
       "3     145                 1          11    2654.9776         2654.9429   \n",
       "4     149                 1           8    2669.6853         2669.6668   \n",
       "5     151                 1          13    2675.5981         2675.3538   \n",
       "6     155                 1          23    2686.8573         2686.4794   \n",
       "7     162                 1          12    2705.4332         2705.4055   \n",
       "8     166                 1          11    2717.0506         2716.7483   \n",
       "9     169                 1          36    2725.5291         2725.4187   \n",
       "10    243                 2          20    2629.9610         2629.9265   \n",
       "11    260                 3          22    2611.2710         2610.2360   \n",
       "12    264                 3          12    2700.7282         2700.6185   \n",
       "\n",
       "    event_length  last_spike_time  \\\n",
       "0         0.0349        2596.9444   \n",
       "1         0.0092        2610.7238   \n",
       "2         0.4668        2651.0848   \n",
       "3         0.0347        2654.9776   \n",
       "4         0.0185        2669.6853   \n",
       "5         0.2443        2675.5981   \n",
       "6         0.3779        2686.8573   \n",
       "7         0.0277        2705.4332   \n",
       "8         0.3023        2717.0506   \n",
       "9         0.1104        2725.5291   \n",
       "10        0.0345        2629.9610   \n",
       "11        1.0350        2611.2710   \n",
       "12        0.1097        2700.7282   \n",
       "\n",
       "                                  cluster_spike_times  \\\n",
       "0   [2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...   \n",
       "1   [2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...   \n",
       "2   [2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...   \n",
       "3   [2654.9429, 2654.9522, 2654.9514, 2654.958, 26...   \n",
       "4   [2669.6843, 2669.6847, 2669.678, 2669.6778, 26...   \n",
       "5   [2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...   \n",
       "6   [2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...   \n",
       "7   [2705.418, 2705.4073, 2705.4135, 2705.4151, 27...   \n",
       "8   [2716.7483, 2716.9622, 2716.9666, 2716.986, 27...   \n",
       "9   [2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...   \n",
       "10  [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...   \n",
       "11  [2610.3344, 2610.678, 2610.8732, 2611.271, 261...   \n",
       "12  [2700.6185, 2700.685, 2700.7036, 2700.7084, 27...   \n",
       "\n",
       "                                      cluster_neurons  \\\n",
       "0           [39.0, 102.0, 103.0, 104.0, 104.0, 104.0]   \n",
       "1            [82.0, 96.0, 102.0, 104.0, 104.0, 107.0]   \n",
       "2   [90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...   \n",
       "3   [30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...   \n",
       "4   [90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...   \n",
       "5   [30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...   \n",
       "6   [40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...   \n",
       "7   [82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...   \n",
       "8   [101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...   \n",
       "9   [35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....   \n",
       "10  [45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....   \n",
       "11  [1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...   \n",
       "12  [1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                 spike_plotting_order  coactive_cluster_group  \\\n",
       "0                           [84. 62. 61. 60. 60. 60.]                   120.0   \n",
       "1                           [33. 57. 62. 60. 60. 82.]                   125.0   \n",
       "2   [68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...                   143.0   \n",
       "3       [67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]                   145.0   \n",
       "4           [ 68.  58. 115.  64.  64.  56.  61.  60.]                   149.0   \n",
       "5   [67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...                   151.0   \n",
       "6   [59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...                   155.0   \n",
       "7   [ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...                   162.0   \n",
       "8       [56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]                   166.0   \n",
       "9   [ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...                   169.0   \n",
       "10  [ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...                   229.0   \n",
       "11  [ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...                   124.0   \n",
       "12  [ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...                   159.0   \n",
       "\n",
       "   ordering_classification  rem_events  nrem_events  \n",
       "0               sequential           0            0  \n",
       "1               sequential           0            0  \n",
       "2               sequential           1            0  \n",
       "3               sequential           1            0  \n",
       "4               sequential           1            0  \n",
       "5               sequential           1            0  \n",
       "6               sequential           1            0  \n",
       "7               sequential           0            0  \n",
       "8               sequential           0            0  \n",
       "9               sequential           0            0  \n",
       "10              sequential           0            0  \n",
       "11              sequential           0            0  \n",
       "12              sequential           0            0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10]),)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>2596.9095</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>[2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...</td>\n",
       "      <td>[39.0, 102.0, 103.0, 104.0, 104.0, 104.0]</td>\n",
       "      <td>[84. 62. 61. 60. 60. 60.]</td>\n",
       "      <td>120.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>2610.7146</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>[2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...</td>\n",
       "      <td>[82.0, 96.0, 102.0, 104.0, 104.0, 107.0]</td>\n",
       "      <td>[33. 57. 62. 60. 60. 82.]</td>\n",
       "      <td>125.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>2650.6180</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>[2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...</td>\n",
       "      <td>[90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...</td>\n",
       "      <td>[68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>2654.9429</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>[2654.9429, 2654.9522, 2654.9514, 2654.958, 26...</td>\n",
       "      <td>[30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...</td>\n",
       "      <td>[67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]</td>\n",
       "      <td>145.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>2669.6668</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>[2669.6843, 2669.6847, 2669.678, 2669.6778, 26...</td>\n",
       "      <td>[90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...</td>\n",
       "      <td>[ 68.  58. 115.  64.  64.  56.  61.  60.]</td>\n",
       "      <td>149.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>2675.3538</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>[2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...</td>\n",
       "      <td>[30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...</td>\n",
       "      <td>[67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>2686.4794</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>[2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...</td>\n",
       "      <td>[40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...</td>\n",
       "      <td>[59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>2705.4055</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>[2705.418, 2705.4073, 2705.4135, 2705.4151, 27...</td>\n",
       "      <td>[82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...</td>\n",
       "      <td>[ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>2716.7483</td>\n",
       "      <td>0.3023</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>[2716.7483, 2716.9622, 2716.9666, 2716.986, 27...</td>\n",
       "      <td>[101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...</td>\n",
       "      <td>[56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]</td>\n",
       "      <td>166.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>2725.4187</td>\n",
       "      <td>0.1104</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>[2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...</td>\n",
       "      <td>[35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....</td>\n",
       "      <td>[ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>2629.9265</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>[2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...</td>\n",
       "      <td>[45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....</td>\n",
       "      <td>[ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>2610.2360</td>\n",
       "      <td>1.0350</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>[2610.3344, 2610.678, 2610.8732, 2611.271, 261...</td>\n",
       "      <td>[1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...</td>\n",
       "      <td>[ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>2700.6185</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>[2700.6185, 2700.685, 2700.7036, 2700.7084, 27...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...</td>\n",
       "      <td>159.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_seq_type  num_spikes  num_neurons  first_spike_time  \\\n",
       "0     120                 1           6    2596.9444         2596.9095   \n",
       "1     125                 1           6    2610.7238         2610.7146   \n",
       "2     143                 1          29    2651.0848         2650.6180   \n",
       "3     145                 1          11    2654.9776         2654.9429   \n",
       "4     149                 1           8    2669.6853         2669.6668   \n",
       "5     151                 1          13    2675.5981         2675.3538   \n",
       "6     155                 1          23    2686.8573         2686.4794   \n",
       "7     162                 1          12    2705.4332         2705.4055   \n",
       "8     166                 1          11    2717.0506         2716.7483   \n",
       "9     169                 1          36    2725.5291         2725.4187   \n",
       "10    243                 2          20    2629.9610         2629.9265   \n",
       "11    260                 3          22    2611.2710         2610.2360   \n",
       "12    264                 3          12    2700.7282         2700.6185   \n",
       "\n",
       "    event_length  last_spike_time  \\\n",
       "0         0.0349        2596.9444   \n",
       "1         0.0092        2610.7238   \n",
       "2         0.4668        2651.0848   \n",
       "3         0.0347        2654.9776   \n",
       "4         0.0185        2669.6853   \n",
       "5         0.2443        2675.5981   \n",
       "6         0.3779        2686.8573   \n",
       "7         0.0277        2705.4332   \n",
       "8         0.3023        2717.0506   \n",
       "9         0.1104        2725.5291   \n",
       "10        0.0345        2629.9610   \n",
       "11        1.0350        2611.2710   \n",
       "12        0.1097        2700.7282   \n",
       "\n",
       "                                  cluster_spike_times  \\\n",
       "0   [2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...   \n",
       "1   [2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...   \n",
       "2   [2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...   \n",
       "3   [2654.9429, 2654.9522, 2654.9514, 2654.958, 26...   \n",
       "4   [2669.6843, 2669.6847, 2669.678, 2669.6778, 26...   \n",
       "5   [2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...   \n",
       "6   [2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...   \n",
       "7   [2705.418, 2705.4073, 2705.4135, 2705.4151, 27...   \n",
       "8   [2716.7483, 2716.9622, 2716.9666, 2716.986, 27...   \n",
       "9   [2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...   \n",
       "10  [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...   \n",
       "11  [2610.3344, 2610.678, 2610.8732, 2611.271, 261...   \n",
       "12  [2700.6185, 2700.685, 2700.7036, 2700.7084, 27...   \n",
       "\n",
       "                                      cluster_neurons  \\\n",
       "0           [39.0, 102.0, 103.0, 104.0, 104.0, 104.0]   \n",
       "1            [82.0, 96.0, 102.0, 104.0, 104.0, 107.0]   \n",
       "2   [90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...   \n",
       "3   [30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...   \n",
       "4   [90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...   \n",
       "5   [30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...   \n",
       "6   [40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...   \n",
       "7   [82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...   \n",
       "8   [101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...   \n",
       "9   [35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....   \n",
       "10  [45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....   \n",
       "11  [1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...   \n",
       "12  [1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                 spike_plotting_order  coactive_cluster_group  \\\n",
       "0                           [84. 62. 61. 60. 60. 60.]                   120.0   \n",
       "1                           [33. 57. 62. 60. 60. 82.]                   125.0   \n",
       "2   [68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...                   143.0   \n",
       "3       [67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]                   145.0   \n",
       "4           [ 68.  58. 115.  64.  64.  56.  61.  60.]                   149.0   \n",
       "5   [67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...                   151.0   \n",
       "6   [59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...                   155.0   \n",
       "7   [ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...                   162.0   \n",
       "8       [56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]                   166.0   \n",
       "9   [ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...                   169.0   \n",
       "10  [ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...                   229.0   \n",
       "11  [ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...                   124.0   \n",
       "12  [ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...                   159.0   \n",
       "\n",
       "   ordering_classification  rem_events  nrem_events  \n",
       "0               sequential           0            0  \n",
       "1               sequential           0            0  \n",
       "2               sequential           1            0  \n",
       "3               sequential           1            0  \n",
       "4               sequential           1            0  \n",
       "5               sequential           1            0  \n",
       "6               sequential           1            0  \n",
       "7               sequential           0            0  \n",
       "8               sequential           0            0  \n",
       "9               sequential           0            0  \n",
       "10              sequential           0            0  \n",
       "11              sequential           0            0  \n",
       "12              sequential           0            0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(11)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_id in seq_order:\n",
    "    np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(array([10]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:173\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(array([10]),)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfiltered_chunk_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_chunk_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_seq_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3824\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3824\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6072\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   6068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   6069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   6070\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   6071\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 6072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (array([10]),)"
     ]
    }
   ],
   "source": [
    "filtered_chunk_data[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 3, 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, mouse in enumerate(list(all_mice_dict)):\n",
    "    replay_data_found = False\n",
    "    print('----------')\n",
    "    print(mouse)\n",
    "    animals += [mouse]\n",
    "    print(index)\n",
    "    if '_final_analysis_output' in os.listdir(all_mice_dict[mouse]['sleep_path']):\n",
    "\n",
    "        # load in replay data \n",
    "        for run_index,pp_file in enumerate(os.listdir(dat_path)):\n",
    "            if mouse_file in pp_file:\n",
    "                print(mouse_file)\n",
    "                # set path to processed files \n",
    "                current_mouse_path = dat_path + pp_file + '\\\\_final_analysis_output'\n",
    "                print('replay data path found: ' + current_mouse_path)\n",
    "                replay_data_found = True\n",
    "                \n",
    "                # load in sleep start time \n",
    "                current_sleep_start = sleep_start[mouse]\n",
    "                params_file = dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json'\n",
    "                with open(params_file, 'r') as file:\n",
    "                    params = json.load(file)\n",
    "                time_spans = params['time_span']\n",
    "                            \n",
    "                break\n",
    "\n",
    "    if not replay_data_found:\n",
    "        print('No replay data found for ' + mouse_file)\n",
    "        continue\n",
    "    \n",
    "    # rpm\n",
    "    chunk_rpm= []\n",
    "    # event lengths\n",
    "    chunk_event_lens = []\n",
    "    # decay \n",
    "    chunk_binned_rate,chunk_bins_relative_so = [],[]\n",
    "    # coactive freqs\n",
    "    coactive_freqs_chunk  = {}\n",
    "    # ordered vs misordered\n",
    "    chunk_ordered_misordered_proportions = []\n",
    "    # task vs non task related events\n",
    "    nontask_task_chunk = []\n",
    "    # awake seq by seq neuron involvements\n",
    "    chunk_mouse_neuron_rel_awake_positions_reverse = []\n",
    "    chunk_mouse_neuron_rel_reverse_replay_positions = []\n",
    "    chunk_mouse_neuron_rel_awake_positions_forwards = []\n",
    "    chunk_mouse_neuron_rel_forward_replay_positions = []\n",
    "    \n",
    "    ## loop across all chunk files ################################\n",
    "    for chunk_number,file in enumerate(os.listdir(current_mouse_path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            current_data_path = current_mouse_path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(current_data_path + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(current_data_path + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            ###### FILTERING AND MASKING ##################################################################''\n",
    "            ## filter this data for sequential ordering\n",
    "            sequential_condition = data.ordering_classification == 'sequential'\n",
    "            # filter is set up so that any true will carry forward \n",
    "            filtered_chunk_data = data[sequential_condition].reset_index()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_ppseq_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\paper_submission\\post_sleep\\\\\"\n",
    "out_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\behaviour_to_replay\\processed_data\\\\\" \n",
    "\n",
    "useable_mirs  = [ '149_1_2']\n",
    "# done '136_1_3', '136_1_4', '149_1_1', '149_1_2', '149_1_3','178_1_4', '178_1_5', '178_1_6', '178_1_7','178_1_8','178_1_9', '178_2_1', '178_2_2', '178_2_4', '268_1_10','269_1_4',\n",
    "#['ap5R_1_1','ap5R_1_2','ap5R_1_3','seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11','seq008_1_3']\n",
    "\n",
    "# load in sleep time points\n",
    "sleep_time_point_df = pd.read_csv(sleep_ppseq_path + 'sleep_time_points.csv')\n",
    "# decide when sleep started\n",
    "sleep_start = {}\n",
    "for index,value in enumerate(sleep_time_point_df.approx_sleep_start.values):\n",
    "    mouse = sleep_time_point_df.mir.values[index]\n",
    "    sleep_start[mouse] = value\n",
    "# define mice/sessions in each group    \n",
    "expert_mice = sleep_time_point_df[sleep_time_point_df.group == 'expert'].mir.values\n",
    "hlesion_mice = sleep_time_point_df[sleep_time_point_df.group == 'h_lesion'].mir.values\n",
    "learning_mice = sleep_time_point_df[sleep_time_point_df.group == 'learning'].mir.values\n",
    "\n",
    "\n",
    "var_dict = {'expert':[],'hlesion':[],'learning' :[],'mirs':[],'current_sleep_start':[], 'time_spans':[]}\n",
    "\n",
    "#Load in seq order data \n",
    "sequence_order_df = pd.read_csv(sleep_ppseq_path+\"sequence_order.csv\")\n",
    "# get all the relevant path name and some other data \n",
    "current_mouse_path,var_dict = find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start)\n",
    "# loop across each mouse path:\n",
    "for loop_index, path in enumerate(current_mouse_path):\n",
    "    # create empty chunk vars dict\n",
    "    chunk_vars = empty_chunk_vars()\n",
    "    \n",
    "    print(f\"run index: {loop_index}, processing {var_dict['mirs'][loop_index]}\")\n",
    "\n",
    "    ## loop across all chunk files\n",
    "    for chunk_index, file in enumerate(os.listdir(path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            path_ = path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(path_ + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(path_ + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            # filter based on the sequential/rem-nrem conditions set above\n",
    "            filter_mask = make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only)\n",
    "            filtered_chunk_data = data[filter_mask].reset_index()\n",
    "            \n",
    "            # how many reactivations found\n",
    "            reactivations_found = len(filtered_chunk_data)\n",
    "            print(reactivations_found)\n",
    "            \n",
    "            ####################################### chunk rate per minute: (# this one depends on rem/nrem filter... )\n",
    "            mins = determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path_)\n",
    "            if mins > 0:\n",
    "                chunk_vars['chunk_reactivations'] += [reactivations_found] \n",
    "                chunk_vars['chunk_mins'] += [mins]     \n",
    "                \n",
    "            ####################################### replay rate per motif type\n",
    "            all_motif_type_reactivations = []\n",
    "            all_motif_type_reactivations_min = []\n",
    "            all_motif_type_relative_proportion = []\n",
    "            for seq_type in range(1,7):\n",
    "                motif_type_reactivations = [len(np.where(filtered_chunk_data.cluster_seq_type.values == seq_type)[0])][0]\n",
    "                all_motif_type_reactivations += [motif_type_reactivations]\n",
    "            chunk_vars['chunk_motif_type_reactivations'] += [all_motif_type_reactivations]\n",
    "            \n",
    "            ##################################### av. spikes involved\n",
    "            chunk_vars['mean_spikes_per_event'] += [[len(item) for item in filtered_chunk_data.cluster_spike_times]]\n",
    "            # per motif\n",
    "            motif_by_motif_mean_spikes_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_spikes_per_event += [[len(item) for item in motif_data.cluster_spike_times]]\n",
    "            chunk_vars['motif_by_motif_mean_spikes_per_event'] += [motif_by_motif_mean_spikes_per_event]  \n",
    "                    \n",
    "            ########################################## average units involved \n",
    "            chunk_vars['mean_units_per_event'] += [[len(np.unique(ast.literal_eval(item))) for item in filtered_chunk_data.cluster_neurons]]\n",
    "            motif_by_motif_mean_units_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_units_per_event += [[len(np.unique(ast.literal_eval(item))) for item in motif_data.cluster_neurons]]\n",
    "            chunk_vars['motif_by_motif_mean_units_per_event'] += [motif_by_motif_mean_units_per_event]\n",
    "            \n",
    "            ########################################### replay length overall \n",
    "            chunk_vars['chunk_event_lengths'] += [filtered_chunk_data.event_length.values]\n",
    "            \n",
    "            ########################################### replay length per motif \n",
    "            motif_event_lenghts = []\n",
    "            for i in range(1,7):\n",
    "                motif_event_lenghts += [filtered_chunk_data[filtered_chunk_data.cluster_seq_type == i].event_length.values]\n",
    "            chunk_vars['motif_event_lenghts'] += [motif_event_lenghts]\n",
    "            \n",
    "            ########################################### coactive rate overall\n",
    "            event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "            # refind the clusters\n",
    "            filtered_chunk_data  = refind_cluster_events(filtered_chunk_data,event_proximity_filter)\n",
    "            # how many single evtns coactivly paired? average coactive rate? proportion of global events coactive? \n",
    "            cocative_total,coactive_len_per_chunk,overall_total = coactive_rate(filtered_chunk_data)\n",
    "            chunk_vars['total_single_events_coacitvely_paired'] += [cocative_total]\n",
    "            chunk_vars['coactive_lenghts'] += [coactive_len_per_chunk]\n",
    "            chunk_vars['overall_total_coactive_or_single_cluster_events'] += [overall_total]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # ordering of coactive?\n",
    "            if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "                multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "            else:\n",
    "                multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "            # pull out sequence order for current mouse\n",
    "            seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "            num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "            real_order = np.array(seq_order)+1\n",
    "\n",
    "            #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "            if not len(real_order) == num_dominant_seqs:\n",
    "                dominant = list(real_order[0:num_dominant_seqs])\n",
    "                other_ = list(real_order[num_dominant_seqs::])\n",
    "            else:\n",
    "                dominant = list(real_order)\n",
    "                other_ = []\n",
    "                \n",
    "            # orderng amounts for mean ordering - this calculated for each pair in the chunk \n",
    "            ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)\n",
    "            chunk_vars['meaned_order_task_related_ordered'] += [ordered]\n",
    "            chunk_vars['meaned_order_task_related_misordered'] += [misordered]\n",
    "            chunk_vars['meaned_order_task_related_other'] += [other]\n",
    "            \n",
    "            # orderng amounts for first spike ordering\n",
    "            ordered,misordered,other = calculate_ordering_amounts(fs_order,dominant,other_)\n",
    "            chunk_vars['fs_order_task_related_ordered'] += [ordered]\n",
    "            chunk_vars['fs_order_task_related_misordered'] += [misordered]\n",
    "            chunk_vars['fs_order_task_related_other'] += [other]\n",
    "            ### motif by motif:\n",
    "            # does one motif appear more in coactive?\n",
    "            if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "                all_motifs_total_coactive = all_motifs_proportion_coactive(multi_cluster_df)\n",
    "                chunk_vars['all_motifs_total_coactive'] += [all_motifs_total_coactive]\n",
    "            else:\n",
    "                chunk_vars['all_motifs_total_coactive'] += [[0,0,0,0,0,0]]\n",
    "            \n",
    "            # does one motif appeaer more ordered? \n",
    "            # # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "            # meaned ordering \n",
    "            all_motifs_meaned_ordering_task_related_ordered,all_motifs_meaned_ordering_task_related_misordered,all_motifs_meaned_ordering_task_related_other = motif_by_motif_ordering(meaned_order,real_order,dominant,other_)\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_ordered'] += [all_motifs_meaned_ordering_task_related_ordered]\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_misordered'] += [all_motifs_meaned_ordering_task_related_misordered]\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_other'] += [all_motifs_meaned_ordering_task_related_other]\n",
    "            \n",
    "            # first spike ordering \n",
    "            all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other = motif_by_motif_ordering(fs_order,real_order,dominant,other_)\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_ordered'] += [all_motifs_fs_task_related_ordered]\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_misordered'] += [all_motifs_fs_task_related_misordered]\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_other'] += [all_motifs_fs_task_related_other]\n",
    "\n",
    "            ########################################### task related vs other rate\n",
    "            task_seqs = np.array(seq_order)+1\n",
    "            # mask each condition\n",
    "            mask = np.isin(filtered_chunk_data.cluster_seq_type.values, task_seqs)\n",
    "            opposite_mask = ~mask\n",
    "            task_related = filtered_chunk_data[mask]\n",
    "            non_task_related = filtered_chunk_data[opposite_mask]\n",
    "\n",
    "            #  task v nontask overallrate\n",
    "            chunk_vars['normalised_task_related_total'] += [len(task_related)/len(task_seqs)]\n",
    "            if len(task_seqs) == 6:\n",
    "                chunk_vars['normalised_non_task_related_total'] += [0]\n",
    "            else:\n",
    "                chunk_vars['normalised_non_task_related_total'] += [len(non_task_related)/(6-len(task_seqs))]\n",
    "            \n",
    "            ### extra stuff to add in:\n",
    "            \n",
    "            # same but motif by motif\n",
    "            \n",
    "            # number of spikes task related \n",
    "            # number of units task related\n",
    "            \n",
    "            # # coative rate \n",
    "            # task_related_number = 0\n",
    "            # non_task_related_number = 0\n",
    "            # for coactive_ in meaned_order:\n",
    "            #     for motif_item in coactive_:\n",
    "            #         if motif_item in task_seqs:\n",
    "            #             task_related_number += 1\n",
    "            #         else:\n",
    "            #             non_task_related_number += 1\n",
    "            # # make it relative:\n",
    "            # task_related_number = task_related_number/len(task_seqs) \n",
    "            # non_task_related_number = non_task_related_number/(6-len(task_seqs))\n",
    "            # proportion_coacitve_event_that_are_task_related = task_related_number/(task_related_number+non_task_related_number)\n",
    "            # chunk_vars['proportion_coacitve_event_that_are_task_related'] = proportion_coacitve_event_that_are_task_related\n",
    "            \n",
    "            # # motif coactive rate for task and non task \n",
    "            # # task\n",
    "            # task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "            # task_proportion_single_events_coacitvely_paired,task_av_coactive_len_per_chunk,task_proporiton_of_events_coactive = coactive_rate(task_related)\n",
    "            # # non task:\n",
    "            # non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "            # nontask_proportion_single_events_coacitvely_paired,nontask_av_coactive_len_per_chunk,nontask_proporiton_of_events_coactive = coactive_rate(non_task_related)\n",
    "\n",
    "            # replay length\n",
    "            #task v non task\n",
    "            # motif by motif  \n",
    "            # spikes involved\n",
    "            \n",
    "            # save out to newly made place\n",
    "            \n",
    "        ###now do averages for each chunk and save out to a new file\n",
    "        ########## Calculate averages across chunks/ combine across chunks for each data variable\n",
    "        out_vars = {}\n",
    "\n",
    "        # 1 overall event rate \n",
    "        if sum(chunk_vars['chunk_reactivations']) == 0:\n",
    "            out_vars['event_rpm'] = 0\n",
    "        else:\n",
    "            out_vars['event_rpm'] = sum(chunk_vars['chunk_reactivations'])/sum(chunk_vars['chunk_mins'])\n",
    "\n",
    "        # 2 motif by motif event rate \n",
    "        out_vars['motif_event_rpm'] = np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)/sum(chunk_vars['chunk_mins'])\n",
    "\n",
    "        # 3 motif by motif proportion of all events\n",
    "        out_vars['motif_relative_event_proportion'] = np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)/sum(np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0))\n",
    "\n",
    "        # 4 spikes per replay event\n",
    "        chunk_spikes_per_event = chunk_vars['mean_spikes_per_event']\n",
    "        spikes_per_event = [item for sublist in chunk_spikes_per_event for item in sublist]\n",
    "        out_vars['spikes_per_event'] = spikes_per_event\n",
    "\n",
    "        # 5 motif by motif spikes per replay event\n",
    "        motif_by_motif_spikes_per_event = []\n",
    "        for seq in range(1,7):\n",
    "            motif_spikes_per_event = []\n",
    "            for chunk_ in chunk_vars['motif_by_motif_mean_spikes_per_event']:\n",
    "                motif_spikes_per_event +=chunk_[seq-1]\n",
    "            motif_by_motif_spikes_per_event += [motif_spikes_per_event]\n",
    "        out_vars['motif_by_motif_spikes_per_event'] = motif_by_motif_spikes_per_event\n",
    "\n",
    "        # 6 units per event\n",
    "        chunk_units_per_event = chunk_vars['mean_units_per_event']\n",
    "        units_per_event = [item for sublist in chunk_units_per_event for item in sublist]\n",
    "        out_vars['units_per_event'] = units_per_event\n",
    "\n",
    "        # 7 motif by motif units per replay event\n",
    "        motif_by_motif_units_per_event = []\n",
    "        for seq in range(1,7):\n",
    "            motif_units_per_event = []\n",
    "            for chunk_ in chunk_vars['motif_by_motif_mean_units_per_event']:\n",
    "                motif_units_per_event +=chunk_[seq-1]\n",
    "            motif_by_motif_units_per_event += [motif_units_per_event]\n",
    "        out_vars['motif_by_motif_units_per_event'] = motif_by_motif_units_per_event\n",
    "\n",
    "        # 8 event lengths\n",
    "        chunk_event_lengths = chunk_vars['chunk_event_lengths']\n",
    "        event_lengths = [item for sublist in chunk_event_lengths for item in sublist]\n",
    "        out_vars['event_lengths'] = event_lengths\n",
    "\n",
    "        # 9 motif by motif event lengths\n",
    "        motif_by_motif_event_lengths = []\n",
    "        for seq in range(1,7):\n",
    "            motif_event_lengths = []\n",
    "            for chunk_ in chunk_vars['motif_event_lenghts']:\n",
    "                motif_event_lengths +=list(chunk_[seq-1])\n",
    "            motif_by_motif_event_lengths += [motif_event_lengths]\n",
    "        out_vars['motif_by_motif_event_lengths'] = motif_by_motif_event_lengths\n",
    "\n",
    "        # 10 coactive rate (proportion of single events coacitvly paired)\n",
    "        if sum(chunk_vars['total_single_events_coacitvely_paired']) == 0:\n",
    "            out_vars['proportion_single_events_coactivly_paired'] = 0\n",
    "        else:\n",
    "            out_vars['proportion_single_events_coactivly_paired'] = sum(chunk_vars['total_single_events_coacitvely_paired'])/sum(chunk_vars['overall_total_coactive_or_single_cluster_events'])\n",
    "\n",
    "\n",
    "        # 11 number of motifs in each coative group\n",
    "        out_vars['coactive_group_lengths'] = [item for sublist in chunk_vars['coactive_lenghts']  for item in sublist]\n",
    "\n",
    "        # 12 meaned order\n",
    "        # ordered proporiton out of all \n",
    "        ordered = sum(chunk_vars['meaned_order_task_related_ordered'])\n",
    "        misordered = sum(chunk_vars['meaned_order_task_related_misordered'])\n",
    "        other = sum(chunk_vars['meaned_order_task_related_other'])\n",
    "        if ordered+misordered+other == 0:\n",
    "            out_vars['meaned_order_overall_ordered_prop'] = 0\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            out_vars['meaned_order_task_related_ordered_prop'] = 0\n",
    "            # other proportion\n",
    "            out_vars['meaned_order_overall_other_prop'] = 0\n",
    "        else:\n",
    "            out_vars['meaned_order_overall_ordered_prop'] = ordered/(ordered+misordered+other)\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            if ordered+misordered == 0:\n",
    "                out_vars['meaned_order_task_related_ordered_prop'] = 0\n",
    "            else:\n",
    "                out_vars['meaned_order_task_related_ordered_prop'] = ordered/(ordered+misordered)\n",
    "            # other proportion\n",
    "            out_vars['meaned_order_overall_other_prop'] = other/(ordered+misordered+other)\n",
    "\n",
    "        # 13 first spike order\n",
    "        # ordered proporiton out of all \n",
    "        ordered = sum(chunk_vars['fs_order_task_related_ordered'])\n",
    "        misordered = sum(chunk_vars['fs_order_task_related_misordered'])\n",
    "        other = sum(chunk_vars['fs_order_task_related_other'])\n",
    "        if ordered+misordered+other == 0:\n",
    "            out_vars['fs_order_overall_ordered_prop'] = 0\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            out_vars['fs_order_task_related_ordered_prop'] = 0\n",
    "            # other proportion\n",
    "            out_vars['fs_order_overall_other_prop'] = 0\n",
    "        else:\n",
    "            out_vars['fs_order_overall_ordered_prop'] = ordered/(ordered+misordered+other)\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            if ordered+misordered == 0:\n",
    "                out_vars['fs_order_task_related_ordered_prop'] = 0\n",
    "            else:\n",
    "                out_vars['fs_order_task_related_ordered_prop'] = ordered/(ordered+misordered)\n",
    "            # other proportion\n",
    "            out_vars['fs_order_overall_other_prop'] = other/(ordered+misordered+other)\n",
    "\n",
    "        # 14 motif by motif proportion coactive, what proprotion of each motif is coactive\n",
    "        out_vars['motif_proportion_coactive'] = np.sum(chunk_vars['all_motifs_total_coactive'],axis = 0)/np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)\n",
    "\n",
    "        # 15 does one motif appeaer more ordered? Proportion of moitfs that were in ordered events for coactive task related motifs\n",
    "        # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "        ordered_prop = []\n",
    "        for seq in range(1,7):\n",
    "            ordered = []\n",
    "            misordered = []\n",
    "            other  = []\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_ordered']:\n",
    "                ordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_misordered']:\n",
    "                misordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_other']:\n",
    "                other += [chunk[seq-1]]\n",
    "            if not 'nan' in ordered and (sum(ordered)+sum(misordered)+sum(other)) > 0:\n",
    "                ordered_prop += [sum(ordered)/(sum(ordered)+sum(misordered)+sum(other))]\n",
    "            else:\n",
    "                ordered_prop += ['nan']\n",
    "        out_vars['motif_meaned_ordering_ordered_prop_out_of_all_task_related'] = ordered_prop\n",
    "\n",
    "        # 16 same but for first spike ordering\n",
    "        ordered_prop = []\n",
    "        for seq in range(1,7):\n",
    "            ordered = []\n",
    "            misordered = []\n",
    "            other  = []\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_ordered']:\n",
    "                ordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_misordered']:\n",
    "                misordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_other']:\n",
    "                other += [chunk[seq-1]]\n",
    "            if not 'nan' in ordered and (sum(ordered)+sum(misordered)+sum(other)) > 0:\n",
    "                ordered_prop += [sum(ordered)/(sum(ordered)+sum(misordered)+sum(other))]\n",
    "            else:\n",
    "                ordered_prop += ['nan']\n",
    "        out_vars['motif_fs_ordering_ordered_prop_out_of_all_task_related'] = ordered_prop\n",
    "        \n",
    "        if sum(chunk_vars['normalised_non_task_related_total']) == 0:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = sum(chunk_vars['normalised_task_related_total'])\n",
    "        elif sum(chunk_vars['normalised_task_related_total']) == 0:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = 0\n",
    "        else:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = sum(chunk_vars['normalised_task_related_total'])/sum(chunk_vars['normalised_non_task_related_total'])\n",
    "\n",
    "\n",
    "        ####### SAVE OUT THE DATA \n",
    "\n",
    "        try: \n",
    "            int(var_dict['mirs'][loop_index].split('_')[0])\n",
    "            current_save_path = out_path + 'EJT' + var_dict['mirs'][loop_index] + '\\\\' + save_var + '\\\\'\n",
    "        except:\n",
    "            current_save_path = out_path + var_dict['mirs'][loop_index] + '\\\\' + save_var + '\\\\'\n",
    "            \n",
    "\n",
    "        ## if the path doesnt exist, make a new dir \n",
    "        if not os.path.exists(current_save_path):\n",
    "            os.makedirs(current_save_path)\n",
    "            \n",
    "        # convert all np arrays to list\n",
    "        for key, value in out_vars.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                out_vars[key] = value.tolist()\n",
    "\n",
    "        ### save out the out_vars dict\n",
    "        with open(current_save_path + 'replay_data_variables.json', 'w') as file:\n",
    "            json.dump(out_vars, file)\n",
    "            \n",
    "        # convert all np arrays to list\n",
    "        for key, value in var_dict.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                var_dict[key] = value.tolist()\n",
    "            \n",
    "        ### save out the var_dict \n",
    "        with open(current_save_path + 'general_mouse_info.json', 'w') as file:\n",
    "            json.dump(var_dict, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_mouse_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
