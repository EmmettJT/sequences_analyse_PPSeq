{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6782dc6f",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78aeab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial import KDTree\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "def load_in_behavioural_data(current_path):\n",
    "\n",
    "    day2_tracking = False\n",
    "    port_tracking2,movement_tracking2,port_tracking1,movement_tracking1 = None, None, None, None\n",
    "    \n",
    "    for day_folder in os.listdir(current_path):\n",
    "        day_path = os.path.join(current_path, day_folder)\n",
    "        if 'day2' in day_path:\n",
    "            print(f'Processing {day_folder}')\n",
    "            for file in os.listdir(day_path):\n",
    "                if file.endswith('.csv'):\n",
    "                    poke_df_2 = pd.read_csv(os.path.join(day_path,file))\n",
    "\n",
    "                if file.endswith('.h5'):\n",
    "                    print('day 2 tracking found')\n",
    "                    day2_tracking = True\n",
    "                    # tracking exists\n",
    "                    if 'port' in file or 'PORT' in file:\n",
    "                        port_tracking2 = get_dlc_data(os.path.join(day_path, file),interp = True,val = 0.9995)\n",
    "                    else:\n",
    "                        movement_tracking2 = get_dlc_data(os.path.join(day_path, file),interp = True,val = 0.9995)  \n",
    "                        print('--------------')                      \n",
    "    for day_folder in os.listdir(current_path):\n",
    "        day_path = os.path.join(current_path, day_folder)\n",
    "        if 'day1' in day_path:\n",
    "            print(f'Processing {day_folder}')\n",
    "            for file in os.listdir(day_path):\n",
    "                if file.endswith('.csv'):\n",
    "                    poke_df_1 = pd.read_csv(os.path.join(day_path,file))\n",
    "                if day2_tracking:\n",
    "                    # if day 2 tracking exists, then we also load in the day 1 tracking\n",
    "                    if file.endswith('.h5'):\n",
    "                        print('day 1 tracking found')\n",
    "                        # tracking exists\n",
    "                        if 'port' in file or 'PORT' in file:\n",
    "                            port_tracking1 = get_dlc_data(os.path.join(day_path, file),interp = True,val = 0.9995)\n",
    "                        else:\n",
    "                            movement_tracking1 = get_dlc_data(os.path.join(day_path, file),interp = True,val = 0.9995)\n",
    "                            print('--------------') \n",
    "\n",
    "    return poke_df_1, movement_tracking1, port_tracking1, poke_df_2, movement_tracking2, port_tracking2\n",
    "\n",
    "\n",
    "def get_dlc_data(Tracking_data_path,interp,val):\n",
    "    # Load in '.h5' file:\n",
    "    h5_read=pd.read_hdf(Tracking_data_path)\n",
    "    # Access the head center      \n",
    "    scorer =  h5_read.columns.tolist()[0][0]\n",
    "\n",
    "    colum_headings = h5_read[scorer].columns\n",
    "    bodyparts = np.unique([item[0] for item in colum_headings])\n",
    "\n",
    "    output = {}\n",
    "    for name in bodyparts:\n",
    "        dat_ =  h5_read[scorer][name]\n",
    "        if interp:\n",
    "            dat_interped=clean_and_interpolate(dat_,val)\n",
    "        output[name] =[dat_interped]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_and_interpolate(data,threshold):\n",
    "\n",
    "    bad_confidence_inds = np.where(data.likelihood.values<threshold)[0]\n",
    "    newx = data.x.values\n",
    "    newx[bad_confidence_inds] = 0\n",
    "    newy = data.y.values\n",
    "    newy[bad_confidence_inds] = 0\n",
    "\n",
    "    start_value_cleanup(newx)\n",
    "    interped_x = interp_0_coords(newx)\n",
    "\n",
    "    start_value_cleanup(newy)\n",
    "    interped_y = interp_0_coords(newy)\n",
    "    \n",
    "    data['interped_x'] = interped_x\n",
    "    data['interped_y'] = interped_y\n",
    "    \n",
    "    return data\n",
    "\n",
    "def start_value_cleanup(coords):\n",
    "    # This is for when the starting value of the coords == 0; interpolation will not work on these coords until the first 0 \n",
    "    #is changed. The 0 value is changed to the first non-zero value in the coords lists\n",
    "    for index, value in enumerate(coords):\n",
    "        working = 0\n",
    "        if value > 0:\n",
    "            start_value = value\n",
    "            start_index = index\n",
    "            working = 1\n",
    "            break\n",
    "    if working == 1:\n",
    "        for x in range(start_index):\n",
    "            coords[x] = start_value\n",
    "            \n",
    "def interp_0_coords(coords_list):\n",
    "    #coords_list is one if the outputs of the get_x_y_data = a list of co-ordinate points\n",
    "    for index, value in enumerate(coords_list):\n",
    "        if value == 0:\n",
    "            if coords_list[index-1] > 0:\n",
    "                value_before = coords_list[index-1]\n",
    "                interp_start_index = index-1\n",
    "                \n",
    "\n",
    "        if index < len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                if coords_list[index+1] > 0:\n",
    "                    interp_end_index = index+1\n",
    "                    value_after = coords_list[index+1]\n",
    "                   \n",
    "\n",
    "                    #now code to interpolate over the values\n",
    "                    try:\n",
    "                        interp_diff_index = interp_end_index - interp_start_index\n",
    "                    except UnboundLocalError:\n",
    "#                         print('the first value in list is 0, use the function start_value_cleanup to fix')\n",
    "                        break\n",
    "                 \n",
    "\n",
    "                    new_values = np.linspace(value_before, value_after, interp_diff_index)\n",
    "                    #print(new_values)\n",
    "\n",
    "                    interp_index = interp_start_index+1\n",
    "                    for x in range(interp_diff_index):\n",
    "                        \n",
    "                        coords_list[interp_index] = new_values[x]\n",
    "                        interp_index +=1\n",
    "        if index == len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                for x in range(30):\n",
    "                    coords_list[index-x] = coords_list[index-30]\n",
    "                    #print('')\n",
    "    return(coords_list)\n",
    "\n",
    "def find_awake_ppseq_base_path(mir,awake_ppseq_path):\n",
    "    awake_ppseq_mirs = np.array(['_'.join(item.split('_')[0:3]) for item in os.listdir(awake_ppseq_path)])\n",
    "    awake_file_mir = None\n",
    "    for ind,item in enumerate(awake_ppseq_mirs):\n",
    "        if item in mir:\n",
    "            awake_file_mir = os.listdir(awake_ppseq_path)[ind]\n",
    "    if awake_file_mir == None:\n",
    "        raise Exception(\"No awake file found for mir\")\n",
    "    else:\n",
    "        return(os.path.join(awake_ppseq_path,awake_file_mir))\n",
    "    \n",
    "def process_awake_ppseq_path(m_i_r, expert_awake_ppseq_path, learning_awake_ppseq_path):\n",
    "    try: \n",
    "        awake_ppseq_base_path = find_awake_ppseq_base_path(m_i_r,expert_awake_ppseq_path)\n",
    "    except:\n",
    "        awake_ppseq_base_path = find_awake_ppseq_base_path(m_i_r,learning_awake_ppseq_path)\n",
    "    print(f'Awake PPSeq base path: {awake_ppseq_base_path}')\n",
    "    return awake_ppseq_base_path\n",
    "\n",
    "def get_sequence_regions(mir,awake_ppseq_base_path,sequence_order):\n",
    "    standard_space_path = awake_ppseq_base_path + r'//analysis_output/reordered_recolored/fixed_standard_space//'\n",
    "    overlap_positions_standard_space = np.load(standard_space_path + 'overlap_positions_standard_space.npy',allow_pickle=True)\n",
    "\n",
    "    mir_row = None\n",
    "    for ind, row in sequence_order.iterrows():\n",
    "        if row.mir in mir:\n",
    "            mir_row = row\n",
    "    seq_order = literal_eval(mir_row.seq_order)\n",
    "    continuous_regions = []\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8, 2))\n",
    "    for i,seq in enumerate(seq_order):\n",
    "        ax.plot(overlap_positions_standard_space[seq],np.ones(len(overlap_positions_standard_space[seq]))*i,'o')\n",
    "        continuous_regions += [find_largest_continuous_region(overlap_positions_standard_space[seq])]\n",
    "\n",
    "    # create new df \n",
    "    continuous_regions_df = pd.DataFrame(continuous_regions,columns=['start','end'])\n",
    "    continuous_regions_df['sequence'] = seq_order\n",
    "    return continuous_regions_df\n",
    "\n",
    "def find_largest_continuous_region(positions, max_gap=5, circular_max=100):\n",
    "    # Sort positions for easier processing\n",
    "    positions = sorted(positions)\n",
    "    \n",
    "    # Identify continuous regions\n",
    "    regions = []\n",
    "    current_region = [positions[0]]\n",
    "    \n",
    "    for i in range(1, len(positions)):\n",
    "        if positions[i] - positions[i - 1] < max_gap:\n",
    "            current_region.append(positions[i])\n",
    "        else:\n",
    "            regions.append(current_region)\n",
    "            current_region = [positions[i]]\n",
    "    \n",
    "    regions.append(current_region)  # Append the last region\n",
    "    \n",
    "    # Handle circular case (if the first and last regions can be merged)\n",
    "    if regions and len(regions) > 1:\n",
    "        first_region = regions[0]\n",
    "        last_region = regions[-1]\n",
    "        \n",
    "        if (circular_max - last_region[-1] + first_region[0]) < max_gap:\n",
    "            merged_region = last_region + first_region\n",
    "            regions = regions[1:-1]  # Remove first and last\n",
    "            regions.append(merged_region)\n",
    "    \n",
    "    # Find the largest region\n",
    "    largest_region = max(regions, key=lambda r: r[-1] - r[0] if r[-1] >= r[0] else (r[-1] + circular_max - r[0]))\n",
    "    \n",
    "    # Determine start and end points\n",
    "    start = largest_region[0]\n",
    "    end = largest_region[-1]\n",
    "    \n",
    "    return start, end\n",
    "\n",
    "# loop over and load in behavioural data from useable mirs\n",
    "def find_organised_path(mir,dat_path):\n",
    "    dat_path_2 = None\n",
    "    recording = None\n",
    "    print(mir)\n",
    "    for animal_implant in os.listdir(dat_path):\n",
    "        current_m_i = '_'.join([animal_implant.split('_')[0],animal_implant.split('_')[-1][-1]])\n",
    "        mi = '_'.join(mir.split('_')[0:-1])\n",
    "        if current_m_i == mi:\n",
    "            dat_path_2 = os.path.join(dat_path,animal_implant)\n",
    "            break\n",
    "    print(dat_path_2)\n",
    "    for ind,item in enumerate([record.split('ing')[-1].split('_')[0] for record in os.listdir(dat_path_2)]):\n",
    "        if item == mir.split('_')[-1]:\n",
    "            recording = os.listdir(dat_path_2)[ind]\n",
    "    full_org_dat_path = os.path.join(dat_path_2,recording)\n",
    "    print(full_org_dat_path)\n",
    "    return full_org_dat_path\n",
    "\n",
    "def make_transition_df(df):\n",
    "    transitions = []\n",
    "    in_in_Latency = []\n",
    "    for index in range(len(df.Port.values)):\n",
    "        if index < len(df.Port.values)-1:\n",
    "            transit = int(str(df.Port.values[index]) + str(df.Port.values[index+1]))\n",
    "            transitions += [transit]\n",
    "            in_in_diff = df.PokeIn_Time.values[index+1] - df.PokeIn_Time.values[index]\n",
    "            in_in_Latency += [in_in_diff]\n",
    "        \n",
    " \n",
    "    transit_df = pd.DataFrame({'Trial_id': df.Trial_id.values[0:-1], \n",
    "                                'Transition_type': transitions,\n",
    "                                'in_in_Latency': in_in_Latency,\n",
    "                                '2s_Time_Filter_in_in': list((np.array(in_in_Latency) <= 2).astype(int))})\n",
    "    return transit_df\n",
    "\n",
    "def find_error_rates(transition_sync_df): \n",
    "\n",
    "    Correct = [21,16,63,37,72]\n",
    "    Error = [23,24,25,26,27,28,12,13,14,15,17,18,61,62,64,65,67,68,31,32,34,35,36,38]\n",
    "    Neutral = [11,22,33,66,41,42,43,44,45,46,47,48,51,52,53,54,55,56,57,58,71,73,74,75,76,77,78,81,82,83,84,85,86,87,88]\n",
    "\n",
    "    filt_df = transition_sync_df[transition_sync_df.in_in_Latency < 2]\n",
    "\n",
    "    corrects = 0\n",
    "    errors = 0\n",
    "    neutral = 0\n",
    "    for transit in filt_df.Transition_type.values:\n",
    "        if transit in Correct:\n",
    "            corrects += 1\n",
    "        elif transit in Error:\n",
    "            errors += 1\n",
    "        elif transit in Neutral:\n",
    "            neutral += 1\n",
    "        else:\n",
    "            raise Exception(\"Transition type not found!\")\n",
    "    total = corrects + errors + neutral\n",
    "    return corrects/total,errors/total,neutral/total\n",
    "\n",
    "def get_perfect_sequence_score(transition_sync_df):\n",
    "    transitions = []\n",
    "    Tfilt = []\n",
    "    latency = []\n",
    "    for trial in transition_sync_df.groupby('Trial_id'):\n",
    "        trial_id = trial[0]\n",
    "        trial_df = trial[1]\n",
    "        transitions += [trial_df.Transition_type.values]\n",
    "        latency += [trial_df.in_in_Latency.values] \n",
    "        # time filter\n",
    "        Tfilt += [trial_df['2s_Time_Filter_in_in'].values]\n",
    "\n",
    "    # for each trial,remove transntions and latencies that were too long and split into reaminign time relevant fragments - but for both latency types, hence the loop\n",
    "    timesplitseqs = [] \n",
    "    timesplitlatencies = []\n",
    "    for trial_index,time_filter in enumerate(Tfilt):\n",
    "        start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "        split = parts(transitions[trial_index],list(np.array(start_end_inds)+1))\n",
    "        split2 = parts(time_filter,list(np.array(start_end_inds)+1))\n",
    "        TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "        timesplitseqs += [TfiltSplit]\n",
    "        # now do the same for the latency data\n",
    "        split3 = parts(latency[trial_index],list(np.array(start_end_inds)+1))\n",
    "        TfiltSplit_latencies = RemoveSlowSequences(split3,split2)\n",
    "        timesplitlatencies += [TfiltSplit_latencies]\n",
    "\n",
    "        \n",
    "    # for fragments in each trial,sort and trim so that seqs start at initiation port poke and then remove fragments that are too short. ie. remove any transitions sequences that dont inlcude the first port or are just a single transition.\n",
    "    processed_seqs,processed_latencies = aligntofirstpokeandremovesingletransits(timesplitseqs,timesplitlatencies)  ## use  timesplitlatencies[0] for Out to in Transition times \n",
    "\n",
    "    ## determine perfect sequences and correspondng training level and shaping parameters\n",
    "    trial_perfects = []\n",
    "    for trial_index,fragments in enumerate(processed_seqs):\n",
    "        perfect = []\n",
    "        for fragment in fragments:\n",
    "            if sequence_contains_sequence(fragment,[21, 16, 63, 37]):\n",
    "                perfect += [1]\n",
    "            else:\n",
    "                perfect += [0]\n",
    "        trial_perfects = trial_perfects + [perfect]  \n",
    "        \n",
    "    ## code if i jst want average from all fragments\n",
    "    # pefs = 0\n",
    "    # total = 0\n",
    "    # for item in trial_perfects:\n",
    "    #     pefs += sum(item)\n",
    "    #     total += len(item)  \n",
    "\n",
    "    # code if i want to average over each trial\n",
    "    trial_by_trial_p_score = []\n",
    "    for trial_ in trial_perfects:\n",
    "        if len(trial_) > 0:\n",
    "            trial_by_trial_p_score += [sum(trial_)/len(trial_)]\n",
    "            \n",
    "    return np.mean(trial_by_trial_p_score)\n",
    "\n",
    "def reward_rate(behav_sync_df):\n",
    "    # transitions per reward\n",
    "    behav_sync_df.Reward_Times.values\n",
    "    indices = np.where(~np.isnan(behav_sync_df.Reward_Times.values))[0]\n",
    "    transits_per_reward = indices[-1]/(len(indices)-1) #-1 to turn it into the number of transitions \n",
    "    # seconds per reward \n",
    "    mask = ~np.isnan(behav_sync_df.Reward_Times.values)\n",
    "    start_offset = behav_sync_df.Trial_Start[0]\n",
    "    last_reward_time = behav_sync_df[mask].Reward_Times.values[-1] - start_offset\n",
    "    seconds_per_reward = last_reward_time / len(indices)\n",
    "    return transits_per_reward,seconds_per_reward\n",
    "\n",
    "def sequence_contains_sequence(haystack_seq, needle_seq):\n",
    "    for i in range(0, len(haystack_seq) - len(needle_seq) + 1):\n",
    "        if needle_seq == haystack_seq[i:i+len(needle_seq)]:\n",
    "            return True\n",
    "    return False\n",
    "            \n",
    "def parts(list_, indices):\n",
    "    indices = [0]+indices+[len(list_)]\n",
    "    return [list_[v:indices[k+1]] for k, v in enumerate(indices[:-1])]\n",
    "\n",
    "def RemoveSlowSequences(split,split2):\n",
    "    timefiltered_split = []\n",
    "    for i,item in enumerate(split2):\n",
    "        if item[0] == 1:\n",
    "            timefiltered_split = timefiltered_split + [split[i]]\n",
    "    return timefiltered_split\n",
    "\n",
    "def aligntofirstpokeandremovesingletransits(timesplitseqs,timesplitlatencies):\n",
    "    \n",
    "    newseqs = []\n",
    "    newlatencies = []\n",
    "    # align to first poke:\n",
    "    for index_1,fragments in enumerate(timesplitseqs):\n",
    "        current_newseqs = []\n",
    "        current_newlatencies = []\n",
    "        count = -1\n",
    "        seqs = False\n",
    "        for index_2,sequence in enumerate(fragments):\n",
    "            for index_3,transit in enumerate(sequence):\n",
    "                if not str(transit)[0] == str(transit)[1]: # remove repeat pokes\n",
    "                    if str(transit)[0] == '2':\n",
    "                        seqs = True\n",
    "                        current_newseqs = current_newseqs + [[]]\n",
    "                        current_newlatencies = current_newlatencies + [[]]\n",
    "                        count = count + 1\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]\n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "                    elif seqs == True:\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]   \n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "            seqs = False\n",
    " \n",
    "        newseqs = newseqs + [current_newseqs]\n",
    "        newlatencies = newlatencies + [current_newlatencies]\n",
    "    return(newseqs,newlatencies)\n",
    "\n",
    "def find_average_curves(port_centroids,T1_start_ind,T1_end_ind,T2_start_ind,T2_end_ind,T3_start_ind,T3_end_ind,T4_start_ind,T4_end_ind,current_x,current_y,buffer,radius):\n",
    "\n",
    "    ## add to the start and end of each to ensure that the segmetns overlap - this is impotant for joining them into a representaitve continuous line \n",
    "    T1_start_ind = np.array(T1_start_ind) - buffer\n",
    "    T2_start_ind = np.array(T2_start_ind) - buffer\n",
    "    T3_start_ind = np.array(T3_start_ind) - buffer\n",
    "    T4_start_ind = np.array(T4_start_ind) - buffer\n",
    "    T1_end_ind = np.array(T1_end_ind) + buffer\n",
    "    T2_end_ind = np.array(T2_end_ind) + buffer\n",
    "    T3_end_ind = np.array(T3_end_ind) + buffer\n",
    "    T4_end_ind = np.array(T4_end_ind) + buffer\n",
    "\n",
    "\n",
    "    # plot these filtered trajectories:\n",
    "    nrow = 1 \n",
    "    ncol = 1\n",
    "    fig, axs = plt.subplots(nrow, ncol,figsize=(6, 4))\n",
    "    for ind, ax in enumerate(fig.axes):\n",
    "        for index,port_centroid in enumerate(port_centroids):\n",
    "            ## define rings around important ports: port 5, port2, port 3, port4\n",
    "            \n",
    "            c = ['blue','grey','blue','grey','blue']\n",
    "            circle1 = plt.Circle(port_centroid, radius, color=c[index], alpha = 0.2)\n",
    "            ax.add_patch(circle1)\n",
    "\n",
    "    segment1 = plot_and_create_xy_segments(T1_start_ind,T1_end_ind,ax,'red',current_x,current_y)\n",
    "    segment2 = plot_and_create_xy_segments(T2_start_ind,T2_end_ind,ax,'green',current_x,current_y)\n",
    "    segment3 = plot_and_create_xy_segments(T3_start_ind,T3_end_ind,ax,'blue',current_x,current_y)\n",
    "    segment4 = plot_and_create_xy_segments(T4_start_ind,T4_end_ind,ax,'grey',current_x,current_y)\n",
    "\n",
    "    a_curve1 = interpolate_to_longest_and_find_average_curve(segment1,'None')\n",
    "    a_curve2 = interpolate_to_longest_and_find_average_curve(segment2,'None')\n",
    "    a_curve3 = interpolate_to_longest_and_find_average_curve(segment3,'None')\n",
    "    a_curve4 = interpolate_to_longest_and_find_average_curve(segment4,'None')\n",
    "\n",
    "    x = [point[0] for point in a_curve1]\n",
    "    y = [point[1] for point in a_curve1]\n",
    "    ax.plot(x, y, '-', color ='black',alpha = 1)\n",
    "\n",
    "    x = [point[0] for point in a_curve2]\n",
    "    y = [point[1] for point in a_curve2]\n",
    "    ax.plot(x, y, '-', color ='black',alpha = 1)\n",
    "\n",
    "    x = [point[0] for point in a_curve3]\n",
    "    y = [point[1] for point in a_curve3]\n",
    "    ax.plot(x, y, '-', color ='black',alpha = 1)\n",
    "\n",
    "    x = [point[0] for point in a_curve4]\n",
    "    y = [point[1] for point in a_curve4]\n",
    "    ax.plot(x, y, '-', color ='black',alpha = 1)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    return a_curve1,a_curve2,a_curve3,a_curve4\n",
    "\n",
    "def plot_and_create_xy_segments(T1_start_ind,T1_end_ind,ax,col,current_x,current_y):\n",
    "    segment1 = []\n",
    "    for i in range(len(T1_start_ind)):\n",
    "        ax.plot(current_x[T1_start_ind[i]:T1_end_ind[i]],current_y[T1_start_ind[i]:T1_end_ind[i]],'-', color = col, alpha = 1)    \n",
    "        x_vals = current_x[T1_start_ind[i]:T1_end_ind[i]]\n",
    "        y_vals = current_y[T1_start_ind[i]:T1_end_ind[i]]\n",
    "        xy_coords = []\n",
    "        for index,x in enumerate(x_vals):\n",
    "            xy_coords += [(x,y_vals[index])]\n",
    "        segment1 += [xy_coords]\n",
    "    return(segment1)\n",
    "\n",
    "def extract_port_to_port_trajetories(start_port,end_port,frame_filter,threshold_breaks,exclude_port_1,exclude_port_2,exclude_port_3):\n",
    "\n",
    "    start_ind = []\n",
    "    end_ind = []\n",
    "\n",
    "\n",
    "    index = 0\n",
    "    while index < len(threshold_breaks[:-1]):\n",
    "        break_ = threshold_breaks[index]\n",
    "        if break_ == start_port and not threshold_breaks[index+1] ==start_port:\n",
    "            # find min valin this that is larger than current - ie. next index\n",
    "            p3_ind = find_next_val(index,threshold_breaks,frame_filter,end_port)\n",
    "            # ignore any really bad ones that enter othe rports first, the -1 takes care of the excluded trajectories (gets rid of weird noise hwere the DLC tracking jumps outsid eof the task zone)\n",
    "            if not exclude_port_1 in threshold_breaks[index:p3_ind] and not exclude_port_2 in threshold_breaks[index:p3_ind] and not exclude_port_3 in threshold_breaks[index:p3_ind] and not -1 in threshold_breaks[index:p3_ind]:\n",
    "                if p3_ind != -1:\n",
    "                    start_ind += [index-3]\n",
    "                    end_ind += [p3_ind+3]\n",
    "                    if not index == (p3_ind - 1):\n",
    "                        index = p3_ind - 1\n",
    "                    else:\n",
    "                        index = p3_ind\n",
    "                else:\n",
    "                    index+=1\n",
    "            else:\n",
    "                index += 1\n",
    "        else:\n",
    "            index +=1\n",
    "\n",
    "    return start_ind, end_ind\n",
    "def find_task_relevant_tracking_points(back_head_centre_df,p1,p2,p3,p4,p5,radius):\n",
    "    # find task relevant tracking periods\n",
    "    #extract times mouse is close to each behavioural port\n",
    "\n",
    "\n",
    "    current_x = back_head_centre_df.interped_x.values\n",
    "    current_y = back_head_centre_df.interped_y.values\n",
    "\n",
    "    port_positions = []\n",
    "    for i in range(5):\n",
    "        port = [p1,p2,p3,p4,p5][i]\n",
    "        port_positions += [[np.median(port.interped_x),np.median(port.interped_y)]]\n",
    "        \n",
    "    port_centroids = port_positions\n",
    "        \n",
    "    coords = []\n",
    "    for ind_,item in enumerate(current_x):\n",
    "        coords += [[item,current_y[ind_]]]\n",
    "\n",
    "    \n",
    "    threshold_breaks = np.zeros(len(coords))\n",
    "    # for each port we care about find where the mouse breaks the distcance threshold\n",
    "    for ind_ in range(0,len(port_centroids)):\n",
    "        threshold = radius\n",
    "        target = port_centroids[ind_]\n",
    "        closest,indicies = closest_points(target, coords, threshold)\n",
    "        \n",
    "        threshold_breaks[indicies] = ind_ + 1\n",
    "\n",
    "    # exclude (by labelling with -1) any times the trajetcory goes outside of the port area\n",
    "    half_dist = (port_centroids[0][-1] - port_centroids[4][-1])/2\n",
    "    exclusion_mask = (np.array(current_y) < (port_centroids[0][-1] + half_dist)) * (np.array(current_y) > (port_centroids[4][-1] - half_dist))\n",
    "    exclusion_inds = np.where(exclusion_mask == False)\n",
    "    threshold_breaks[exclusion_inds] = -1\n",
    "    \n",
    "    return threshold_breaks,port_centroids,current_x,current_y, radius\n",
    "\n",
    "\n",
    "def find_closest_points(curve1, curve2, cut):\n",
    "    \"\"\"Find the closest points between two curves.\"\"\"\n",
    "    if cut:#only use the very end of the line to do the joining procedure to prevent verlap errors\n",
    "        curve1 = np.array(curve1[-100::])\n",
    "        curve2 = np.array(curve2[0:100])\n",
    "    else:     \n",
    "        curve1 = np.array(curve1)\n",
    "        curve2 = np.array(curve2)\n",
    "    \n",
    "    dist_matrix = distance.cdist(curve1, curve2)\n",
    "    min_idx = np.unravel_index(np.argmin(dist_matrix), dist_matrix.shape)\n",
    "    return curve1[min_idx[0]], curve2[min_idx[1]]\n",
    "\n",
    "def join_curves(curve1, curve2, cut = True):\n",
    "    \"\"\"Join two curves at the closest points if they touch, otherwise join endpoints.\"\"\"\n",
    "    point1, point2 = find_closest_points(curve1, curve2, cut)\n",
    "    \n",
    "    if np.linalg.norm(np.array(point1) - np.array(point2)) < 2:  # Assuming touching if distance < small epsilon\n",
    "        # Directly connect at closest point\n",
    "        print('overlap detected')\n",
    "        index1 = curve1.index(point1.tolist())\n",
    "        index2 = curve2.index(point2.tolist())\n",
    "        return curve1[:index1+1] + curve2[index2:]\n",
    "    else:\n",
    "        # Join by endpoints\n",
    "        return curve1 + curve2\n",
    "    \n",
    "def join_make_full_circle(curve1, cut = False):\n",
    "    \"\"\"Join two curves at the closest points if they touch, otherwise join endpoints.\"\"\"\n",
    "    point1, point2 = find_closest_points(curve1[0:100], curve1[-100::], cut)\n",
    "    \n",
    "    if np.linalg.norm(np.array(point1) - np.array(point2)) < 2:  # Assuming touching if distance < small epsilon\n",
    "        # Directly connect at closest point\n",
    "        print('overlap detected!')\n",
    "        index1 = curve1[0:100].index(point1.tolist())\n",
    "        index2 = curve1[-100::].index(point2.tolist())\n",
    "        index2 = index2 + (len(curve1) - 100)\n",
    "        return curve1[index1:index2]\n",
    "    else:\n",
    "        # Join by endpoints\n",
    "        return curve1 + [curve1[0]]\n",
    "\n",
    "def resample_curve(complete_average, num_points):\n",
    "    \"\"\"\n",
    "    Resample the given curve so that it has `num_points` evenly spaced points.\n",
    "    \"\"\"\n",
    "    a_curve_copy = np.array(complete_average.copy())\n",
    "    data_points = len(a_curve_copy)\n",
    "    \n",
    "    # Compute cumulative distance along the curve\n",
    "    distances = [0]\n",
    "    for i in range(1, data_points):\n",
    "        distances.append(distances[-1] + math.dist(a_curve_copy[i], a_curve_copy[i-1]))\n",
    "    total_length = distances[-1]\n",
    "    \n",
    "    # Create new evenly spaced distance values\n",
    "    new_distances = np.linspace(0, total_length, num_points)\n",
    "    \n",
    "    # Interpolate new points\n",
    "    x_vals = [p[0] for p in a_curve_copy]\n",
    "    y_vals = [p[1] for p in a_curve_copy]\n",
    "    x_interp = np.interp(new_distances, distances, x_vals)\n",
    "    y_interp = np.interp(new_distances, distances, y_vals)\n",
    "    \n",
    "    return np.column_stack((x_interp, y_interp))\n",
    "\n",
    "def plot_av_and_new_standard_line(complete_average,standard_av_curve,radius_used,port_centroids):\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(8, 2))\n",
    "    for ind, ax in enumerate(fig.axes):\n",
    "        for index,port_centroid in enumerate(port_centroids):\n",
    "            ## define rings around important ports: port 5, port2, port 3, port4\n",
    "            c = ['blue','grey','blue','grey','blue']\n",
    "            circle1 = plt.Circle(port_centroid, radius_used, color=c[index], alpha = 0.2)\n",
    "            ax.add_patch(circle1)\n",
    "        if ind == 0:\n",
    "            x = [point[0] for point in complete_average]\n",
    "            y = [point[1] for point in complete_average]\n",
    "            ax.plot(x, y, '--', color ='black',alpha = 1)\n",
    "        if ind == 1:\n",
    "            x = [point[0] for point in standard_av_curve]\n",
    "            y = [point[1] for point in standard_av_curve]\n",
    "            ax.plot(x, y, 'x', color ='black',alpha = 1)\n",
    "\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "\n",
    "def shift_curve_start(resampled_curve, p5, radius=45):\n",
    "    \"\"\"\n",
    "    Shift the curve start to the first point that exits a circle of given radius from p5.\n",
    "    \n",
    "    :param resampled_curve: np.array of shape (N, 2), evenly spaced curve points\n",
    "    :param p5: Tuple (x, y) representing the circle centroid\n",
    "    :param radius: Radius of the circle\n",
    "    :return: Shifted curve starting from the first point outside the circle\n",
    "    \"\"\"\n",
    "    # Find the first index where the point is outside the circle\n",
    "    for i, point in enumerate(resampled_curve):\n",
    "        if math.dist(point, p5) > radius:\n",
    "            break  # Found the new start index\n",
    "    \n",
    "    # Rearrange the curve so this new point becomes the start\n",
    "    shifted_curve = np.vstack((resampled_curve[i:], resampled_curve[:i]))  # Maintain order\n",
    "    \n",
    "    return shifted_curve\n",
    "\n",
    "def closest_points(target, points, threshold):\n",
    "    import math\n",
    "    closest = []\n",
    "    indicies = []\n",
    "    for index,point in enumerate(points):\n",
    "        distance = math.dist(target,point)\n",
    "        if distance <= threshold:\n",
    "            closest.append(point)\n",
    "            indicies.append(index)\n",
    "    return closest,indicies\n",
    "\n",
    "def find_next_val(index,threshold_breaks,frame_filter,port_type):\n",
    "    p2_indicies = np.where(threshold_breaks == port_type)[0]\n",
    "    try:\n",
    "        p2_min_val = min(i for i in p2_indicies if i > index)\n",
    "        distance = p2_min_val - index\n",
    "    except:\n",
    "        distance = 9999999\n",
    "    if distance<frame_filter:\n",
    "        return p2_min_val\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def find_similar_segments(tracking, template, threshold, centroid_thresh=3, radius=3):\n",
    "    \"\"\"\n",
    "    Find similar segments with centroid pre-filtering.\n",
    "    \n",
    "    Parameters:\n",
    "        tracking: Nx2 array of (x,y) points.\n",
    "        template: Mx2 array of template (x,y) points.\n",
    "        threshold: DTW distance threshold.\n",
    "        centroid_thresh: Pre-filter threshold on centroid distance.\n",
    "        radius: DTW radius for fastdtw.\n",
    "    \n",
    "    Returns:\n",
    "        List of matching segment indices.\n",
    "    \"\"\"\n",
    "    tracking = np.array(tracking)\n",
    "    template = np.array(template)\n",
    "    M = len(template)\n",
    "    matches = []\n",
    "\n",
    "    for start in tqdm(range(len(tracking) - M + 1), desc=\"Processing segments\"):\n",
    "        window = tracking[start : start + M]\n",
    "\n",
    "        # Pre-filter: Check centroid distance first\n",
    "        if centroid_distance(window, template) > centroid_thresh:\n",
    "            continue  # Skip if centroids are too far apart\n",
    "\n",
    "        # Run DTW only if centroid distance is low\n",
    "        distance, _ = fastdtw(window, template, radius=radius, dist=euclidean)\n",
    "        if distance < threshold:\n",
    "            matches.append((start, start + M))\n",
    "\n",
    "    return matches\n",
    "\n",
    "def centroid_distance(window, template):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between centroids of the window and the template.\n",
    "    \"\"\"\n",
    "    centroid_window = np.mean(window, axis=0)\n",
    "    centroid_template = np.mean(template, axis=0)\n",
    "    return euclidean(centroid_window, centroid_template)\n",
    "\n",
    "def find_motif_points(continuous_regions_df, standard_av_curve, port_centroids, seq_colours, radius_used,num_intermediate_points=5):\n",
    "\n",
    "    start_end_intermediate = []\n",
    "    fig, axs = plt.subplots(1, len(continuous_regions_df), figsize=(4 * len(continuous_regions_df), 2))\n",
    "\n",
    "    for i, row in continuous_regions_df.iterrows():\n",
    "        buffer_ = 2  # 2% buffer on each side\n",
    "        start = row.start - buffer_\n",
    "        end = row.end + buffer_\n",
    "        \n",
    "        # Handle circular nature at 100%\n",
    "        if start < 0:\n",
    "            start += 100\n",
    "        if end > 100:\n",
    "            end -= 100\n",
    "        \n",
    "        # Compute percentage points\n",
    "        if start < end:\n",
    "            positions = np.linspace(start, end, num_intermediate_points + 2)[1:-1]  # Exclude start and end\n",
    "        else:  # Wraps around 100%\n",
    "            positions = np.linspace(0, (100-start)+end, num_intermediate_points + 2)[1:-1]  # Exclude start and end\n",
    "            positions = start + positions\n",
    "            positions = [position-100 if position>100 else position for position in positions]\n",
    "        \n",
    "        \n",
    "        intermediate_xy = get_percentage_points(standard_av_curve, positions)\n",
    "        s_, e_ = get_percentage_points(standard_av_curve, [start, end])\n",
    "        \n",
    "        # Plot\n",
    "        plot_percentage_interval(standard_av_curve, [start, end], port_centroids, axs[i], row.sequence, seq_colours[int(row.sequence) + 1],radius_used)\n",
    "        \n",
    "        # Store results\n",
    "        start_end_intermediate.append([s_, *intermediate_xy, e_])\n",
    "\n",
    "    # Create column names dynamically based on number of intermediate points\n",
    "    columns = [\"start_xy\"] + [f\"intermediate_{i}_xy\" for i in range(1, num_intermediate_points + 1)] + [\"end_xy\"]\n",
    "    return pd.DataFrame(start_end_intermediate, columns=columns)\n",
    "\n",
    "def get_percentage_points(resampled_curve, percentages):\n",
    "    \"\"\"\n",
    "    Get the points at specific percentage locations along the resampled curve.\n",
    "    \n",
    "    :param resampled_curve: np.array of shape (N, 2), evenly spaced curve points\n",
    "    :param percentages: List of percentages (0-100) where points should be extracted\n",
    "    :return: List of (x, y) points corresponding to the given percentages\n",
    "    \"\"\"\n",
    "    num_points = len(resampled_curve)\n",
    "    indices = [int(p / 100 * (num_points - 1)) for p in percentages]  # Convert percentage to index\n",
    "    return resampled_curve[indices]\n",
    "\n",
    "def plot_percentage_interval(resampled_curve, percentages,port_centroids,ax,sequence_name,colour_,radius_used):\n",
    "    \"\"\"\n",
    "    Plot the full resampled curve and highlight the region between two percentage points.\n",
    "    \n",
    "    :param resampled_curve: np.array of shape (N, 2), evenly spaced curve points\n",
    "    :param percentages: List [start%, end%] defining the highlighted region\n",
    "    \"\"\"\n",
    "    num_points = len(resampled_curve)\n",
    "    start_idx = int(percentages[0] / 100 * (num_points - 1))\n",
    "    end_idx = int(percentages[1] / 100 * (num_points - 1))\n",
    "\n",
    "    # Handle case where interval wraps around (e.g., 97% to 14%)\n",
    "    if start_idx <= end_idx:\n",
    "        highlight_indices = range(start_idx, end_idx + 1)\n",
    "    else:\n",
    "        highlight_indices = list(range(start_idx, num_points)) + list(range(0, end_idx + 1))\n",
    "\n",
    "    # Extract x, y coordinates\n",
    "    x_vals, y_vals = resampled_curve[:, 0], resampled_curve[:, 1]\n",
    "\n",
    "    # Plot full curve\n",
    "    ax.plot(x_vals, y_vals, color='black', linewidth=1, label=\"Full Curve\")\n",
    "\n",
    "    # Highlight section\n",
    "    highlighted_x = x_vals[list(highlight_indices)]\n",
    "    highlighted_y = y_vals[list(highlight_indices)]\n",
    "    ax.plot(highlighted_x, highlighted_y, color=colour_, linewidth=2, label=\"Highlighted Region\")\n",
    "\n",
    "    # Scatter start and end points\n",
    "    ax.scatter([x_vals[start_idx], x_vals[end_idx]], \n",
    "                [y_vals[start_idx], y_vals[end_idx]], \n",
    "                color='blue', zorder=3, label=\"Start/End Points\")\n",
    "\n",
    "    for index,port_centroid in enumerate(port_centroids):\n",
    "        circle1 = plt.Circle(port_centroid, radius_used, color='grey', alpha = 0.2)\n",
    "        ax.add_patch(circle1)\n",
    "        \n",
    "    ax.set_title(f\"sequence {int(sequence_name)+1}\")\n",
    "        \n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "def process_and_validate_trajectories(tracking, matches, template,add_amount,frame_filter,dist_filter,num_points):\n",
    "    \"\"\"\n",
    "    Process each trajectory by extending, trimming, and validating.\n",
    "\n",
    "    Parameters:\n",
    "        tracking: Nx2 array of (x, y) points.\n",
    "        matches: List of (start, end) indices of matching segments.\n",
    "        template: Mx2 array of template (x, y) points.\n",
    "        frame_filter: Maximum allowed length for a trajectory.\n",
    "\n",
    "    Returns:\n",
    "        List of processed (start, end) indices for valid trajectories.\n",
    "    \"\"\"\n",
    "    processed_trajectories = []\n",
    "\n",
    "    # Define the start and end points from the template\n",
    "    start_centroid = template[0]\n",
    "    end_centroid = template[-1]\n",
    "\n",
    "    for start, end in matches:\n",
    "        # Step 1: Extend the trajectory\n",
    "        start = start - add_amount\n",
    "        end = end + add_amount\n",
    "        extended_trajectory = tracking[start:end]\n",
    "        # Step 2: Find closest points to the start and end centroids in teh first and l;ast 30% of the trajectory\n",
    "        closest_start_indices,start_distances = find_closest_to_centroid(extended_trajectory[0:int(abs(len(extended_trajectory)*0.3))], start_centroid, num_points)\n",
    "        closest_end_indices,end_distances = find_closest_to_centroid(extended_trajectory[int(abs(len(extended_trajectory)*0.6))::], end_centroid, num_points)\n",
    "        \n",
    "        # if closest is too far away then skip\n",
    "        if start_distances[np.min(closest_start_indices)] > dist_filter:\n",
    "            continue\n",
    "        if end_distances[np.max(closest_end_indices)] > dist_filter:\n",
    "            continue\n",
    "        \n",
    "        # Step 3: Trim the trajectory\n",
    "        trimmed_start = start + np.min(closest_start_indices)\n",
    "        trimmed_end =start + int(abs(len(extended_trajectory)*0.6)) + np.max(closest_end_indices)\n",
    "    \n",
    "        \n",
    "        # Step 4: Validate length\n",
    "        if (trimmed_end-trimmed_start) > frame_filter:\n",
    "            continue  # Skip trajectories longer than frame_filter\n",
    "        \n",
    "        # step 5, Add valid trajectory indices to the final list\n",
    "        processed_trajectories.append([trimmed_start,trimmed_end])\n",
    "\n",
    "    return processed_trajectories\n",
    "\n",
    "def find_closest_to_centroid(tracking_, centroid, num_points=100):\n",
    "    \"\"\"\n",
    "    Find the indices of the `num_points` closest points to a given centroid.\n",
    "\n",
    "    Parameters:\n",
    "        tracking: Nx2 array of (x, y) points.\n",
    "        centroid: 1x2 array of the centroid coordinates.\n",
    "        num_points: Number of closest points to find.\n",
    "\n",
    "    Returns:\n",
    "        Indices of the closest points in the tracking array.\n",
    "    \"\"\"\n",
    "    distances = distance.cdist(tracking_, [centroid], metric='euclidean').flatten()\n",
    "    closest_indices = np.argsort(distances)[:num_points]\n",
    "    return closest_indices,distances\n",
    "\n",
    "def remove_overlaps(start_end_inds):\n",
    "    # Sort by start index to make overlap checking easier\n",
    "    start_end_inds.sort()\n",
    "\n",
    "    to_remove = []\n",
    "\n",
    "    for i in range(len(start_end_inds)):\n",
    "        for j in range(i + 1, len(start_end_inds)):\n",
    "            start1, end1 = start_end_inds[i]\n",
    "            start2, end2 = start_end_inds[j]\n",
    "\n",
    "            # Check if there's an overlap of more than 5 indices\n",
    "            if start2 <= end1 and end2 >= start1:  # They overlap\n",
    "                overlap = min(end1, end2) - max(start1, start2) + 1\n",
    "                if overlap > 5:\n",
    "                    # Remove the longer one\n",
    "                    if end1 - start1 > end2 - start2:\n",
    "                        to_remove.append(j)\n",
    "                    else:\n",
    "                        to_remove.append(i)\n",
    "\n",
    "    # Remove duplicates from the to_remove list and create the new list\n",
    "    to_remove = sorted(set(to_remove), reverse=True)\n",
    "    for index in to_remove:\n",
    "        start_end_inds.pop(index)\n",
    "\n",
    "    return start_end_inds\n",
    "\n",
    "def interpolate_to_longest_and_find_average_curve(curves,num_points):\n",
    "    \n",
    "    if num_points == 'None':\n",
    "        # Find the length of the longest curve\n",
    "        max_length = max([len(curve) for curve in curves])\n",
    "    else:\n",
    "        max_length = num_points\n",
    "\n",
    "    # Interpolate each curve to the length of the longest curve\n",
    "    interpolated_curves = []\n",
    "    for curve in curves:\n",
    "        if len(curve) > 0:\n",
    "            x = [point[0] for point in curve]\n",
    "            y = [point[1] for point in curve]\n",
    "\n",
    "            # find lots of points on the piecewise linear curve defined by x and y\n",
    "            M = max_length\n",
    "            t = np.linspace(0, len(x), M)\n",
    "            x_interp = np.interp(t, np.arange(len(x)), x)\n",
    "            y_interp = np.interp(t, np.arange(len(y)), y)\n",
    "\n",
    "            interpolated_curves.append([[x, y] for x, y in zip(x_interp, y_interp)])\n",
    "\n",
    "    # # Average the x and y coordinates of all the interpolated curves\n",
    "    average_curve = []\n",
    "    for i in range(max_length):\n",
    "        x_sum = 0\n",
    "        y_sum = 0\n",
    "        for curve in interpolated_curves:\n",
    "            x_sum += curve[i][0]\n",
    "            y_sum += curve[i][1]\n",
    "        average_curve.append([x_sum / len(interpolated_curves), y_sum / len(interpolated_curves)])\n",
    "\n",
    "    return average_curve\n",
    "\n",
    "def closest_points_distances(line1, line2):\n",
    "    tree = KDTree(line2)\n",
    "    dist, index = tree.query(line1)\n",
    "    return dist\n",
    "\n",
    "def normalized_dtw(traj1, traj2):\n",
    "    dtw_dist = dtw_distance(traj1, traj2)  # Original DTW distance\n",
    "    norm_dtw = dtw_dist / (len(traj1) + len(traj2))  # Normalize by total length\n",
    "    return norm_dtw\n",
    "\n",
    "def dtw_distance(traj1, traj2):\n",
    "    \"\"\"\n",
    "    Calculate the Dynamic Time Warping (DTW) distance between two trajectories.\n",
    "    \n",
    "    traj1, traj2: Lists of points [(x1, y1), (x2, y2), ...]\n",
    "    Returns: The DTW distance as a float.\n",
    "    \"\"\"\n",
    "    n, m = len(traj1), len(traj2)\n",
    "    # Create a DP table initialized with infinity\n",
    "    dp = np.full((n + 1, m + 1), np.inf)\n",
    "    \n",
    "    # Base case: matching the first point of each trajectory\n",
    "    dp[0, 0] = 0\n",
    "\n",
    "    # Fill the DP table\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = euclidean_distance(traj1[i - 1], traj2[j - 1])\n",
    "            dp[i, j] = cost + min(dp[i - 1, j], dp[i, j - 1], dp[i - 1, j - 1])\n",
    "\n",
    "    # The DTW distance is the value in the bottom-right corner of the DP table\n",
    "    return dp[n, m]\n",
    "\n",
    "\n",
    "def calculate_speed(trajectory, _1mm, fps):\n",
    "    \"\"\"\n",
    "    Calculate the speed for a single trajectory.\n",
    "    \n",
    "    Parameters:\n",
    "    - trajectory: List of tuples or arrays containing x, y coordinates [(x1, y1), (x2, y2), ...]\n",
    "    - _1mm: Distance represented by 1mm in the coordinates\n",
    "    - fps: Frames per second (tracking rate)\n",
    "    \n",
    "    Returns:\n",
    "    - speed: The average speed for the trajectory (distance/time)\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    # Calculate the total distance and total time\n",
    "    for i in range(1, len(trajectory)):\n",
    "        x1, y1 = trajectory[i - 1]\n",
    "        x2, y2 = trajectory[i]\n",
    "        \n",
    "        # Euclidean distance between two consecutive points\n",
    "        distance_mm = (np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)) / _1mm\n",
    "        time = 1 / fps  # Time between frames (1/fps)\n",
    "        \n",
    "        total_distance += distance_mm\n",
    "        total_time += time\n",
    "    \n",
    "    # Average speed: total distance / total time\n",
    "    speed = total_distance / total_time if total_time != 0 else 0\n",
    "    return speed\n",
    "\n",
    "def calculate_speed_variability(trajectories, _1mm, fps):\n",
    "    \"\"\"\n",
    "    Calculate the speed variability (standard deviation) between different trajectories.\n",
    "    \n",
    "    Parameters:\n",
    "    - trajectories: List of trajectories, where each trajectory is a list of (x, y) coordinates\n",
    "    - _1mm: Distance represented by 1mm in the coordinates\n",
    "    - fps: Frames per second (tracking rate)\n",
    "    \n",
    "    Returns:\n",
    "    - speed_std: Standard deviation of speeds across all trajectories\n",
    "    \"\"\"\n",
    "    speeds = []\n",
    "    \n",
    "    # Calculate speed for each trajectory\n",
    "    for trajectory in trajectories:\n",
    "        speed = calculate_speed(trajectory, _1mm, fps)\n",
    "        speeds.append(speed)\n",
    "    \n",
    "    # Calculate the standard deviation of speeds\n",
    "    speed_std = np.std(speeds)\n",
    "    speed_mean = np.mean(speeds)\n",
    "    \n",
    "    return speed_std,speed_mean\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points p1 and p2.\"\"\"\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "# Create a function to save all open figures in one large PNG\n",
    "def save_all_figs(filename=\"all_plots.png\", dpi=100):\n",
    "    figs = [plt.figure(num) for num in plt.get_fignums()]\n",
    "    if not figs:\n",
    "        print(\"No figures to save.\")\n",
    "        return\n",
    "\n",
    "    # make the folder temp_figs\n",
    "    if not os.path.exists('temp_figs'):\n",
    "        os.makedirs('temp_figs')\n",
    "\n",
    "    # Save each figure to a temporary image\n",
    "    images = []\n",
    "    for fig in figs:\n",
    "        # Save each figure to a temporary buffer\n",
    "        buf = f\"temp_figs/temp_fig_{fig.number}.png\"\n",
    "        fig.savefig(buf, dpi=dpi)\n",
    "        images.append(Image.open(buf))\n",
    "\n",
    "    # Concatenate all images vertically\n",
    "    total_width = max(im.width for im in images)\n",
    "    total_height = sum(im.height for im in images)\n",
    "\n",
    "    # Create a blank canvas for the concatenated image\n",
    "    combined_image = Image.new(\"RGB\", (total_width, total_height))\n",
    "\n",
    "    # Paste each figure into the canvas\n",
    "    y_offset = 0\n",
    "    for im in images:\n",
    "        combined_image.paste(im, (0, y_offset))\n",
    "        y_offset += im.height\n",
    "\n",
    "    # Save the concatenated image\n",
    "    combined_image.save(filename)\n",
    "    \n",
    "    # delete the temp figs dir\n",
    "    shutil.rmtree('temp_figs')\n",
    "    \n",
    "    \n",
    "    print(f\"All figures saved as {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ddb5e",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb74a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in behavioural data from first and second day and extract relevant information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f10dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviour_path = r'Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\replay_to_behaviour\\\\'\n",
    "expert_awake_ppseq_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\awake\\\\\"\n",
    "learning_awake_ppseq_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\learning\\awake\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fb75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Processing behavioural data for: EJT136_1_4</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day2_13-11-2021\n",
      "Processing day1_12-11-2021\n",
      "Awake PPSeq base path: Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\awake\\\\136_1_4_run_2701023_1813\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m continuous_regions_df \u001b[38;5;241m=\u001b[39m get_sequence_regions(mir,awake_ppseq_base_path,sequence_order)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# tracking data \u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m p1_1,p2_1,p3_1,p4_1,p5_1 \u001b[38;5;241m=\u001b[39m \u001b[43mport_tracking1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mport2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m],port_tracking1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport6\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport3\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport7\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     46\u001b[0m p1_2,p2_2,p3_2,p4_2,p5_2 \u001b[38;5;241m=\u001b[39m port_tracking2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport6\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport3\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],port_tracking2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport7\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m back_head_centre_df_1 \u001b[38;5;241m=\u001b[39m movement_tracking1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead_centre\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop over all animals \n",
    "for m_i_r in os.listdir(behaviour_path):\n",
    "    \n",
    "    # for now ignore the new data\n",
    "    if not 'EJT' in m_i_r:\n",
    "        continue\n",
    "    \n",
    "    current_path = os.path.join(behaviour_path, m_i_r)\n",
    "    # load in the behavioural data\n",
    "    display(HTML(f\"<b>Processing behavioural data for: {m_i_r}</b>\"))\n",
    "    poke_df_1, movement_tracking1, port_tracking1, poke_df_2, movement_tracking2, port_tracking2 = load_in_behavioural_data(current_path)\n",
    "    \n",
    "    # make output paths\n",
    "    for day_folder in os.listdir(current_path):\n",
    "        day_path = os.path.join(current_path, day_folder)\n",
    "        if 'day2' in day_path:\n",
    "            out_path_2 = day_path + '\\\\processed\\\\'\n",
    "            if not os.path.isdir(out_path_2):\n",
    "                os.makedirs(out_path_2)\n",
    "        if 'day1' in day_path:\n",
    "            out_path_1 = day_path + '\\\\processed\\\\'\n",
    "            if not os.path.isdir(out_path_1):\n",
    "                os.makedirs(out_path_1)\n",
    "\n",
    "    if 'AP5' in m_i_r:\n",
    "        mir = m_i_r.replace('AP','ap')\n",
    "    elif 'SEQ' in m_i_r: \n",
    "        mir = m_i_r.lower()\n",
    "    else:  \n",
    "        mir = m_i_r\n",
    "\n",
    "    # get awake ppseq path:\n",
    "    awake_ppseq_base_path = process_awake_ppseq_path(mir, expert_awake_ppseq_path, learning_awake_ppseq_path)\n",
    "\n",
    "    # load sequence order\n",
    "    try: \n",
    "        sequence_order = pd.read_csv(expert_awake_ppseq_path + r'sequence_order.csv')\n",
    "    except:\n",
    "        sequence_order = pd.read_csv(learning_awake_ppseq_path + r'sequence_order.csv')\n",
    "    # get continuous regions information\n",
    "    continuous_regions_df = get_sequence_regions(mir,awake_ppseq_base_path,sequence_order)\n",
    "\n",
    "\n",
    "    if not movement_tracking1 == None:\n",
    "        # tracking data \n",
    "        p1_1,p2_1,p3_1,p4_1,p5_1 = port_tracking1['port2'][0],port_tracking1['port1'][0],port_tracking1['port6'][0],port_tracking1['port3'][0],port_tracking1['port7'][0]\n",
    "        p1_2,p2_2,p3_2,p4_2,p5_2 = port_tracking2['port2'][0],port_tracking2['port1'][0],port_tracking2['port6'][0],port_tracking2['port3'][0],port_tracking2['port7'][0]\n",
    "\n",
    "        back_head_centre_df_1 = movement_tracking1['head_centre'][0]\n",
    "        back_head_centre_df_2 = movement_tracking2['head_centre'][0]\n",
    "\n",
    "\n",
    "    ## extract simple metrics (number of trials, error rate, number of transitions,reward rate (vs time and vs transitions), number of each transition type)\n",
    "    # number of trials\n",
    "    num_trials_1 = len(poke_df_1.Trial_id.unique())\n",
    "    num_trials_2 = len(poke_df_2.Trial_id.unique())\n",
    "    # # error rate\n",
    "    transit_df_1 = make_transition_df(poke_df_1)\n",
    "    correct_transit_rate_1, error_transit_rate_1, neutral_transit_rate_1 = find_error_rates(transit_df_1)\n",
    "    transit_df_2 = make_transition_df(poke_df_2)\n",
    "    correct_transit_rate_2, error_transit_rate_2, neutral_transit_rate_2 = find_error_rates(transit_df_2)\n",
    "    # perfect sequence score (behavioural score)\n",
    "    perf_score_1 = get_perfect_sequence_score(transit_df_1)\n",
    "    perf_score_2 = get_perfect_sequence_score(transit_df_2)\n",
    "    # number of transitions\n",
    "    total_transitions_1 = len(transit_df_1.Transition_type.values)\n",
    "    total_transitions_2 = len(transit_df_2.Transition_type.values)\n",
    "    # reward rate (vs time and vs transitions)\n",
    "    transits_per_reward_1,seconds_per_reward_1 = reward_rate(poke_df_1)\n",
    "    transits_per_reward_2,seconds_per_reward_2 = reward_rate(poke_df_2)\n",
    "\n",
    "    ### make a df for the behavioural data i have collected:\n",
    "    # Create the DataFrame with the existing variables\n",
    "    pokedata_output_df_1 = pd.DataFrame({\n",
    "        \"num_trials\": [num_trials_1],\n",
    "        \"correct_transit_rate\": [correct_transit_rate_1],\n",
    "        \"error_transit_rate\": [error_transit_rate_1],\n",
    "        \"neutral_transit_rate\": [neutral_transit_rate_1],\n",
    "        \"perf_score\": [perf_score_1],\n",
    "        \"total_transitions\": [total_transitions_1],\n",
    "        \"transits_per_reward\": [transits_per_reward_1],\n",
    "        \"seconds_per_reward\": [seconds_per_reward_1]\n",
    "    })  \n",
    "    pokedata_output_df_2 = pd.DataFrame({\n",
    "        \"num_trials\": [num_trials_2],\n",
    "        \"correct_transit_rate\": [correct_transit_rate_2],\n",
    "        \"error_transit_rate\": [error_transit_rate_2],\n",
    "        \"neutral_transit_rate\": [neutral_transit_rate_2],\n",
    "        \"perf_score\": [perf_score_2],\n",
    "        \"total_transitions\": [total_transitions_2],\n",
    "        \"transits_per_reward\": [transits_per_reward_2],\n",
    "        \"seconds_per_reward\": [seconds_per_reward_2]\n",
    "    })  \n",
    "\n",
    "    #save out this data \n",
    "    pokedata_output_df_1.to_csv(out_path_1 + r'processed_poke_data_1.csv',index=False)\n",
    "    pokedata_output_df_2.to_csv(out_path_2 + r'processed_poke_data_2.csv',index=False)\n",
    "\n",
    "    if not movement_tracking1 == None:\n",
    "    \n",
    "        # load in awake ppseq data for the same mir and pull in linearised thingy  \n",
    "        for i_day in range(2): \n",
    "            if i_day == 0:\n",
    "                back_head_centre_df = back_head_centre_df_1\n",
    "                p1,p2,p3,p4,p5 = p1_1,p2_1,p3_1,p4_1,p5_1\n",
    "                out_p = out_path_1\n",
    "            else:\n",
    "                back_head_centre_df = back_head_centre_df_2\n",
    "                p1,p2,p3,p4,p5 = p1_2,p2_2,p3_2,p4_2,p5_2\n",
    "                out_p = out_path_2\n",
    "\n",
    "                \n",
    "            ############################# movement variability for each motif region ####################\n",
    "            ## TRACKING PREP - laod in data, find average curves etc. \n",
    "            #############################################################################################\n",
    "\n",
    "            # find task relevant tracking periods, extract times mouse is close to each behavioural port\n",
    "            threshold_breaks,port_centroids,current_x,current_y,radius_used = find_task_relevant_tracking_points(back_head_centre_df,p1,p2,p3,p4,p5,radius = 45)\n",
    "\n",
    "            ## each of these is a frame so we know the timing between them. ie. one frame is 1/60s \n",
    "            if 'EJT' in m_i_r:\n",
    "                fps = 60\n",
    "            else:\n",
    "                dat_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "                full_org_dat_path = find_organised_path(mir,dat_path)\n",
    "                Ephys_Camera_sync = pd.read_csv(os.path.join(full_org_dat_path,r'behav_sync\\\\1_pre_sleep\\\\Presleep_Ephys_Camera_sync.csv'))\n",
    "                fps = 1/np.mean(np.diff(Ephys_Camera_sync['Time Stamps'].values))\n",
    "                \n",
    "                \n",
    "            time_filter = 2 #in s\n",
    "            frame_filter = int(time_filter / (1/fps))\n",
    "\n",
    "            #we know when the mouse is close to each port, so find times when the mouse goes from port to port \n",
    "            start_port,end_port = 5,2\n",
    "            T1_start_ind, T1_end_ind = extract_port_to_port_trajetories(start_port,end_port,frame_filter,threshold_breaks,4,-1,-1)\n",
    "            start_port,end_port = 2,3\n",
    "            T2_start_ind, T2_end_ind = extract_port_to_port_trajetories(start_port,end_port,frame_filter,threshold_breaks,1,4,5)\n",
    "            start_port,end_port = 3,4\n",
    "            T3_start_ind, T3_end_ind = extract_port_to_port_trajetories(start_port,end_port,frame_filter,threshold_breaks,1,2,5)\n",
    "            start_port,end_port = 4,5\n",
    "            T4_start_ind, T4_end_ind = extract_port_to_port_trajetories(start_port,end_port,frame_filter,threshold_breaks,1,2,3)\n",
    "\n",
    "            # find average curves by taking traject lines and make them roughly equivalent by interpolating so they have the same number of trakcing points, then for each point in each trajectory find the average xy position to create an average trajectory line \n",
    "            a_curve1,a_curve2,a_curve3,a_curve4 = find_average_curves(port_centroids,T1_start_ind,T1_end_ind,T2_start_ind,T2_end_ind,T3_start_ind,T3_end_ind,T4_start_ind,T4_end_ind,current_x,current_y,buffer = 10, radius = 45)\n",
    "\n",
    "            # join each curve, join them at the point that they touch, or if they dont touch just join the two ends to each other\n",
    "            new = join_curves(a_curve1, a_curve2,cut = True)\n",
    "            new = join_curves(new, a_curve3,cut = True)\n",
    "            new = join_curves(new, a_curve4,cut = True)\n",
    "            # join the two ends together to make a complete circle\n",
    "            complete_average = join_make_full_circle(new)\n",
    "\n",
    "            #interpolate to make standardspace:\n",
    "            standard_av_curve = resample_curve(complete_average, num_points = 10000)\n",
    "\n",
    "            # shift the start point of the curve so that is the first time the mous eleaves port 5 radius\n",
    "            standard_av_curve = shift_curve_start(standard_av_curve,port_centroids[-1])\n",
    "\n",
    "            # plot this stuff to check it looks okay\n",
    "            plot_av_and_new_standard_line(complete_average,standard_av_curve,radius_used,port_centroids)\n",
    "\n",
    "            # load in the seq colours \n",
    "            seq_colours = np.load(awake_ppseq_base_path+ r'/analysis_output/reordered_recolored/colors',allow_pickle=True)\n",
    "\n",
    "            ##### FIND THE MOTIF REGIONS - #################################\n",
    "            # find all the examples of each motif \n",
    "            ################################################################\n",
    "\n",
    "            # next define the regions of each motif \n",
    "            motif_start_ends_df = find_motif_points(continuous_regions_df,standard_av_curve,port_centroids,seq_colours,radius_used,num_intermediate_points=50)\n",
    "\n",
    "            #convert to xy coords\n",
    "            full_tracking_coords = []\n",
    "            for ind_,item in enumerate(current_x):\n",
    "                full_tracking_coords += [[item,current_y[ind_]]]\n",
    "\n",
    "            # now take these start and end points of each motif to find all motif examples:\n",
    "            motif_start_ends = []\n",
    "            for i in range(len(motif_start_ends_df)):\n",
    "\n",
    "                points = []\n",
    "                for column in list(motif_start_ends_df):\n",
    "                    row = motif_start_ends_df[(f'{column}')][i]\n",
    "                    points += [row]\n",
    "                    \n",
    "                centroid_distance_threshold = 1000000  # You need to set an appropriate threshold based on your data.\n",
    "                radius_threshold = 30\n",
    "                similar_segments = find_similar_segments(full_tracking_coords, points, centroid_distance_threshold,radius_threshold)\n",
    "                ## each of these is a frame so we know the timing between them. ie. one frame is 1/60s \n",
    "                time_filter = 6 #in s\n",
    "                # minimum distance from start/end centroid\n",
    "                dist_filter = 60\n",
    "                # number of closest points to start/end centroid to find and chose from\n",
    "                num_points = 3\n",
    "                # number of points to add to the start and end of the trajectory\n",
    "                add_start_end = 5\n",
    "                valid_trajectories = process_and_validate_trajectories(full_tracking_coords, similar_segments, points,add_start_end,int(time_filter / (1/fps)),dist_filter,num_points)\n",
    "\n",
    "                # then remove any duplicates:\n",
    "                valid_trajectories_filtered = remove_overlaps(valid_trajectories)\n",
    "\n",
    "                fig, ax = plt.subplots(1, 1,figsize=(6, 4))\n",
    "                for index,port_centroid in enumerate(port_centroids):\n",
    "                    circle1 = plt.Circle(port_centroid, 45, color='grey', alpha = 1)\n",
    "                    ax.add_patch(circle1)\n",
    "\n",
    "                trajects = []\n",
    "                for ind in valid_trajectories_filtered:\n",
    "                    traject = full_tracking_coords[ind[0]:ind[1]]\n",
    "                    trajects += [traject]\n",
    "                    x = [point[0] for point in traject]\n",
    "                    y = [point[1] for point in traject]\n",
    "                    ax.plot(x,y,'-', color = 'blue', alpha = 0.2)\n",
    "                    ax.plot(x[0],y[0],'o', color = 'yellow', alpha = 0.2)\n",
    "                    ax.plot(x[-1],y[-1],'o', color = 'yellow', alpha = 0.2)\n",
    "\n",
    "                x = [point[0] for point in points]\n",
    "                y = [point[1] for point in points]\n",
    "                ax.plot(x,y,'-', color = 'red', alpha = 1)\n",
    "\n",
    "                av_of_found = interpolate_to_longest_and_find_average_curve(trajects,500)\n",
    "                x = [point[0] for point in av_of_found]\n",
    "                y = [point[1] for point in av_of_found]\n",
    "                ax.plot(x,y,'-', color = 'k', alpha = 1)\n",
    "                    \n",
    "                ax.invert_yaxis()\n",
    "\n",
    "                print(len(valid_trajectories_filtered))\n",
    "                \n",
    "                motif_start_ends += [valid_trajectories_filtered]\n",
    "\n",
    "            ##### FIND THE MOVEMENT VARIABILITY FOR EACH MOTIF - ########################################\n",
    "            # using two different methods, h distance and dtw\n",
    "            #############################################################################################\n",
    "\n",
    "            # needs to be large to make the h distance calcualtions accurate\n",
    "            motif_start_ends_df_large = find_motif_points(continuous_regions_df,standard_av_curve,port_centroids,seq_colours,radius_used,num_intermediate_points=10000)\n",
    "            plt.close()\n",
    "            # slightly smaller to save computational power when doing dtw\n",
    "            motif_start_ends_df_medium = find_motif_points(continuous_regions_df,standard_av_curve,port_centroids,seq_colours,radius_used,num_intermediate_points=300)\n",
    "            plt.close()\n",
    "\n",
    "            motif_dists = []\n",
    "            motif_std_dists = []\n",
    "            motif_median_dtws = []\n",
    "            motif_std_dtws = []\n",
    "            for i in tqdm(range(len(motif_start_ends_df_large))):\n",
    "\n",
    "                points = []\n",
    "                for column in list(motif_start_ends_df_large):\n",
    "                    row = motif_start_ends_df_large[(f'{column}')][i]\n",
    "                    points += [row]\n",
    "                    \n",
    "                points2 = []\n",
    "                for column in list(motif_start_ends_df_medium):\n",
    "                    row = motif_start_ends_df_medium[(f'{column}')][i]\n",
    "                    points2 += [row]\n",
    "\n",
    "                motif_ranges = motif_start_ends[i]\n",
    "                median_distances = []\n",
    "                std_distances = []\n",
    "                dtws = []\n",
    "                for start, end in motif_ranges:\n",
    "                    segment = full_tracking_coords[start:end]\n",
    "                    \n",
    "                    \"\"\" \n",
    "                    a simplified version of the Hausdorff distance\n",
    "                    \"\" Measures the greatest distance between any point on one trajectory to the closest point on the other.\n",
    "                    determine deviation from average\n",
    "                    - for each tracking point in each trajectory find the point on the relevant interpolated average line which it is closest to\n",
    "                    - this gives us a list of distances (deviations) from the average for each trajectory line\n",
    "                    - this can be used to find the standard devation of the average distance from the average line.\n",
    "                    - this gives us a meausre of how steretyped the port-port trjectories were:\n",
    "                    - stereotyped = similar to average line = smaller devation\n",
    "                    - not stereotyped = different to average line = larger devation \"\"\n",
    "                    \"\"\" \n",
    "                    distances = closest_points_distances(segment, points)\n",
    "                    median_distances.append(np.median(distances))\n",
    "                    std_distances.append(np.std(distances))\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    dynamic time warping distance \n",
    "                    WHAT IS THE DTW NUMBER:\n",
    "                    here i take the mean , so the number represents, trajectories, when optimally aligned to teh average line, the mean amount by which they deviate in total (here in mm). This doesn't necessarily mean each corresponding point deviates that many mm—\n",
    "                    it's the total sum of all point-to-point deviations, which can vary depending on the trajectory.\n",
    "                    eg. \"The Dynamic Time Warping (DTW) distance between the two trajectories was xx mm, representing the total accumulated deviation over the entire alignment.\"\n",
    "                    I calculate the dwt normalised to length of the trajectory\n",
    "                    \"\"\"\n",
    "                    dtws += [normalized_dtw(segment, points2)]\n",
    "                    \n",
    "                            \n",
    "                # for h distance \n",
    "                motif_dists.append(median_distances)\n",
    "                motif_std_dists.append(std_distances)\n",
    "                # for dtw\n",
    "                motif_median_dtws.append(np.mean(dtws))\n",
    "                motif_std_dtws.append(np.std(dtws))\n",
    "\n",
    "            # put the data into vars\n",
    "            all_motif_mean_dists = [np.mean(dist) for dist in motif_dists]\n",
    "            all_motif_std_dists = [np.mean(std_dist) for std_dist in motif_std_dists]\n",
    "\n",
    "\n",
    "            ## convert to cm, we know distance bwteen two ports should be 3cm apart\n",
    "            port_pairs = [0,1],[0,3],[2,4],[0,2],[3,4]\n",
    "            ptp_distances = []\n",
    "            for pair in port_pairs:\n",
    "                ptp_distances += [math.sqrt((port_centroids[pair[0]][0] - port_centroids[pair[1]][0])**2 + (port_centroids[pair[0]][-1] - port_centroids[pair[1]][-1])**2)]\n",
    "            _1_mm = (np.mean(ptp_distances)/3)/10 \n",
    "\n",
    "            all_motif_mean_dists_mm = np.array(all_motif_mean_dists)/_1_mm\n",
    "            all_motif_std_dists_mm = np.array(all_motif_std_dists)/_1_mm\n",
    "\n",
    "            all_motif_mean_dtws_mm = np.array(motif_median_dtws)/_1_mm\n",
    "            all_motif_std_dtws_mm = np.array(motif_std_dtws)/_1_mm\n",
    "                \n",
    "            #############################################################################################\n",
    "            ### FIND VARIANCE IN TIME TAKEN TO DO EACH MOTIF - also average movement speed\n",
    "            #############################################################################################\n",
    "\n",
    "            # average speed - distance over time\n",
    "            # speed variability, \n",
    "\n",
    "            all_motif_mean_speed_mm_s = []\n",
    "            all_motif_mean_std_speed_mm_s = []\n",
    "            for motif_ranges in motif_start_ends:\n",
    "                segments = []\n",
    "                for start, end in motif_ranges:\n",
    "                    segments += [full_tracking_coords[start:end]]\n",
    "                # Calculate average speeds and speed variability\n",
    "                speed_std,speed_mean = calculate_speed_variability(segments, _1_mm, fps)\n",
    "\n",
    "                all_motif_mean_speed_mm_s += [speed_mean]\n",
    "                all_motif_mean_std_speed_mm_s += [speed_std]\n",
    "\n",
    "\n",
    "            tracking_output_df = pd.DataFrame({\n",
    "                \"sequence_motif\" : continuous_regions_df.sequence.values+1,\n",
    "                \"mean_h_distance_from_av_mm\": all_motif_mean_dists_mm,\n",
    "                \"std_h_distance_from_av_mm\": all_motif_std_dists_mm,\n",
    "                \"mean_dtw_distance_from_av_mm\":all_motif_mean_dtws_mm,\n",
    "                \"std_dtw_distance_from_av_mm\":all_motif_std_dtws_mm,\n",
    "                \"mean_movement_speed_mm_s\": all_motif_mean_speed_mm_s,\n",
    "                \"std_movement_speed_mm_s\": all_motif_mean_std_speed_mm_s   \n",
    "            })\n",
    "            \n",
    "            tracking_output_df.to_csv(out_p + r'/processed_tracking_data.csv',index=False)\n",
    "            \n",
    "        # Save all figures  \n",
    "        save_all_figs(out_p + mir + r\".png\")\n",
    "        plt.close('all')\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6f4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91474d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_tracking1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed72a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
