{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast \n",
    "\n",
    "def assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict):\n",
    "    # if session in one of the groups (and define which)   \n",
    "    if mouse in list(expert_mice) + list(hlesion_mice) + list(learning_mice):\n",
    "        if mouse in expert_mice:\n",
    "            var_dict['expert'] += [1]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [0]               \n",
    "        elif mouse in hlesion_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [1]\n",
    "            var_dict['learning'] += [0]   \n",
    "        elif mouse in learning_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [1]   \n",
    "    return var_dict\n",
    "\n",
    "def get_time_span(dat_path,pp_file,mouse):\n",
    "    with open(dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json', 'r') as file:\n",
    "        params = json.load(file)\n",
    "    time_spans = params['time_span']\n",
    "    return time_spans\n",
    "\n",
    "def find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start):\n",
    "    current_mouse_path = []\n",
    "    for run_index,pp_file in enumerate(os.listdir(sleep_ppseq_path)):\n",
    "        if not 'sleep_time_points' in pp_file:\n",
    "            # current mouse\n",
    "            mouse = '_'.join(pp_file.split('_')[0:3])    \n",
    "\n",
    "            if mouse in useable_mirs:\n",
    "                    \n",
    "                    # asign to experimental group in var_dict\n",
    "                    var_dict = assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict)\n",
    "\n",
    "                    # load in sleep start time and time span\n",
    "                    var_dict['current_sleep_start'] += sleep_start[mouse]\n",
    "                    var_dict['time_spans'] += get_time_span(sleep_ppseq_path,pp_file,mouse)\n",
    "\n",
    "                    # set path to processed files \n",
    "                    current_mouse_path += [sleep_ppseq_path + pp_file + '\\\\analysis_output\\\\']\n",
    "                    var_dict['mirs'] += [mouse]\n",
    "    return current_mouse_path,var_dict\n",
    "\n",
    "\n",
    "def make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only):\n",
    "    ## filter this data\n",
    "    if sequential_filter == True: \n",
    "        sequential_condition = data.ordering_classification == 'sequential'\n",
    "    else:\n",
    "        sequential_condition = np.array([True]*len(data.ordering_classification))\n",
    "\n",
    "    if sleep_filters_on == True:\n",
    "        if nrem_filter == True: \n",
    "            nrem_condition = data.nrem_events == 1\n",
    "        else:\n",
    "            nrem_condition = np.array([False]*len(data.nrem_events))\n",
    "\n",
    "        if rem_filter == True: \n",
    "            rem_condition = data.rem_events == 1\n",
    "        else:\n",
    "            rem_condition = np.array([False]*len(data.rem_events))\n",
    "\n",
    "        if background_only == True:\n",
    "            rem_condition = data.rem_events == 0\n",
    "            nrem_condition = data.nrem_events == 0\n",
    "\n",
    "    else:\n",
    "        nrem_condition = np.array([True]*len(data))\n",
    "        rem_condition = np.array([True]*len(data))\n",
    "        \n",
    "    # filter is set up so that any true will carry forward \n",
    "    filter_mask = sequential_condition * (nrem_condition + rem_condition)\n",
    "        \n",
    "    return filter_mask\n",
    "\n",
    "def determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path):\n",
    "    # if sleep_filters_on is false, use all chunk time\n",
    "    if sleep_filters_on == False:\n",
    "        mins = np.diff(chunk_time)[0]\n",
    "    else:\n",
    "        # load in state times\n",
    "        rem_state_times = np.load(path + 'rem_state_times.npy')\n",
    "        nrem_state_times = np.load(path + 'nrem_state_times.npy')\n",
    "        if len(rem_state_times) > 0:\n",
    "            tot_rem = sum(np.diff(rem_state_times))[0]\n",
    "        else:\n",
    "            tot_rem = 0\n",
    "        if len(nrem_state_times) > 0:\n",
    "            tot_nrem = sum(np.diff(nrem_state_times))[0]\n",
    "        else:\n",
    "            tot_nrem = 0\n",
    "\n",
    "        # if background then use all non rem and non nrem times\n",
    "        if background_only:\n",
    "            mins = np.diff(chunk_time)[0] - (tot_rem+tot_nrem)\n",
    "        else:\n",
    "            # if both, use both \n",
    "            if nrem_filter == True and rem_filter == True:\n",
    "                mins = tot_rem+tot_nrem\n",
    "            elif nrem_filter == True and rem_filter == False:\n",
    "                mins = tot_nrem\n",
    "            elif nrem_filter == False and rem_filter == True:\n",
    "                mins = tot_rem\n",
    "    # convert to mins            \n",
    "    mins = mins/60\n",
    "    \n",
    "    return mins\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def relative_dict(input_dict):\n",
    "    total_sum = sum(input_dict.values())\n",
    "    relative_dict = {key: value / total_sum for key, value in input_dict.items()}\n",
    "    return relative_dict\n",
    "\n",
    "def refind_cluster_events(filtered_chunk_data,event_proximity_filter):\n",
    "    \n",
    "    ### ignore the origonal clusterg rosp and remake them: \n",
    "    start_times = filtered_chunk_data.first_spike_time.values\n",
    "    end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "    clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "    cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "    for index,cluster in enumerate(clustered_events):\n",
    "        for item in cluster:\n",
    "            cluster_group[item] = int(index)\n",
    "    filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "    \n",
    "    return filtered_chunk_data\n",
    "\n",
    "def create_multicluster_dataframe(filtered_chunk_data):\n",
    "    meaned_order = []\n",
    "    fs_order = []\n",
    "    event_times = []\n",
    "    count = 0\n",
    "    for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "        group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "        current_cluster = filtered_chunk_data[group_mask].copy()\n",
    "        if len(current_cluster) > 1:\n",
    "            means = []\n",
    "            event_types = []\n",
    "            fs_orders = []\n",
    "            for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                # calculate event order based on spike time weighted mean\n",
    "                means += [np.mean(ast.literal_eval(events))]\n",
    "                # calculate order based on first spike time:\n",
    "                fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "            # order by mean time:    \n",
    "            meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "            # order by first spike:\n",
    "            fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "            event_times += [fs_orders]\n",
    "\n",
    "            current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "            current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "            current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "            if count == 0:\n",
    "                multi_cluster_df = current_cluster.copy()\n",
    "            else:\n",
    "                # Concatenate the DataFrames vertically (row-wise)\n",
    "                multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                # Reset the index if needed\n",
    "                multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "            count += 1\n",
    "    return multi_cluster_df,meaned_order,fs_order\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_ordering_amounts(meaned_order,dominant,other_):\n",
    "    ordered = 0\n",
    "    misordered = 0\n",
    "    other = 0\n",
    "    for cluster in meaned_order:\n",
    "        for ind,item in enumerate(cluster):\n",
    "            if not ind == len(cluster)-1:\n",
    "                pair = [item,cluster[ind+1]]\n",
    "                outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                    ordered += 1\n",
    "                elif outcome == 'misordered':\n",
    "                    misordered += 1\n",
    "                else:\n",
    "                    other +=1\n",
    "    return ordered,misordered,other\n",
    "\n",
    "def all_motifs_proportion_coactive(multi_cluster_df):\n",
    "    motif_motif_coative_events = []\n",
    "    for seq_type in range(1,7):\n",
    "        motif_cluster_groups = multi_cluster_df[multi_cluster_df['cluster_seq_type'] == seq_type].new_cluster_group\n",
    "        if not len(motif_cluster_groups) == 0:\n",
    "            coative_motif_events = len(motif_cluster_groups)\n",
    "        else:\n",
    "            coative_motif_events = 0\n",
    "        motif_motif_coative_events += [coative_motif_events]\n",
    "    return motif_motif_coative_events\n",
    "\n",
    "def motif_by_motif_ordering(meaned_order,real_order,dominant,other_):\n",
    "\n",
    "    all_motifs_fs_task_related_ordered = []\n",
    "    all_motifs_fs_task_related_misordered = []\n",
    "    all_motifs_fs_task_related_other = []\n",
    "\n",
    "    for motif_type in range(1,7):\n",
    "        ordered = 0\n",
    "        misordered = 0\n",
    "        other = 0\n",
    "        \n",
    "        if motif_type in real_order:\n",
    "            for cluster in meaned_order:\n",
    "                for ind,item in enumerate(cluster):\n",
    "                    if not ind == len(cluster)-1:\n",
    "                        pair = [item,cluster[ind+1]]\n",
    "                        if motif_type in pair:\n",
    "                            outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                            if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                                ordered += 1\n",
    "                            elif outcome == 'misordered':\n",
    "                                misordered += 1\n",
    "                            else:\n",
    "                                other +=1  \n",
    "                                \n",
    "            all_motifs_fs_task_related_ordered += [ordered]\n",
    "            all_motifs_fs_task_related_misordered += [misordered]\n",
    "            all_motifs_fs_task_related_other += [other]\n",
    "        else:\n",
    "            all_motifs_fs_task_related_ordered += ['nan']\n",
    "            all_motifs_fs_task_related_misordered += ['nan']\n",
    "            all_motifs_fs_task_related_other += ['nan']\n",
    "\n",
    "    return all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other\n",
    "\n",
    "def coactive_rate(filtered_chunk_data):\n",
    "    # how mnay coacitve in chunk: \n",
    "    current_coactive_freqs_chunk = {}\n",
    "    for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "        num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "        if num in current_coactive_freqs_chunk:\n",
    "            current_coactive_freqs_chunk[num] += 1\n",
    "        else:\n",
    "            current_coactive_freqs_chunk[num] = 1\n",
    "            \n",
    "    # total single events that are cocaitve with at least one other\n",
    "    cocative_total = 0\n",
    "    overall_total = 0\n",
    "    for item in list(current_coactive_freqs_chunk):\n",
    "        if item > 1:\n",
    "            cocative_total += current_coactive_freqs_chunk[item]\n",
    "        overall_total += current_coactive_freqs_chunk[item] * item\n",
    "\n",
    "    # coactive_lengths (only coactive, ignore single events)\n",
    "    coactive_len_per_chunk =[]\n",
    "    for item in current_coactive_freqs_chunk:\n",
    "        if item > 1:\n",
    "            coactive_len_per_chunk += current_coactive_freqs_chunk[item] * [item]\n",
    "\n",
    "\n",
    "    return cocative_total,coactive_len_per_chunk,overall_total\n",
    "\n",
    "def empty_chunk_vars():\n",
    "    ## set chunk vars \n",
    "    chunk_vars = {\"chunk_reactivations\" : [],\n",
    "                \"chunk_mins\" : [],\n",
    "                \"chunk_motif_type_reactivations\" : [],\n",
    "                \"mean_spikes_per_event\" : [],\n",
    "                \"motif_by_motif_mean_spikes_per_event\" : [],\n",
    "                \"mean_units_per_event\" : [],\n",
    "                \"motif_by_motif_mean_units_per_event\" : [],\n",
    "                \"chunk_event_lengths\" : [],\n",
    "                \"motif_event_lenghts\" : [],\n",
    "                \"total_single_events_coacitvely_paired\" : [],\n",
    "                \"coactive_lenghts\" : [],\n",
    "                \"overall_total_coactive_or_single_cluster_events\" : [],\n",
    "                \"meaned_order_task_related_ordered\":[],\n",
    "                \"meaned_order_task_related_misordered\":[],\n",
    "                \"meaned_order_task_related_other\":[],\n",
    "                \"fs_order_task_related_ordered\":[],\n",
    "                \"fs_order_task_related_misordered\":[],\n",
    "                \"fs_order_task_related_other\":[],\n",
    "                \"normalised_task_related_total\":[],\n",
    "                \"normalised_non_task_related_total\":[],\n",
    "                \"all_motifs_total_coactive\":[],\n",
    "                \"meaned_ordering_all_motifs_task_related_ordered\":[],\n",
    "                \"meaned_ordering_all_motifs_task_related_misordered\":[],\n",
    "                \"meaned_ordering_all_motifs_task_related_other\":[],\n",
    "                \"fs_ordering_all_motifs_task_related_ordered\":[],\n",
    "                \"fs_ordering_all_motifs_task_related_misordered\":[],\n",
    "                \"fs_ordering_all_motifs_task_related_other\":[],\n",
    "                \n",
    "                \"normalised_task_related_total\":[],\n",
    "                \"normalised_non_task_related_total\":[],\n",
    "                                \n",
    "                \n",
    "\n",
    "    }\n",
    "    return chunk_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this filtering gives...\n",
      " - only sequential events\n",
      "and only those which are in\n",
      " - nrem\n",
      " - rem\n"
     ]
    }
   ],
   "source": [
    "# seq filter takes presedence, if its on: only sequential events, if it is off: all events \n",
    "sequential_filter = True\n",
    "## master switch - turns all sleep filters on/off (if you want all evets turn this off)\n",
    "sleep_filters_on = True\n",
    "# these filters refer to seq one above, and both can be true at the same time. \n",
    "nrem_filter = True\n",
    "rem_filter = True\n",
    "# set this as true (along with the sleep filter one) to override the other two an djust take the background \n",
    "background_only = False\n",
    "\n",
    "\n",
    "## sanity checker / set save path:\n",
    "print('this filtering gives...')\n",
    "if sequential_filter == True:\n",
    "    print(' - only sequential events')\n",
    "    save_var = 'sequential_no_sleep_selected'\n",
    "    type_var = 'sequential'\n",
    "else:\n",
    "    print('- all events')\n",
    "    save_var = 'all_events_no_sleep_selected'\n",
    "    type_var = 'all_events'\n",
    "if sleep_filters_on == True:\n",
    "    if not background_only:\n",
    "        print('and only those which are in')\n",
    "        if nrem_filter == True:\n",
    "            print(' - nrem')\n",
    "            save_var = type_var+'_NREM_sleep'\n",
    "        if rem_filter == True:\n",
    "            print(' - rem')\n",
    "            save_var = type_var+'_REM_sleep'\n",
    "        if nrem_filter == True and rem_filter == True:\n",
    "            save_var = type_var+'_NREM_and_REM_sleep'\n",
    "        \n",
    "    else:\n",
    "        print('and only those which are not in rem/nrem')\n",
    "        save_var = type_var + '_OTHER_nonsleep'\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_ppseq_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\postsleep\\\\\"\n",
    "out_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\replay_to_behaviour\\\\\"\n",
    "useable_mirs  = ['136_1_3','136_1_4','149_1_1','178_1_6','178_1_7','178_1_8','178_2_1','178_2_2','178_2_3','268_1_2','269_1_2','269_1_4','270_1_6']\n",
    "#['ap5r_1_1','ap5r_1_2','ap5r_1_3','seq006_1_1','seq006_1_10','seq006_1_11','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq006_1_8','seq006_1_9','seq007_1_1','seq007_1_2','seq007_1_3','seq007_1_4','seq008_1_3']\n",
    "# load in sleep time points\n",
    "sleep_time_point_df = pd.read_csv(sleep_ppseq_path + 'sleep_time_points.csv')\n",
    "# decide when sleep started\n",
    "sleep_start = {}\n",
    "for index,value in enumerate(sleep_time_point_df.approx_sleep_start.values):\n",
    "    mouse = sleep_time_point_df.mir.values[index]\n",
    "    sleep_start[mouse] = value\n",
    "# define mice/sessions in each group    \n",
    "expert_mice = sleep_time_point_df[sleep_time_point_df.group == 'expert'].mir.values\n",
    "hlesion_mice = sleep_time_point_df[sleep_time_point_df.group == 'h_lesion'].mir.values\n",
    "learning_mice = sleep_time_point_df[sleep_time_point_df.group == 'learning'].mir.values\n",
    "\n",
    "\n",
    "var_dict = {'expert':[],'hlesion':[],'learning' :[],'mirs':[],'current_sleep_start':[], 'time_spans':[]}\n",
    "\n",
    "#Load in seq order data \n",
    "sequence_order_df = pd.read_csv(sleep_ppseq_path+\"sequence_order.csv\")\n",
    "# get all the relevant path name and some other data \n",
    "current_mouse_path,var_dict = find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start)\n",
    "# loop across each mouse path:\n",
    "for loop_index, path in enumerate(current_mouse_path):\n",
    "    # create empty chunk vars dict\n",
    "    chunk_vars = empty_chunk_vars()\n",
    "    \n",
    "    print(f\"run index: {loop_index}, processing {var_dict['mirs'][loop_index]}\")\n",
    "\n",
    "    ## loop across all chunk files\n",
    "    for chunk_index, file in enumerate(os.listdir(path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            path_ = path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(path_ + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(path_ + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            # filter based on the sequential/rem-nrem conditions set above\n",
    "            filter_mask = make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only)\n",
    "            filtered_chunk_data = data[filter_mask].reset_index()\n",
    "            \n",
    "            # how many reactivations found\n",
    "            reactivations_found = len(filtered_chunk_data)\n",
    "            print(reactivations_found)\n",
    "            \n",
    "            ####################################### chunk rate per minute: (# this one depends on rem/nrem filter... )\n",
    "            mins = determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path_)\n",
    "            if mins > 0:\n",
    "                chunk_vars['chunk_reactivations'] += [reactivations_found] \n",
    "                chunk_vars['chunk_mins'] += [mins]     \n",
    "                \n",
    "            ####################################### replay rate per motif type\n",
    "            all_motif_type_reactivations = []\n",
    "            all_motif_type_reactivations_min = []\n",
    "            all_motif_type_relative_proportion = []\n",
    "            for seq_type in range(1,7):\n",
    "                motif_type_reactivations = [len(np.where(filtered_chunk_data.cluster_seq_type.values == seq_type)[0])][0]\n",
    "                all_motif_type_reactivations += [motif_type_reactivations]\n",
    "            chunk_vars['chunk_motif_type_reactivations'] += [all_motif_type_reactivations]\n",
    "            \n",
    "            ##################################### av. spikes involved\n",
    "            chunk_vars['mean_spikes_per_event'] += [[len(item) for item in filtered_chunk_data.cluster_spike_times]]\n",
    "            # per motif\n",
    "            motif_by_motif_mean_spikes_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_spikes_per_event += [[len(item) for item in motif_data.cluster_spike_times]]\n",
    "            chunk_vars['motif_by_motif_mean_spikes_per_event'] += [motif_by_motif_mean_spikes_per_event]  \n",
    "                    \n",
    "            ########################################## average units involved \n",
    "            chunk_vars['mean_units_per_event'] += [[len(np.unique(ast.literal_eval(item))) for item in filtered_chunk_data.cluster_neurons]]\n",
    "            motif_by_motif_mean_units_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_units_per_event += [[len(np.unique(ast.literal_eval(item))) for item in motif_data.cluster_neurons]]\n",
    "            chunk_vars['motif_by_motif_mean_units_per_event'] += [motif_by_motif_mean_units_per_event]\n",
    "            \n",
    "            ########################################### replay length overall \n",
    "            chunk_vars['chunk_event_lengths'] += [filtered_chunk_data.event_length.values]\n",
    "            \n",
    "            ########################################### replay length per motif \n",
    "            motif_event_lenghts = []\n",
    "            for i in range(1,7):\n",
    "                motif_event_lenghts += [filtered_chunk_data[filtered_chunk_data.cluster_seq_type == i].event_length.values]\n",
    "            chunk_vars['motif_event_lenghts'] += [motif_event_lenghts]\n",
    "            \n",
    "            ########################################### coactive rate overall\n",
    "            event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "            # refind the clusters\n",
    "            filtered_chunk_data  = refind_cluster_events(filtered_chunk_data,event_proximity_filter)\n",
    "            # how many single evtns coactivly paired? average coactive rate? proportion of global events coactive? \n",
    "            cocative_total,coactive_len_per_chunk,overall_total = coactive_rate(filtered_chunk_data)\n",
    "            chunk_vars['total_single_events_coacitvely_paired'] += [cocative_total]\n",
    "            chunk_vars['coactive_lenghts'] += [coactive_len_per_chunk]\n",
    "            chunk_vars['overall_total_coactive_or_single_cluster_events'] += [overall_total]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # ordering of coactive?\n",
    "            if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "                multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "            else:\n",
    "                multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "            # pull out sequence order for current mouse\n",
    "            seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "            num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "            real_order = np.array(seq_order)+1\n",
    "\n",
    "            #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "            if not len(real_order) == num_dominant_seqs:\n",
    "                dominant = list(real_order[0:num_dominant_seqs])\n",
    "                other_ = list(real_order[num_dominant_seqs::])\n",
    "            else:\n",
    "                dominant = list(real_order)\n",
    "                other_ = []\n",
    "                \n",
    "            # orderng amounts for mean ordering - this calculated for each pair in the chunk \n",
    "            ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)\n",
    "            chunk_vars['meaned_order_task_related_ordered'] += [ordered]\n",
    "            chunk_vars['meaned_order_task_related_misordered'] += [misordered]\n",
    "            chunk_vars['meaned_order_task_related_other'] += [other]\n",
    "            \n",
    "            # orderng amounts for first spike ordering\n",
    "            ordered,misordered,other = calculate_ordering_amounts(fs_order,dominant,other_)\n",
    "            chunk_vars['fs_order_task_related_ordered'] += [ordered]\n",
    "            chunk_vars['fs_order_task_related_misordered'] += [misordered]\n",
    "            chunk_vars['fs_order_task_related_other'] += [other]\n",
    "            ### motif by motif:\n",
    "            # does one motif appear more in coactive?\n",
    "            if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "                all_motifs_total_coactive = all_motifs_proportion_coactive(multi_cluster_df)\n",
    "                chunk_vars['all_motifs_total_coactive'] += [all_motifs_total_coactive]\n",
    "            else:\n",
    "                chunk_vars['all_motifs_total_coactive'] += [[0,0,0,0,0,0]]\n",
    "            \n",
    "            # does one motif appeaer more ordered? \n",
    "            # # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "            # meaned ordering \n",
    "            all_motifs_meaned_ordering_task_related_ordered,all_motifs_meaned_ordering_task_related_misordered,all_motifs_meaned_ordering_task_related_other = motif_by_motif_ordering(meaned_order,real_order,dominant,other_)\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_ordered'] += [all_motifs_meaned_ordering_task_related_ordered]\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_misordered'] += [all_motifs_meaned_ordering_task_related_misordered]\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_other'] += [all_motifs_meaned_ordering_task_related_other]\n",
    "            \n",
    "            # first spike ordering \n",
    "            all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other = motif_by_motif_ordering(fs_order,real_order,dominant,other_)\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_ordered'] += [all_motifs_fs_task_related_ordered]\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_misordered'] += [all_motifs_fs_task_related_misordered]\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_other'] += [all_motifs_fs_task_related_other]\n",
    "\n",
    "            ########################################### task related vs other rate\n",
    "            task_seqs = np.array(seq_order)+1\n",
    "            # mask each condition\n",
    "            mask = np.isin(filtered_chunk_data.cluster_seq_type.values, task_seqs)\n",
    "            opposite_mask = ~mask\n",
    "            task_related = filtered_chunk_data[mask]\n",
    "            non_task_related = filtered_chunk_data[opposite_mask]\n",
    "\n",
    "            #  task v nontask overallrate\n",
    "            chunk_vars['normalised_task_related_total'] += [len(task_related)/len(task_seqs)]\n",
    "            if len(task_seqs) == 6:\n",
    "                chunk_vars['normalised_non_task_related_total'] += [0]\n",
    "            else:\n",
    "                chunk_vars['normalised_non_task_related_total'] += [len(non_task_related)/(6-len(task_seqs))]\n",
    "            \n",
    "            ### extra stuff to add in:\n",
    "            \n",
    "            # same but motif by motif\n",
    "            \n",
    "            # number of spikes task related \n",
    "            # number of units task related\n",
    "            \n",
    "            # # coative rate \n",
    "            # task_related_number = 0\n",
    "            # non_task_related_number = 0\n",
    "            # for coactive_ in meaned_order:\n",
    "            #     for motif_item in coactive_:\n",
    "            #         if motif_item in task_seqs:\n",
    "            #             task_related_number += 1\n",
    "            #         else:\n",
    "            #             non_task_related_number += 1\n",
    "            # # make it relative:\n",
    "            # task_related_number = task_related_number/len(task_seqs) \n",
    "            # non_task_related_number = non_task_related_number/(6-len(task_seqs))\n",
    "            # proportion_coacitve_event_that_are_task_related = task_related_number/(task_related_number+non_task_related_number)\n",
    "            # chunk_vars['proportion_coacitve_event_that_are_task_related'] = proportion_coacitve_event_that_are_task_related\n",
    "            \n",
    "            # # motif coactive rate for task and non task \n",
    "            # # task\n",
    "            # task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "            # task_proportion_single_events_coacitvely_paired,task_av_coactive_len_per_chunk,task_proporiton_of_events_coactive = coactive_rate(task_related)\n",
    "            # # non task:\n",
    "            # non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "            # nontask_proportion_single_events_coacitvely_paired,nontask_av_coactive_len_per_chunk,nontask_proporiton_of_events_coactive = coactive_rate(non_task_related)\n",
    "\n",
    "            # replay length\n",
    "            #task v non task\n",
    "            # motif by motif  \n",
    "            # spikes involved\n",
    "            \n",
    "            # save out to newly made place\n",
    "            \n",
    "        ###now do averages for each chunk and save out to a new file\n",
    "        ########## Calculate averages across chunks/ combine across chunks for each data variable\n",
    "        out_vars = {}\n",
    "\n",
    "        # 1 overall event rate \n",
    "        if sum(chunk_vars['chunk_reactivations']) == 0:\n",
    "            out_vars['event_rpm'] = 0\n",
    "        else:\n",
    "            out_vars['event_rpm'] = sum(chunk_vars['chunk_reactivations'])/sum(chunk_vars['chunk_mins'])\n",
    "\n",
    "        # 2 motif by motif event rate \n",
    "        out_vars['motif_event_rpm'] = np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)/sum(chunk_vars['chunk_mins'])\n",
    "\n",
    "        # 3 motif by motif proportion of all events\n",
    "        out_vars['motif_relative_event_proportion'] = np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)/sum(np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0))\n",
    "\n",
    "        # 4 spikes per replay event\n",
    "        chunk_spikes_per_event = chunk_vars['mean_spikes_per_event']\n",
    "        spikes_per_event = [item for sublist in chunk_spikes_per_event for item in sublist]\n",
    "        out_vars['spikes_per_event'] = spikes_per_event\n",
    "\n",
    "        # 5 motif by motif spikes per replay event\n",
    "        motif_by_motif_spikes_per_event = []\n",
    "        for seq in range(1,7):\n",
    "            motif_spikes_per_event = []\n",
    "            for chunk_ in chunk_vars['motif_by_motif_mean_spikes_per_event']:\n",
    "                motif_spikes_per_event +=chunk_[seq-1]\n",
    "            motif_by_motif_spikes_per_event += [motif_spikes_per_event]\n",
    "        out_vars['motif_by_motif_spikes_per_event'] = motif_by_motif_spikes_per_event\n",
    "\n",
    "        # 6 units per event\n",
    "        chunk_units_per_event = chunk_vars['mean_units_per_event']\n",
    "        units_per_event = [item for sublist in chunk_units_per_event for item in sublist]\n",
    "        out_vars['units_per_event'] = units_per_event\n",
    "\n",
    "        # 7 motif by motif units per replay event\n",
    "        motif_by_motif_units_per_event = []\n",
    "        for seq in range(1,7):\n",
    "            motif_units_per_event = []\n",
    "            for chunk_ in chunk_vars['motif_by_motif_mean_units_per_event']:\n",
    "                motif_units_per_event +=chunk_[seq-1]\n",
    "            motif_by_motif_units_per_event += [motif_units_per_event]\n",
    "        out_vars['motif_by_motif_units_per_event'] = motif_by_motif_units_per_event\n",
    "\n",
    "        # 8 event lengths\n",
    "        chunk_event_lengths = chunk_vars['chunk_event_lengths']\n",
    "        event_lengths = [item for sublist in chunk_event_lengths for item in sublist]\n",
    "        out_vars['event_lengths'] = event_lengths\n",
    "\n",
    "        # 9 motif by motif event lengths\n",
    "        motif_by_motif_event_lengths = []\n",
    "        for seq in range(1,7):\n",
    "            motif_event_lengths = []\n",
    "            for chunk_ in chunk_vars['motif_event_lenghts']:\n",
    "                motif_event_lengths +=list(chunk_[seq-1])\n",
    "            motif_by_motif_event_lengths += [motif_event_lengths]\n",
    "        out_vars['motif_by_motif_event_lengths'] = motif_by_motif_event_lengths\n",
    "\n",
    "        # 10 coactive rate (proportion of single events coacitvly paired)\n",
    "        if sum(chunk_vars['total_single_events_coacitvely_paired']) == 0:\n",
    "            out_vars['proportion_single_events_coactivly_paired'] = 0\n",
    "        else:\n",
    "            out_vars['proportion_single_events_coactivly_paired'] = sum(chunk_vars['total_single_events_coacitvely_paired'])/sum(chunk_vars['overall_total_coactive_or_single_cluster_events'])\n",
    "\n",
    "\n",
    "        # 11 number of motifs in each coative group\n",
    "        out_vars['coactive_group_lengths'] = [item for sublist in chunk_vars['coactive_lenghts']  for item in sublist]\n",
    "\n",
    "        # 12 meaned order\n",
    "        # ordered proporiton out of all \n",
    "        ordered = sum(chunk_vars['meaned_order_task_related_ordered'])\n",
    "        misordered = sum(chunk_vars['meaned_order_task_related_misordered'])\n",
    "        other = sum(chunk_vars['meaned_order_task_related_other'])\n",
    "        if ordered+misordered+other == 0:\n",
    "            out_vars['meaned_order_overall_ordered_prop'] = 0\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            out_vars['meaned_order_task_related_ordered_prop'] = 0\n",
    "            # other proportion\n",
    "            out_vars['meaned_order_overall_other_prop'] = 0\n",
    "        else:\n",
    "            out_vars['meaned_order_overall_ordered_prop'] = ordered/(ordered+misordered+other)\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            if ordered+misordered == 0:\n",
    "                out_vars['meaned_order_task_related_ordered_prop'] = 0\n",
    "            else:\n",
    "                out_vars['meaned_order_task_related_ordered_prop'] = ordered/(ordered+misordered)\n",
    "            # other proportion\n",
    "            out_vars['meaned_order_overall_other_prop'] = other/(ordered+misordered+other)\n",
    "\n",
    "        # 13 first spike order\n",
    "        # ordered proporiton out of all \n",
    "        ordered = sum(chunk_vars['fs_order_task_related_ordered'])\n",
    "        misordered = sum(chunk_vars['fs_order_task_related_misordered'])\n",
    "        other = sum(chunk_vars['fs_order_task_related_other'])\n",
    "        if ordered+misordered+other == 0:\n",
    "            out_vars['fs_order_overall_ordered_prop'] = 0\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            out_vars['fs_order_task_related_ordered_prop'] = 0\n",
    "            # other proportion\n",
    "            out_vars['fs_order_overall_other_prop'] = 0\n",
    "        else:\n",
    "            out_vars['fs_order_overall_ordered_prop'] = ordered/(ordered+misordered+other)\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            if ordered+misordered == 0:\n",
    "                out_vars['fs_order_task_related_ordered_prop'] = 0\n",
    "            else:\n",
    "                out_vars['fs_order_task_related_ordered_prop'] = ordered/(ordered+misordered)\n",
    "            # other proportion\n",
    "            out_vars['fs_order_overall_other_prop'] = other/(ordered+misordered+other)\n",
    "\n",
    "        # 14 motif by motif proportion coactive, what proprotion of each motif is coactive\n",
    "        out_vars['motif_proportion_coactive'] = np.sum(chunk_vars['all_motifs_total_coactive'],axis = 0)/np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)\n",
    "\n",
    "        # 15 does one motif appeaer more ordered? Proportion of moitfs that were in ordered events for coactive task related motifs\n",
    "        # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "        ordered_prop = []\n",
    "        for seq in range(1,7):\n",
    "            ordered = []\n",
    "            misordered = []\n",
    "            other  = []\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_ordered']:\n",
    "                ordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_misordered']:\n",
    "                misordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_other']:\n",
    "                other += [chunk[seq-1]]\n",
    "            if not 'nan' in ordered and (sum(ordered)+sum(misordered)+sum(other)) > 0:\n",
    "                ordered_prop += [sum(ordered)/(sum(ordered)+sum(misordered)+sum(other))]\n",
    "            else:\n",
    "                ordered_prop += ['nan']\n",
    "        out_vars['motif_meaned_ordering_ordered_prop_out_of_all_task_related'] = ordered_prop\n",
    "\n",
    "        # 16 same but for first spike ordering\n",
    "        ordered_prop = []\n",
    "        for seq in range(1,7):\n",
    "            ordered = []\n",
    "            misordered = []\n",
    "            other  = []\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_ordered']:\n",
    "                ordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_misordered']:\n",
    "                misordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_other']:\n",
    "                other += [chunk[seq-1]]\n",
    "            if not 'nan' in ordered and (sum(ordered)+sum(misordered)+sum(other)) > 0:\n",
    "                ordered_prop += [sum(ordered)/(sum(ordered)+sum(misordered)+sum(other))]\n",
    "            else:\n",
    "                ordered_prop += ['nan']\n",
    "        out_vars['motif_fs_ordering_ordered_prop_out_of_all_task_related'] = ordered_prop\n",
    "        \n",
    "        if sum(chunk_vars['normalised_non_task_related_total']) == 0:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = sum(chunk_vars['normalised_task_related_total'])\n",
    "        elif sum(chunk_vars['normalised_task_related_total']) == 0:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = 0\n",
    "        else:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = sum(chunk_vars['normalised_task_related_total'])/sum(chunk_vars['normalised_non_task_related_total'])\n",
    "\n",
    "\n",
    "        ####### SAVE OUT THE DATA \n",
    "\n",
    "        try: \n",
    "            int(var_dict['mirs'][loop_index].split('_')[0])\n",
    "            current_save_path = out_path + 'EJT' + var_dict['mirs'][loop_index] + '\\\\replay\\\\' + save_var \n",
    "        except:\n",
    "            current_save_path = out_path + var_dict['mirs'][loop_index] + '\\\\replay\\\\' + save_var \n",
    "        \n",
    "        ## if the path doesnt exist, make a new dir \n",
    "        if not os.path.exists(current_save_path):\n",
    "            os.makedirs(current_save_path)\n",
    "            \n",
    "        # convert all np arrays to list\n",
    "        for key, value in out_vars.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                out_vars[key] = value.tolist()\n",
    "\n",
    "        ### save out the out_vars dict\n",
    "        with open(current_save_path + '\\\\replay_data_variables.json', 'w') as file:\n",
    "            json.dump(out_vars, file)\n",
    "            \n",
    "        # convert all np arrays to list\n",
    "        for key, value in var_dict.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                var_dict[key] = value.tolist()\n",
    "            \n",
    "        ### save out the var_dict \n",
    "        with open(current_save_path + '\\\\general_mouse_info.json', 'w') as file:\n",
    "            json.dump(var_dict, file)\n",
    "            \n",
    "    print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in seq order data \n",
    "sequence_order_df = pd.read_csv(sleep_ppseq_path+\"sequence_order.csv\")\n",
    "# get all the relevant path name and some other data \n",
    "current_mouse_path,var_dict = find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start)\n",
    "# loop across each mouse path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
