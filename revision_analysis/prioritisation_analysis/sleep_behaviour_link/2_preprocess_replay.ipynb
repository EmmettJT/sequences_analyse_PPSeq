{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast \n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def create_multicluster_dataframe(filtered_chunk_data):\n",
    "    meaned_order = []\n",
    "    fs_order = []\n",
    "    event_times = []\n",
    "    count = 0\n",
    "    for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "        group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "        current_cluster = filtered_chunk_data[group_mask].copy()\n",
    "        if len(current_cluster) > 1:\n",
    "            means = []\n",
    "            event_types = []\n",
    "            fs_orders = []\n",
    "            for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                # calculate event order based on spike time weighted mean\n",
    "                means += [np.mean(ast.literal_eval(events))]\n",
    "                # calculate order based on first spike time:\n",
    "                fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "            # order by mean time:    \n",
    "            meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "            # order by first spike:\n",
    "            fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "            event_times += [fs_orders]\n",
    "\n",
    "            current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "            current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "            current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "            if count == 0:\n",
    "                multi_cluster_df = current_cluster.copy()\n",
    "            else:\n",
    "                # Concatenate the DataFrames vertically (row-wise)\n",
    "                multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                # Reset the index if needed\n",
    "                multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "            count += 1\n",
    "    return multi_cluster_df,meaned_order,fs_order\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def refind_cluster_events(filtered_chunk_data,event_proximity_filter):\n",
    "    \n",
    "    ### ignore the origonal clusterg rosp and remake them: \n",
    "    start_times = filtered_chunk_data.first_spike_time.values\n",
    "    end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "    clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "    cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "    for index,cluster in enumerate(clustered_events):\n",
    "        for item in cluster:\n",
    "            cluster_group[item] = int(index)\n",
    "    filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "    \n",
    "    return filtered_chunk_data\n",
    "\n",
    "def find_data_paths(all_mice_dict,sleep_path):\n",
    "    for i,file in enumerate(os.listdir(sleep_path)):\n",
    "        print(i)\n",
    "        if 'run' in file:\n",
    "            paths_dict = {}\n",
    "            mir = file.split('run')[0][0:-1]\n",
    "            paths_dict['sleep_path'] = sleep_path + file\n",
    "            awake_base = os.path.join(remove_last_folder(sleep_path), 'awake')\n",
    "            for awake_file in os.listdir(awake_base):\n",
    "                if mir in awake_file:\n",
    "                    paths_dict['awake_path'] = os.path.join(awake_base, awake_file)\n",
    "            try: \n",
    "                paths_dict['full_org_dat_path'] = find_organised_path('EJT' + mir,r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\")\n",
    "            except:\n",
    "                paths_dict['full_org_dat_path']  = find_organised_path(mir,r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\")\n",
    "            all_mice_dict[mir] = paths_dict\n",
    "    return all_mice_dict\n",
    "\n",
    "def remove_last_folder(path: str) -> str:\n",
    "    # 1. Normalize: collapse duplicate slashes, strip trailing ones\n",
    "    normalized = os.path.normpath(path)\n",
    "    # 2. dirname: drop the last component\n",
    "    return os.path.dirname(normalized)\n",
    "\n",
    "def find_organised_path(mir,dat_path):\n",
    "    dat_path_2 = None\n",
    "    recording = None\n",
    "    print(mir)\n",
    "    for animal_implant in os.listdir(dat_path):\n",
    "        current_m_i = '_'.join([animal_implant.split('_')[0],animal_implant.split('_')[-1][-1]])\n",
    "        mi = '_'.join(mir.split('_')[0:-1])\n",
    "        if current_m_i == mi:\n",
    "            dat_path_2 = os.path.join(dat_path,animal_implant)\n",
    "            break\n",
    "    print(dat_path_2)\n",
    "    for ind,item in enumerate([record.split('ing')[-1].split('_')[0] for record in os.listdir(dat_path_2)]):\n",
    "        if item == mir.split('_')[-1]:\n",
    "            recording = os.listdir(dat_path_2)[ind]\n",
    "    full_org_dat_path = os.path.join(dat_path_2,recording)\n",
    "    print(full_org_dat_path)\n",
    "    return full_org_dat_path\n",
    "\n",
    "def load_in_sleep_state_scoring(mouse):\n",
    "    print('---------------------')\n",
    "    print('searching for sleep state scoring')\n",
    "    # determine organised data paths for the current mouse\n",
    "    if mouse.split('_')[0].isdigit():\n",
    "        org_dat_path = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "        old_data = True\n",
    "    else:\n",
    "        org_dat_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "        old_data = False\n",
    "    org_mouse_file = None\n",
    "    for file in os.listdir(org_dat_path):\n",
    "        if mouse.split('_')[0] in file:\n",
    "            # if the implant = implant\n",
    "            if mouse.split('_')[1] == file.split('_')[-1][-1]:\n",
    "                print(f'1. mouse file found: \\033[1m{file}\\033[0m')\n",
    "                org_mouse_file = os.path.join(org_dat_path,file)\n",
    "    mouse_org_data_path = None\n",
    "    for recording in os.listdir(org_mouse_file):\n",
    "        if mouse.split('_')[-1] == recording.split('ing')[-1].split('_')[0]:\n",
    "            print(f'2. recording found: \\033[1m{recording}\\033[0m')\n",
    "            mouse_org_data_path = os.path.join(org_mouse_file,recording) + r'\\\\'\n",
    "\n",
    "    # load in sleep scoring data \n",
    "\n",
    "    sleep_state_score_path = mouse_org_data_path + r\"\\ephys\\LFP\\\\sleep_state_score\\\\\"\n",
    "    if not os.path.exists(sleep_state_score_path):\n",
    "        sleep_state_score_path =  mouse_org_data_path + '/ephys/probeA/LFP/'\n",
    "\n",
    "    if not os.path.exists(sleep_state_score_path):\n",
    "        print(f\"Sleep state score files not found for {mouse}.\")\n",
    "    else:\n",
    "        nrem_start_ends = np.load(sleep_state_score_path + \"nrem_start_ends.npy\", allow_pickle=True)\n",
    "        rem_start_ends = np.load(sleep_state_score_path + \"rem_start_ends.npy\", allow_pickle=True)\n",
    "        print (f\"\\033[1mSuccess!\\033[0m Loaded sleep state score files for mouse: {mouse}.\")\n",
    "\n",
    "    print('----------------------')\n",
    "        \n",
    "    return nrem_start_ends,rem_start_ends,mouse_org_data_path,old_data\n",
    "\n",
    "def get_chunk_state_times(rem_start_ends,chunk_time):\n",
    "    chunk_rem_times = []\n",
    "    for start,end in rem_start_ends:\n",
    "        if start >= chunk_time[0] and start <= chunk_time[1]:\n",
    "            start_chunk_rebased = start - chunk_time[0]\n",
    "            end_chunk_rebased = end - chunk_time[0]\n",
    "            # expand by 10%\n",
    "            start_chunk_rebased = start_chunk_rebased * 0.9\n",
    "            end_chunk_rebased = end_chunk_rebased * 1.1\n",
    "            # if the end stetches past the end of the chunk then just set it to the end of the chunk\n",
    "            if end_chunk_rebased > np.diff(chunk_time)[0]:\n",
    "                end_chunk_rebased = np.diff(chunk_time)[0]\n",
    "            chunk_rem_times += [[start_chunk_rebased,end_chunk_rebased]]\n",
    "    return chunk_rem_times\n",
    "\n",
    "def reactivation_per_minute(nrem_filtered_chunk_data, nrem_state_times):\n",
    "    # save out data\n",
    "    reactivations_found = len(nrem_filtered_chunk_data)\n",
    "\n",
    "    if len(nrem_state_times) > 0:\n",
    "        mins = np.diff(nrem_state_times)\n",
    "    else:\n",
    "        mins = 0\n",
    "        \n",
    "    # convert to mins            \n",
    "    mins = mins/60\n",
    "\n",
    "    if mins > 0:\n",
    "        return reactivations_found/mins\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def find_coactive_pair_rate(chunk_data_clusters,seq_id):\n",
    "    coactive_clusts = 0\n",
    "    total = 0\n",
    "    for index,item_group in enumerate(chunk_data_clusters.coactive_cluster_group):\n",
    "        if chunk_data_clusters.cluster_seq_type[index] == seq_id+1:\n",
    "            full_group = np.where(chunk_data_clusters.coactive_cluster_group == item_group)[0]\n",
    "            if len(full_group) > 1:\n",
    "                coactive_clusts += 1\n",
    "            total += 1\n",
    "    if total > 0:\n",
    "        return coactive_clusts,total\n",
    "    else:\n",
    "        return np.nan,np.nan\n",
    "    \n",
    "\n",
    "import pickle\n",
    "\n",
    "def process_awake_data_return_seq_dfs(unmasked_spikes_df, chunk_time, awake_neuron_order, colors, plotting_limit,bin_size=0.2, seq_size_threshold=5):\n",
    "    \"\"\"\n",
    "    Processes spike data to extract time-localized spike events for each sequence type (1–6).\n",
    "    \"\"\"\n",
    "    seq_types = np.unique(unmasked_spikes_df.sequence_type_adjusted)\n",
    "\n",
    "    # Gather spike timestamps by sequence type\n",
    "    seq_spikes = [unmasked_spikes_df.timestamp[unmasked_spikes_df.sequence_type_adjusted == seq_type].values for seq_type in seq_types]\n",
    "\n",
    "    # Compute binned spike histograms\n",
    "    seq_spike_occurrence = [list(np.histogram(spikes, bins=np.arange(0, np.diff(chunk_time)[0], bin_size))[0]) for spikes in seq_spikes]\n",
    "\n",
    "    seq_event_dfs = []\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 5))\n",
    "    for i in range(1, 7):  # process sequence types 1–6\n",
    "        print(f\"Processing sequence type: {i}\")\n",
    "        seq_spike_count = seq_spike_occurrence[i]\n",
    "        groups = return_inds_for_seq_groups(seq_spike_count)\n",
    "\n",
    "        # Plot sequence summary (raster + histogram)\n",
    "        plot_sequence_summary(ax1,ax2,unmasked_spikes_df, awake_neuron_order, colors, seq_spike_count, groups, i, plotting_limit,bin_size)\n",
    "\n",
    "        # Extract sequence events as separate DataFrames\n",
    "        seq_event_dfs.extend(\n",
    "            extract_sequence_events(unmasked_spikes_df, i, groups, bin_size, seq_size_threshold)\n",
    "        )\n",
    "\n",
    "    return seq_event_dfs\n",
    "\n",
    "def split_sequence_events(seq_event_dfs):\n",
    "    # Split sequence events into a dictionary by sequence type.\n",
    "    # Each key is the sequence type, and the value is a list of DataFrames for those events\n",
    "    seq_events = {}\n",
    "    for event in seq_event_dfs:\n",
    "        # add the sequence type as a key if it doesn't exist\n",
    "        if str(int(event.sequence_type_adjusted.values[0])) not in seq_events:\n",
    "            seq_events[str(int(event.sequence_type_adjusted.values[0]))] = [event]\n",
    "        else:\n",
    "            seq_events[str(int(event.sequence_type_adjusted.values[0]))].append(event)\n",
    "    return seq_events\n",
    "\n",
    "def return_inds_for_seq_groups(lst):\n",
    "    groups = []\n",
    "    new = True\n",
    "    for ind,item in enumerate(lst):\n",
    "        if new:\n",
    "            if item > 0:\n",
    "                start = ind\n",
    "                new = False\n",
    "        else:\n",
    "            if item == 0:\n",
    "                end = ind-1\n",
    "                groups.append((start, end))\n",
    "                new = True\n",
    "    return groups\n",
    "\n",
    "\n",
    "def plot_sequence_summary(ax1,ax2,unmasked_spikes_df, awake_neuron_order, colors, seq_spike_count, groups, i, time_window, bin_size):\n",
    "    \"\"\"\n",
    "    Plots spike raster and sequence histogram for a given sequence type.\n",
    "    \"\"\"\n",
    "    # Filter spikes within the plotting window\n",
    "    mask = (unmasked_spikes_df.timestamp > time_window[0]) & (unmasked_spikes_df.timestamp < time_window[1])\n",
    "    visible_spikes = unmasked_spikes_df[mask]\n",
    "    valid_seq_mask = visible_spikes.sequence_type_adjusted >= 0\n",
    "    spike_colors = np.array(colors)[visible_spikes[valid_seq_mask].sequence_type_adjusted.values.astype(int)]\n",
    "\n",
    "    # Spike raster\n",
    "    ax1.scatter(\n",
    "        visible_spikes[valid_seq_mask].timestamp,\n",
    "        awake_neuron_order[mask][valid_seq_mask],\n",
    "        marker='o', s=40, linewidth=0, color=spike_colors, alpha=1\n",
    "    )\n",
    "\n",
    "    # Histogram and detected groups\n",
    "    ax2.plot(seq_spike_count, color=colors[i])\n",
    "    for start, end in groups:\n",
    "        ax2.plot([start, end], [-5, -5], color='red')\n",
    "\n",
    "    ax1.set_xlim([time_window[0], time_window[-1]])\n",
    "    ax2.set_xlim(time_window[0]/bin_size,time_window[-1] / bin_size)\n",
    "\n",
    "def extract_sequence_events(df, sequence_type, groups, bin_size, seq_size_threshold):\n",
    "    \"\"\"\n",
    "    Extracts spike events from continuous groups of time bins for a specific sequence type.\n",
    "    \"\"\"\n",
    "    extracted = []\n",
    "\n",
    "    for start, end in groups:\n",
    "        group_start_time = (start * bin_size) - 0.5\n",
    "        group_end_time = (end * bin_size) + 0.5\n",
    "\n",
    "        time_mask = (df.timestamp > group_start_time) & (df.timestamp < group_end_time)\n",
    "        group_spikes = df[time_mask]\n",
    "        matching_seq = group_spikes[group_spikes.sequence_type_adjusted == sequence_type]\n",
    "\n",
    "        if len(matching_seq) > seq_size_threshold:\n",
    "            extracted.append(matching_seq)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "def calcuate_neuron_consistency(seq_id, unmasked_spikes_df, seq_events_dict):\n",
    "    \"\"\"\n",
    "    Calculate the proportion of times each neuron appears in the sequence events for a given sequence type.\n",
    "    \"\"\"\n",
    "    proportion_appeared = [] \n",
    "    neurons_appeared = []\n",
    "    for neuron_id in unmasked_spikes_df.neuron.unique():\n",
    "        appears = 0\n",
    "        for event in seq_events_dict[str(seq_id)]:\n",
    "            if neuron_id in event.neuron.values:\n",
    "                appears += 1      \n",
    "        if appears > 0:\n",
    "            total_events = len(seq_events_dict[str(seq_id)])\n",
    "            proportion_appeared += [appears/total_events]\n",
    "            neurons_appeared += [neuron_id]   \n",
    "    return proportion_appeared,neurons_appeared\n",
    "                    \n",
    "### spindle filtering!\n",
    "#compute a smoothed envelope of this signal, the magnitude of the Hilbert transforms with convolving by a Gaussian window (200 ms)\n",
    "\n",
    "# We computed a smoothed envelope of this signal, the magnitude of the Hilbert transforms with convolving by a Gaussian window (200 ms). \n",
    "# Next, we determined two thresholds for spindle detection based on the mean (μ) and standard deviation (σ) of the spindle band envelope during NREM sleep; \n",
    "# the upper and lower thresholds were set μ + 2.5 × σ and μ + 1.5 × σ, respectively. Epochs in which the spindle power exceeded the upper threshold for at least one sample \n",
    "# and the spindle power exceeded the lower threshold for at least 500 ms were considered spindles. \n",
    "# Each epoch where the spindle power exceeded the lower threshold was considered the start and stop of the spindle; the duration of each spindle was based on these values as well. \n",
    "# For the reactivation analyses spindles was detected in 13-16 Hz based on the previous finding that fast spindles (13-16 Hz) play a key role for sleep-dependent memory reactivation\n",
    "\n",
    "from scipy.signal import hilbert, convolve\n",
    "\n",
    "\n",
    "def filter_for_spindles_and_plot(spindle_bandpassed,nrem_intervals,rem_intervals,fs,min_event_duration,sleep_period_ephys_start_time):\n",
    "\n",
    "    time = np.arange(len(spindle_bandpassed)) / fs\n",
    "    # adjust time to account for trimmed down ephys lfp\n",
    "    time += sleep_period_ephys_start_time\n",
    "\n",
    "    # Compute the smoothed envelope\n",
    "    smoothed_envelope = compute_smoothed_envelope(spindle_bandpassed, fs)\n",
    "\n",
    "    ## filter for NREM sleep times only and find mean and s-dev\n",
    "    nrem_envelope_perioids = []\n",
    "    for interval in nrem_intervals:\n",
    "        nrem_envelope_perioids += list(smoothed_envelope[(time > interval[0]) * (time < interval[1])])\n",
    "    mean_ = np.mean(nrem_envelope_perioids)\n",
    "    std_ = np.std(nrem_envelope_perioids)\n",
    "\n",
    "    upper_threshold = mean_ + 2*std_\n",
    "    lower_threshold = mean_ + 1*std_\n",
    "\n",
    "    # Plotting the original signal and the smoothed envelope\n",
    "    fig, [ax,ax2] = plt.subplots(2, 1,figsize=(20, 10))\n",
    "    ax.plot(time, spindle_bandpassed, color='b')\n",
    "    ax.plot(time, smoothed_envelope, label='Smoothed Envelope', color='r')\n",
    "    ax.axhline(upper_threshold,color = 'c')\n",
    "    ax.axhline(lower_threshold,color = 'c')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_xlim(sleep_period_ephys_start_time+100,sleep_period_ephys_start_time+120)\n",
    "    ax.set_ylim((-1*upper_threshold)-(0.6*upper_threshold),upper_threshold +(0.6*upper_threshold))\n",
    "\n",
    "    ## filter for spindles\n",
    "\n",
    "    offset = sleep_period_ephys_start_time\n",
    "    spindle_events = find_events(smoothed_envelope,lower_threshold, upper_threshold,fs,min_event_duration,offset)\n",
    "\n",
    "    ax2.plot(time, spindle_bandpassed, color='b')\n",
    "    ax2.set_xlabel('Time (seconds)')\n",
    "    ax2.set_ylabel('Amplitude')\n",
    "\n",
    "    for event in spindle_events:\n",
    "        ax2.axvspan(event['start_time'],event['end_time'], alpha=0.5, color='red')\n",
    "        \n",
    "    ax2.set_ylim((-1*upper_threshold)-(1.5*upper_threshold),upper_threshold +(1.5*upper_threshold))\n",
    "\n",
    "\n",
    "    # Add shaded regions without repeating labels\n",
    "    for ind, (start, end) in enumerate(rem_intervals):\n",
    "        label = 'rem' if ind == 0 else None\n",
    "        ax2.axvspan(start, end, color='yellow', alpha=0.2, label=label)\n",
    "\n",
    "    for ind, (start, end) in enumerate(nrem_intervals):\n",
    "        label = 'nrem' if ind == 0 else None\n",
    "        ax2.axvspan(start, end, color='green', alpha=0.2, label=label)\n",
    "\n",
    "    ax2.legend(loc='center left', bbox_to_anchor=(1.1, 0.2))\n",
    "    \n",
    "    return spindle_events,smoothed_envelope\n",
    "\n",
    "def compute_smoothed_envelope(signal, sampling_rate, window_length_ms=200):\n",
    "    # Convert window length from milliseconds to samples\n",
    "    window_length_samples = int(window_length_ms * sampling_rate / 1000)\n",
    "    \n",
    "    # Compute the analytic signal using Hilbert transform\n",
    "    analytic_signal = hilbert(signal)\n",
    "    \n",
    "    # Compute the magnitude of the analytic signal\n",
    "    magnitude = np.abs(analytic_signal)\n",
    "    \n",
    "    # Create a Gaussian window with specified length\n",
    "    gaussian_window = np.exp(-0.5 * (np.arange(-window_length_samples, window_length_samples+1) / (0.001 * sampling_rate))**2)\n",
    "    \n",
    "    # Normalize the Gaussian window\n",
    "    gaussian_window /= np.sum(gaussian_window)\n",
    "    \n",
    "    # Convolve the magnitude with the Gaussian window\n",
    "    smoothed_envelope = convolve(magnitude, gaussian_window, mode='same')\n",
    "    \n",
    "    return smoothed_envelope\n",
    "\n",
    "def find_events(signal_data, lower_threshold, upper_threshold, sampling_rate,time_filter,offset):\n",
    "    events = []\n",
    "    event_start = None\n",
    "    peak_magnitude = None\n",
    "    sum_magnitude = 0\n",
    "    count_samples = 0\n",
    "\n",
    "    for i, sample in enumerate(signal_data):\n",
    "        # Check if the signal crosses the lower threshold\n",
    "        if sample > lower_threshold:\n",
    "            if event_start is None:\n",
    "                event_start = i\n",
    "\n",
    "            # Check if the signal crosses the upper threshold within the event window\n",
    "            if sample > upper_threshold:\n",
    "                peak_magnitude = max(signal_data[event_start:i + 1])\n",
    "                sum_magnitude += sample\n",
    "                count_samples += 1\n",
    "\n",
    "        else:\n",
    "            # If the event window duration is at least the time filter (s * sampling_rate)\n",
    "            if event_start is not None and (i - event_start) >= (time_filter * sampling_rate):\n",
    "                if peak_magnitude is not None and peak_magnitude > upper_threshold:\n",
    "                    average_magnitude = sum_magnitude / count_samples\n",
    "                    events.append({\n",
    "                        \"start_time\": (event_start / sampling_rate)+offset,\n",
    "                        \"end_time\": (i / sampling_rate)+offset,\n",
    "                        \"peak_magnitude\": peak_magnitude,\n",
    "                        'magnitude': sum_magnitude,\n",
    "                        \"average_magnitude\": average_magnitude,\n",
    "                    })\n",
    "\n",
    "            # Reset event variables\n",
    "            event_start = None\n",
    "            peak_magnitude = None\n",
    "            sum_magnitude = 0\n",
    "            count_samples = 0\n",
    "\n",
    "    return events\n",
    "\n",
    "def load_sync_file(mouse_org_data_path,mouse):\n",
    "    current_animals_behav_sync_path = os.path.join(mouse_org_data_path, r\"behav_sync\\\\\")\n",
    "    for stage in os.listdir(current_animals_behav_sync_path):\n",
    "        if 'post' in stage:\n",
    "            sync_path = current_animals_behav_sync_path + stage + '\\\\'\n",
    "    if mouse == '148_2_2':\n",
    "        sync_df = pd.read_csv(sync_path + r'Postsleep_Ephys_Camera_sync.csv', encoding='cp1252')\n",
    "    else:\n",
    "        sync_df = pd.read_csv(sync_path + r'Postsleep_Ephys_Camera_sync.csv')\n",
    "        \n",
    "    return sync_df \n",
    "\n",
    "def define_spindle_linkage(spindle_start_events,spindle_end_events,replay_times):\n",
    "    spindle_linkage_distance = []\n",
    "    for event in replay_times:\n",
    "        current_shortest_dist = 999999\n",
    "        for i,spin_start in enumerate(spindle_start_events):\n",
    "            s_distance = abs(spin_start - event)\n",
    "            e_distance = abs(spindle_end_events[i] - event)\n",
    "            if s_distance < abs(current_shortest_dist):\n",
    "                current_shortest_dist = s_distance\n",
    "            elif e_distance < abs(current_shortest_dist):\n",
    "                current_shortest_dist = e_distance\n",
    "        spindle_linkage_distance += [current_shortest_dist]\n",
    "    return spindle_linkage_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features \n",
    "\n",
    "# motif rate per min \n",
    "# motif relative event rate \n",
    "# motif relative proportion \n",
    "# motif spikes per event\n",
    "# motif units per event\n",
    "# motif event lengths\n",
    "# motif proportion coactive\n",
    "# motif proportion appearing in ordered coative pairs\n",
    "# motif overall neuron spiking consistency - are the neurons that we saw in replay on average consistently in or not in awake replay?\n",
    "# motif average warp factor\n",
    "# motif forward vs reverse replay ratio \n",
    "# motif proportion spindle linked / distance to closest spindle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EJT149_1_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT149_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT149_implant1\\recording1_16-11-2021\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT149_implant1\\recording1_16-11-2021\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT149_1_1\n",
      "0\n",
      "No replay data found for EJT149_1_1\n",
      "EJT178_1_6\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording6_29-03-2022\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording6_29-03-2022\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT178_1_6\n",
      "1\n",
      "No replay data found for EJT178_1_6\n",
      "EJT178_1_7\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording7_30-03-2022\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording7_30-03-2022\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT178_1_7\n",
      "2\n",
      "No replay data found for EJT178_1_7\n",
      "EJT178_1_8\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording8_31-03-2022\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant1\\recording8_31-03-2022\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT178_1_8\n",
      "3\n",
      "No replay data found for EJT178_1_8\n",
      "EJT178_2_2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording2_05-04-2022\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording2_05-04-2022\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT178_2_2\n",
      "4\n",
      "No replay data found for EJT178_2_2\n",
      "EJT178_2_3\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording3_06-04-2022\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording3_06-04-2022\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT178_2_3\n",
      "5\n",
      "No replay data found for EJT178_2_3\n",
      "EJT269_1_4\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording4_18-05-2023\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording4_18-05-2023\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT269_1_4\n",
      "6\n",
      "No replay data found for EJT269_1_4\n",
      "EJT270_1_6\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording6_19-05-2023\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT270_implant1\\recording6_19-05-2023\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT270_1_6\n",
      "7\n",
      "No replay data found for EJT270_1_6\n",
      "EJT178_2_1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording1_04-04-2022\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT178_implant2\\recording1_04-04-2022\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT178_2_1\n",
      "8\n",
      "No replay data found for EJT178_2_1\n",
      "EJT269_1_2\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording2_15-05-2023\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT269_implant1\\recording2_15-05-2023\n",
      "Loading sleep data from: None\n",
      "----------\n",
      "EJT269_1_2\n",
      "9\n",
      "No replay data found for EJT269_1_2\n",
      "ap5R_1_3\n",
      "None\n",
      "ap5R_1_3\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording3_19-11-2024\n",
      "full_org_dat_path: Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording3_19-11-2024\n",
      "Loading sleep data from: Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\learning\\postsleep\\\\ap5R_1_3_run_2405025_1905\n",
      "----------\n",
      "ap5R_1_3\n",
      "10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'seq_order'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m row\u001b[38;5;241m.\u001b[39mmir \u001b[38;5;129;01min\u001b[39;00m mouse:\n\u001b[0;32m     52\u001b[0m             mir_row \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m---> 53\u001b[0m     seq_order \u001b[38;5;241m=\u001b[39m literal_eval(\u001b[43mmir_row\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_order\u001b[49m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo replay data found for \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mouse)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seq_order'"
     ]
    }
   ],
   "source": [
    "animals = ['EJT149_1_1','EJT178_1_6','EJT178_1_7','EJT178_1_8','EJT178_2_2','EJT178_2_3','EJT269_1_4','EJT270_1_6','EJT178_2_1','EJT269_1_2','ap5R_1_3','seq006_1_1','seq006_1_4','seq006_1_5','seq006_1_6','seq007_1_1','seq007_1_2']\n",
    "# to be added once ppseq sleep has run! ['seq006_1_8','seq006_1_9','seq006_1_10','ap5R_1_2','seq006_1_3']\n",
    "\n",
    "expert_awake_ppseq_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\postsleep\\\\\"\n",
    "learning_awake_ppseq_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\learning\\postsleep\\\\\"\n",
    "other_sleep_ppseq_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\prioritisation_data\\postsleep\\\\\"\n",
    "\n",
    "# loop over all animals \n",
    "for index,mouse in enumerate(animals):\n",
    "    \n",
    "    # find organised path\n",
    "    try: \n",
    "        full_org_dat_path = find_organised_path(mouse,r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\")\n",
    "    except:\n",
    "        full_org_dat_path  = find_organised_path(mouse,r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\")\n",
    "    print(f'full_org_dat_path: {full_org_dat_path}')\n",
    "    \n",
    "    # find replay data path  \n",
    "    sleep_file = None\n",
    "    for file in os.listdir(expert_awake_ppseq_path):\n",
    "        if mouse in file:\n",
    "            sleep_file = os.path.join(learning_awake_ppseq_path,file)\n",
    "    if sleep_file is None:\n",
    "        for file in os.listdir(learning_awake_ppseq_path):\n",
    "            if mouse in file:\n",
    "                sleep_file = os.path.join(learning_awake_ppseq_path,file)\n",
    "    if sleep_file is None:\n",
    "        for file in os.listdir(learning_awake_ppseq_path):\n",
    "            if mouse in file:\n",
    "                sleep_file = os.path.join(learning_awake_ppseq_path,file)\n",
    "    print(f\"Loading sleep data from: {sleep_file}\")\n",
    "    \n",
    "    replay_data_found = False\n",
    "    print('----------')\n",
    "    print(mouse)\n",
    "    print(index)\n",
    "    current_mouse_sleep_path = sleep_file\n",
    "    if '_final_analysis_output' in os.listdir(current_mouse_sleep_path):\n",
    "        # load in sleep start time \n",
    "        sleep_time_point_df = pd.read_csv(remove_last_folder(current_mouse_sleep_path) + '\\sleep_time_points.csv')\n",
    "        current_sleep_start = sleep_time_point_df['approx_sleep_start'][np.where(sleep_time_point_df.mir == mouse)[0][0]]\n",
    "        params_file = current_mouse_sleep_path + r'\\trainingData\\\\' + 'params_' + mouse + '.json'\n",
    "        with open(params_file, 'r') as file:\n",
    "            params = json.load(file)\n",
    "        time_spans = params['time_span']\n",
    "        \n",
    "        # load sequence order\n",
    "        sequence_order = pd.read_csv(r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\\\\" + r'\\sequence_order.csv')\n",
    "        \n",
    "        mir_row = None\n",
    "        for ind, row in sequence_order.iterrows():\n",
    "            if row.mir in mouse:\n",
    "                mir_row = row\n",
    "        seq_order = literal_eval(mir_row.seq_order)\n",
    "        \n",
    "    else:\n",
    "        print('No replay data found for ' + mouse)\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # initialize lists to hold data across chunks \n",
    "    #1\n",
    "    chunk_rpm = []\n",
    "    #2\n",
    "    chunk_event_lens = []\n",
    "    #3\n",
    "    regression_df = pd.DataFrame({'seq_type':[],'regression_line':[],'filt_rel_spike_times':[],'slope':[],'reactivation_ID':[],'warp_factor':[],'mouse':[]})\n",
    "    #4\n",
    "    total_seqs_chunk = []\n",
    "    # 6+7\n",
    "    num_spikes_chunk = []\n",
    "    num_neurons_chunk = []\n",
    "    #8 \n",
    "    coactive_total_per_chunk = []\n",
    "    overall_total_per_chunk  = []\n",
    "    #9\n",
    "    seq_ordered_total_across_chunks = []\n",
    "    seq_misordered_total_across_chunks = []\n",
    "    #10\n",
    "    seq_by_seq_awake_neuron_consistency_per_chunk = []\n",
    "    #11\n",
    "    linkage_distances_per_chunk = []\n",
    "    \n",
    "    current_mouse_replay_path = current_mouse_sleep_path + '\\\\_final_analysis_output'\n",
    "    ## loop across all chunk files ################################\n",
    "    for chunk_number,file in enumerate(os.listdir(current_mouse_replay_path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            current_data_path = current_mouse_replay_path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(current_data_path + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(current_data_path + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            ###### FILTERING AND MASKING ##################################################################''\n",
    "            ## filter this data for sequential ordering\n",
    "            sequential_condition = data.ordering_classification == 'sequential'\n",
    "            # filter is set up so that any true will carry forward \n",
    "            filtered_chunk_data = data[sequential_condition].reset_index()\n",
    "            \n",
    "            ## REM / NREM times only\n",
    "            # load in sleep state scoring\n",
    "            nrem_start_ends,rem_start_ends,mouse_org_data_path,old_data = load_in_sleep_state_scoring(mouse)\n",
    "            # get relevant rem/nrem times for chunk\n",
    "            chunk_nrem_times = get_chunk_state_times(nrem_start_ends,chunk_time)\n",
    "            chunk_rem_times = get_chunk_state_times(rem_start_ends,chunk_time) \n",
    "            # get spike times relative to chunk:\n",
    "            chunk_number = int(file.split('_')[0][-1])\n",
    "            chunk_start_offset = ([0]+list(np.cumsum(np.diff(time_spans))))[chunk_number-1]\n",
    "            # make relative to start of chunk\n",
    "            fs_event_times = filtered_chunk_data['first_spike_time'].values - chunk_start_offset\n",
    "            # find inds of spike times that are in nrem and rem periods:\n",
    "            idx  = []\n",
    "            for start,end in chunk_nrem_times + chunk_rem_times:\n",
    "                idx += list(np.where((fs_event_times >= start) & (fs_event_times <= end))[0])\n",
    "            # filter the data frame, only keeping the rows in idx\n",
    "            filtered_chunk_data = filtered_chunk_data.iloc[idx]\n",
    "            filtered_chunk_data = filtered_chunk_data.reset_index(drop=True)\n",
    "            # get rid of the stupid unnamed columns\n",
    "            filtered_chunk_data = filtered_chunk_data.loc[:, ~filtered_chunk_data.columns.str.startswith('Unnamed')]\n",
    "            \n",
    "            #### keep only the seq types that are in the sequence order\n",
    "            keep_inds = []\n",
    "            for i,seq_type in enumerate(filtered_chunk_data['cluster_seq_type'].values):\n",
    "                if int(seq_type)+1 in seq_order:\n",
    "                    keep_inds += [i]\n",
    "            filtered_chunk_data = filtered_chunk_data.loc[keep_inds].reset_index(drop=True)\n",
    "                        \n",
    "            # 1. REACTIVATION PER MINUTE: #######################################################################\n",
    "            seq_rpm = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_chunk_data= seq_chunk_data.reset_index(drop=True)\n",
    "                seq_rpm += [reactivation_per_minute(seq_chunk_data, chunk_time)]\n",
    "            chunk_rpm += [seq_rpm]\n",
    "              \n",
    "            # 2. REACTIVATION EVENT LENGTH #######################################################################\n",
    "            seq_lens = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_chunk_data= seq_chunk_data.reset_index(drop=True)\n",
    "                seq_lens += [list(seq_chunk_data.event_length.values)]\n",
    "            chunk_event_lens += [seq_lens]\n",
    "            \n",
    "            #3.4. WARP #######################################################################\n",
    "            current_data_path_temporal_structre = current_data_path + 'temporal_structure_analysis\\\\'\n",
    "            # load in regression dfs\n",
    "            df_load = pd.read_csv(current_data_path_temporal_structre+'regression_df.csv',index_col=0)\n",
    "            df_load['mouse'] = [mouse]*len(df_load)\n",
    "            #concat:\n",
    "            regression_df = pd.concat((regression_df,df_load),axis =0)\n",
    "\n",
    "            #5. RELATIVE OVERALL PROPORTION ########################################################################\n",
    "            total_seqs = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_chunk_data= seq_chunk_data.reset_index(drop=True)\n",
    "                total_seqs += [len(seq_chunk_data)]\n",
    "            total_seqs_chunk += [total_seqs]\n",
    "            \n",
    "            #6 + 7 .  SPIKES + UNITS PER EVENT ########################################################################\n",
    "            num_spikes_perseq = []\n",
    "            num_neurons_perseq = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_chunk_data= seq_chunk_data.reset_index(drop=True)\n",
    "                num_spikes = []\n",
    "                num_neurons = []\n",
    "                for i in range(len(seq_chunk_data)):\n",
    "                    num_spikes +=[len(literal_eval(seq_chunk_data['cluster_spike_times'].values[i]))]\n",
    "                    num_neurons += [len(np.unique(literal_eval(seq_chunk_data['cluster_neurons'].values[i])))]\n",
    "                num_spikes_perseq += [[num_spikes]]\n",
    "                num_neurons_perseq += [[num_neurons]]\n",
    "            num_spikes_chunk += [num_spikes_perseq]\n",
    "            num_neurons_chunk += [num_neurons_perseq]\n",
    "            \n",
    "            # 8. COACTIVE EVENTS ########################################################################\n",
    "            # filter for all task related seqs\n",
    "            task_seq_only_data = filtered_chunk_data.loc[filtered_chunk_data.cluster_seq_type.isin(np.array(seq_order) + 1)]\n",
    "            # refine the clusters\n",
    "            event_proximity_filter = 0.3 # seconds\n",
    "            chunk_data_clusters  = refind_cluster_events(task_seq_only_data,event_proximity_filter)\n",
    "            chunk_data_clusters= chunk_data_clusters.reset_index(drop=True)\n",
    "            \n",
    "            # for each sequence get the number of events that were paired with another, and the total number of events - for this chunk \n",
    "            coactive_total_per_seq = []\n",
    "            overall_total_per_seq = []\n",
    "            for seq_id in seq_order:\n",
    "                coacitve_tot, overall_tot = find_coactive_pair_rate(chunk_data_clusters,seq_id)\n",
    "                coactive_total_per_seq += [[coacitve_tot]]\n",
    "                overall_total_per_seq += [[overall_tot]]\n",
    "            coactive_total_per_chunk += [coactive_total_per_seq]\n",
    "            overall_total_per_chunk += [overall_total_per_seq]\n",
    "            \n",
    "            # 9 . ORDERING ########################################################################\n",
    "            \n",
    "            real_order = np.array(seq_order)+1\n",
    "            num_dominant_seqs = int(mir_row.dominant_task_seqs)  \n",
    "            #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "            if not len(real_order) == num_dominant_seqs:\n",
    "                dominant = list(real_order[0:num_dominant_seqs])\n",
    "                other_ = list(real_order[num_dominant_seqs::])\n",
    "            else:\n",
    "                dominant = list(real_order)\n",
    "                other_ = []\n",
    "\n",
    "            # if there are coactive clusters \n",
    "            if len(chunk_data_clusters.coactive_cluster_group.unique()) < len(chunk_data_clusters):\n",
    "                multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(task_seq_only_data)\n",
    "            else:\n",
    "                multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "            seq_ordered_total_chunk = [] \n",
    "            seq_misordered_total_chunk = []\n",
    "            for seq_id in real_order: \n",
    "                # find all the coactive pairs for each sequence and then see if they are ordered or not \n",
    "                seq_cluster_pairs = []\n",
    "                for cluster in meaned_order:\n",
    "                    for index in range(1,len(cluster)):\n",
    "                        if cluster[index] == seq_id or cluster[index-1] == seq_id:\n",
    "                            seq_cluster_pairs += [[cluster[index-1],cluster[index]]]\n",
    "                ordered = 0\n",
    "                misordered = 0\n",
    "                for pair in seq_cluster_pairs:\n",
    "                    outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                    if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                        ordered += 1\n",
    "                    elif outcome == 'misordered':\n",
    "                        misordered += 1\n",
    "                seq_ordered_total_chunk += [ordered]\n",
    "                seq_misordered_total_chunk += [misordered]\n",
    "                \n",
    "            seq_ordered_total_across_chunks += [seq_ordered_total_chunk]\n",
    "            seq_misordered_total_across_chunks += [seq_misordered_total_chunk]\n",
    "            \n",
    "            ## 10. Awake neuron spiking consistency\n",
    "            ### load in awake ppseq data for mouse\n",
    "            with open(all_mice_dict[mouse]['awake_path'] + r'\\analysis_output\\\\' + 'spikes_seq_type_adjusted.pickle', 'rb') as handle:\n",
    "                unmasked_spikes_df = pickle.load(handle)\n",
    "                \n",
    "            with open(all_mice_dict[mouse]['awake_path'] + r'\\analysis_output\\reordered_recolored\\\\' + 'neuron_order', 'rb') as handle:\n",
    "                awake_neuron_order = pickle.load(handle)\n",
    "\n",
    "            colors = pd.read_pickle(all_mice_dict[mouse]['awake_path'] + r\"\\analysis_output\\reordered_recolored\\\\\" + 'colors')\n",
    "\n",
    "                \n",
    "            #load json\n",
    "            import json\n",
    "            params = None\n",
    "            for file_ in os.listdir(all_mice_dict[mouse]['awake_path'] + r'\\trainingData\\\\'):\n",
    "                if 'json' in file_:\n",
    "                    param_path = os.path.join(all_mice_dict[mouse]['awake_path'] + r'\\trainingData\\\\', file_)\n",
    "                    with open(param_path,'r') as f:\n",
    "                        params = json.load(f)\n",
    "            awake_time_span = params['time_span'][0]\n",
    "\n",
    "            # cluster the awake spikes into individual sequence events\n",
    "            seq_event_dfs = process_awake_data_return_seq_dfs(unmasked_spikes_df,awake_time_span,awake_neuron_order,colors,[100,120]) \n",
    "            ## spit the sequence events into individual sequence types\n",
    "            seq_events_dict = split_sequence_events(seq_event_dfs)\n",
    "\n",
    "            seq_by_seq_awake_neuron_consistency = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_chunk_data = seq_chunk_data.reset_index(drop=True)\n",
    "                proportion_appeared,neurons_appeared= calcuate_neuron_consistency(seq_id+1, unmasked_spikes_df, seq_events_dict)\n",
    "                event_by_event_proportion_appeared_in_awake = []\n",
    "                for event_clust_neurons in seq_chunk_data['cluster_neurons']:\n",
    "                    event_clust_neurons = literal_eval(event_clust_neurons)\n",
    "                    event_prop_appeared = []\n",
    "                    for neuron in event_clust_neurons:\n",
    "                        if neuron in neurons_appeared:\n",
    "                            indx = np.where(np.array(neurons_appeared) == neuron)[0][0]\n",
    "                            event_prop_appeared += [proportion_appeared[indx]]\n",
    "                    # get the mean proportion for the neurons that appeared in the awake replay\n",
    "                    event_by_event_proportion_appeared_in_awake += [np.mean(event_prop_appeared)]\n",
    "                seq_by_seq_awake_neuron_consistency += [event_by_event_proportion_appeared_in_awake]\n",
    "            seq_by_seq_awake_neuron_consistency_per_chunk += [seq_by_seq_awake_neuron_consistency]\n",
    "            \n",
    "            ## 11. SPINDLE LINKAGE ########################################################################\n",
    "            # SPINDLE IDENTIFICATION \n",
    "            # are spindle events linked to replay events? proportion linked\n",
    "            # how close is closest spindle on av. \n",
    "\n",
    "            # load behavioural data\n",
    "            post_sleep_sync_df = load_sync_file(full_org_dat_path,mouse)\n",
    "\n",
    "            # get the ephys time from the sleep because this is what the lfp was trimmed down to.                        \n",
    "            sleep_period_ephys_start_time = post_sleep_sync_df.Camera_time_Ephys_Aligned[0]\n",
    "\n",
    "            spind_path = r'Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\spindle_data\\\\'+ mouse\n",
    "\n",
    "            # load in spindle bandpassed data \n",
    "            spindle_bandpassed = np.load(spind_path + '\\zscored_spindle_bandpassed.npy')\n",
    "\n",
    "            # find spindle events\n",
    "            fs = 2500\n",
    "            min_event_duration = 0.3 #s\n",
    "            spindle_events,smoothed_envelope = filter_for_spindles_and_plot(spindle_bandpassed,nrem_start_ends,rem_start_ends,fs,min_event_duration,sleep_period_ephys_start_time)\n",
    "            # plt.savefig(output_path + 'spindle_identification.png')\n",
    "\n",
    "            # get spindle times\n",
    "            spindle_start_times = [event['start_time'] for event in spindle_events]\n",
    "            spindle_end_times = [event['end_time'] for event in spindle_events]\n",
    "\n",
    "            seq_by_seq_linkage_distance = []\n",
    "            for seq_id in seq_order:\n",
    "                seq_chunk_data = filtered_chunk_data.loc[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]\n",
    "                seq_chunk_data = seq_chunk_data.reset_index(drop=True)\n",
    "                replay_start_times = [min(literal_eval(times)) for times in seq_chunk_data.cluster_spike_times]\n",
    "                # replay start times are in chunk time, so convert to general ephys time\n",
    "                # get spike times relative to chunk:\n",
    "                chunk_number = int(file.split('_')[0][-1])\n",
    "                chunk_start_offset = ([0]+list(np.cumsum(np.diff(time_spans))))[chunk_number-1]\n",
    "                # # make relative to start of chunk\n",
    "                replay_start_times_realtive_to_chunk_ephys_time = np.array(replay_start_times) - chunk_start_offset\n",
    "                # convert to ephys time \n",
    "                replay_start_ephys_times = replay_start_times_realtive_to_chunk_ephys_time + chunk_time[0]\n",
    "                \n",
    "                seq_by_seq_linkage_distance += [define_spindle_linkage(spindle_start_times,spindle_end_times,replay_start_ephys_times)]\n",
    "                \n",
    "            linkage_distances_per_chunk += [seq_by_seq_linkage_distance]\n",
    "                        \n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "            \n",
    "    ## outside chunk loop\n",
    "    \n",
    "    #1. REACTIVATION PER MINUTE \n",
    "    seq_event_per_min = []\n",
    "    for i in range(len(seq_order)):\n",
    "        seq_event_per_min += [np.mean([item[i] for item in chunk_rpm])]\n",
    "    #!#\n",
    "    relative_seq_event_rate_per_min = seq_event_per_min/sum(seq_event_per_min)\n",
    "    \n",
    "    #2. REACTIVATION EVENT LENGTH\n",
    "    #!#\n",
    "    seq_event_lens = []\n",
    "    for i in range(len(seq_order)):\n",
    "        seq_event_lens += [np.nanmean([item for sublist in [item[i] for item in chunk_event_lens] for item in sublist])]\n",
    "    \n",
    "        \n",
    "    #3 + 4. WARP +forward/reverse proportion\n",
    "    forward_proportion_compared_to_reverse = []\n",
    "    mean_warp_per_motif = []\n",
    "    regression_df.reset_index(inplace=True,drop=True)\n",
    "    for seq_id in seq_order:\n",
    "        seq_reg_data = regression_df.loc[np.where(regression_df.seq_type == seq_id+1)]\n",
    "        #!#\n",
    "        mean_warp_per_motif += [np.mean(seq_reg_data['warp_factor'])]\n",
    "        forward_proportion_compared_to_reverse += [len(seq_reg_data[seq_reg_data. warp_factor > 0])/len(seq_reg_data[seq_reg_data. warp_factor < 0])]\n",
    "                \n",
    "    # 5. RELATIVE OVERALL PROPORTION\n",
    "    # motif relative proportion \n",
    "    total_seqs = []\n",
    "    \n",
    "    for i in range(len(seq_order)):\n",
    "        total_seqs += [sum([item[i] for item in total_seqs_chunk])]\n",
    "    #!#\n",
    "    relative_motif_expression_total = np.array(total_seqs)/sum(total_seqs)\n",
    "    \n",
    "    ## 6 + 7. SPIKES + UNITS PER EVENT\n",
    "    #!# #!#\n",
    "    mean_spikes_per_seq = []\n",
    "    mean_neurons_per_seq = []\n",
    "    for i in range(len(seq_order)):\n",
    "        mean_spikes_per_seq += [np.nanmean([item for sublist in [item[i][0] for item in num_spikes_chunk] for item in sublist])]\n",
    "        mean_neurons_per_seq += [np.nanmean([item for sublist in [item[i][0] for item in num_neurons_chunk] for item in sublist])]\n",
    "    \n",
    "    # 8 . COACTIVE EVENTS\n",
    "    proportion_events_coactive_per_motif = []\n",
    "    for i in range(len(seq_order)):\n",
    "        total_coactivly_paired = sum([item for sublist in [item[i] for item in coactive_total_per_chunk] for item in sublist])\n",
    "        overall_total = sum([item for sublist in [item[i] for item in overall_total_per_chunk] for item in sublist])\n",
    "        \n",
    "        if overall_total == 0:\n",
    "            coactive_proportion = np.nan\n",
    "        else:\n",
    "            coactive_proportion = total_coactivly_paired/overall_total\n",
    "        #!#\n",
    "        proportion_events_coactive_per_motif += [coactive_proportion]\n",
    "        \n",
    "    # 9 . ORDERING \n",
    "    #!#\n",
    "    ordered_pairs_proportion_per_motif = []\n",
    "    for i in range(len(seq_order)):\n",
    "        ordered = (sum([item[i] for item in seq_ordered_total_across_chunks]))\n",
    "        misordered = (sum([item[i] for item in seq_misordered_total_across_chunks]))\n",
    "        tot = ordered + misordered\n",
    "        if tot > 0:\n",
    "            ordered_pairs_proportion_per_motif += [ordered / tot]\n",
    "        else:   \n",
    "            ordered_pairs_proportion_per_motif += [np.nan]   \n",
    "            \n",
    "    # 10 Awake neuron consistency  \n",
    "    #!#\n",
    "    replay_neuron_awake_consistency_per_motif = []\n",
    "    for i in range(len(seq_order)):\n",
    "        consistency_score = []\n",
    "        for proportion_appeared in [item for sublist in [item[i] for item in seq_by_seq_awake_neuron_consistency_per_chunk] for item in sublist]:\n",
    "            p = np.array(proportion_appeared)\n",
    "            Ci = 1 - 2 * p * (1 - p)\n",
    "            consistency_score += [Ci.mean()]\n",
    "        replay_neuron_awake_consistency_per_motif += [np.nanmean(consistency_score)] \n",
    "        \n",
    "    # 11+12. spindle linkage \n",
    "    #!#\n",
    "    average_linkage_distances_per_motif = []\n",
    "    #!#\n",
    "    proportion_spin_linked_per_motif = []\n",
    "    for i in range(len(seq_order)):\n",
    "        seq_linkage_dists = [item for sublist in [item[i] for item in linkage_distances_per_chunk] for item in sublist]\n",
    "        average_linkage_distances_per_motif += [np.nanmean(seq_linkage_dists)]\n",
    "        if len(seq_linkage_dists) == 0:\n",
    "            proportion_spin_linked_per_motif += [np.nan]\n",
    "        else:\n",
    "            proportion_spin_linked_per_motif += [len(np.array(seq_linkage_dists)[np.array(seq_linkage_dists) < 1.5])/len(seq_linkage_dists)]\n",
    "        \n",
    "        \n",
    "\n",
    "    #####################################\n",
    "    ## SAVE THE DATA AND FIGURES\n",
    "    ##################################### \n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "    \"animal_id\":[mouse]*len(relative_seq_event_rate_per_min),\n",
    "    \"seq_id\":seq_order,\n",
    "    \"relative_event_rate_per_min_per_seq\": relative_seq_event_rate_per_min,\n",
    "    \"mean_event_length_per_seq\": relative_seq_event_rate_per_min,\n",
    "    \"forward_proportion_compared_to_reverse\":forward_proportion_compared_to_reverse,\n",
    "    \"mean_warp_per_motif\":mean_warp_per_motif,\n",
    "    \"relative_motif_expression_total\":relative_motif_expression_total,\n",
    "    \"mean_spikes_per_seq\" :mean_spikes_per_seq,\n",
    "    \"mean_neurons_per_seq\" : mean_neurons_per_seq,\n",
    "    \"proportion_events_coactive_per_motif\":proportion_events_coactive_per_motif,\n",
    "    \"ordered_pairs_proportion_per_motif\" : ordered_pairs_proportion_per_motif,\n",
    "    \"replay_neuron_awake_consistency_per_motif\":replay_neuron_awake_consistency_per_motif,\n",
    "    \"average_linkage_distances_per_motif\":average_linkage_distances_per_motif,\n",
    "    \"proportion_spin_linked_per_motif\":proportion_spin_linked_per_motif\n",
    "    })\n",
    "\n",
    "    base_out_path = 'Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\replay_to_behaviour\\behaviour_data\\\\'\n",
    "    out_p = base_out_path + mouse + r'\\replay\\\\'\n",
    "    if os.path.isdir(out_p):\n",
    "        shutil.rmtree(out_p)\n",
    "    os.makedirs(out_p)\n",
    "    \n",
    "    output_df.to_csv(out_p + r'\\processed_replay_features.csv',index=False)\n",
    "    \n",
    "    print('_______________________________________________________________________________________________')\n",
    "    print('Saved replay features for ' + mouse)\n",
    "    print('_______________________________________________________________________________________________')    \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\ppseq_finalised_publication_data\\\\learning\\\\postsleep'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_last_folder(current_mouse_sleep_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ap5R_1_3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>mir</th>\n",
       "      <th>dominant_task_seqs</th>\n",
       "      <th>other_task_seqs</th>\n",
       "      <th>seq_order</th>\n",
       "      <th>color_order_names</th>\n",
       "      <th>color_order_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expert</td>\n",
       "      <td>148_2_2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 3, 5, 1]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expert</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2, 0, 3, 1]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>expert</td>\n",
       "      <td>136_1_4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 2, 5, 4, 3]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>expert</td>\n",
       "      <td>149_1_1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 0, 4, 1]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expert</td>\n",
       "      <td>149_1_2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[5, 0, 4, 3, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>expert</td>\n",
       "      <td>149_1_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[5, 0, 4, 3, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>expert</td>\n",
       "      <td>162_1_3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[5, 0, 3]</td>\n",
       "      <td>['red', 'green', 'yellow']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_1_4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[4, 1, 2, 3, 0]</td>\n",
       "      <td>['yellow', 'blue', 'red', 'green', 'purple']</td>\n",
       "      <td>['#E2DC92' '#1C79B5' '#BE575F' '#69BD9D' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_1_5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[5, 1, 4, 0]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_1_6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 1, 3, 5, 2]</td>\n",
       "      <td>['red', 'yellow', 'blue', 'purple', 'green']</td>\n",
       "      <td>['#BE575F' '#E2DC92' '#1C79B5' '#724F94' '#69B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_1_7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[4, 3, 0, 5, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_1_8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 5, 2, 4]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_1_9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3, 1, 0, 2, 4, 5]</td>\n",
       "      <td>['green', 'yellow', 'blue', 'gold', 'purple', ...</td>\n",
       "      <td>['#69BD9D' '#E2DC92' '#1C79B5' '#C6963E' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_2_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 1, 2]</td>\n",
       "      <td>['yellow', 'blue', 'purple']</td>\n",
       "      <td>['#E2DC92' '#1C79B5' '#724F94']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_2_2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 4, 3, 5]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_2_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 5, 0, 4, 2]</td>\n",
       "      <td>['yellow', 'blue', 'purple', 'gold', 'green']</td>\n",
       "      <td>['#E2DC92' '#1C79B5' '#724F94' '#C6963E' '#69B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>expert</td>\n",
       "      <td>178_2_4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 3, 4, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>expert</td>\n",
       "      <td>268_1_10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>expert</td>\n",
       "      <td>269_1_4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 1, 5, 0, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>expert</td>\n",
       "      <td>269_1_5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 5, 0, 4]</td>\n",
       "      <td>['red', 'yellow', 'green', 'blue']</td>\n",
       "      <td>['#BE575F' '#E2DC92' '#69BD9D' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>expert</td>\n",
       "      <td>269_1_6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[5, 2, 3, 0]</td>\n",
       "      <td>['red', 'yellow', 'green', 'purple']</td>\n",
       "      <td>['#BE575F' '#E2DC92' '#69BD9D' '#724F94']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>expert</td>\n",
       "      <td>269_1_7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 0, 5, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>expert</td>\n",
       "      <td>270_1_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 0, 3, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>expert</td>\n",
       "      <td>270_1_6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 0, 5, 1]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>expert</td>\n",
       "      <td>270_1_7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2, 0, 3]</td>\n",
       "      <td>['green', 'red', 'blue']</td>\n",
       "      <td>['#69BD9D' '#BE575F' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TRN</td>\n",
       "      <td>238_1_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TRN</td>\n",
       "      <td>238_1_4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 1, 3, 4, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'gold', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#C6963E' '#1C7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TRN</td>\n",
       "      <td>238_1_5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 3, 1, 4]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TRN</td>\n",
       "      <td>238_1_6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 3, 4, 1, 5]</td>\n",
       "      <td>['red', 'green', 'blue', 'yellow', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#1C79B5' '#E2DC92' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>256_1_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 4, 2]</td>\n",
       "      <td>['red', 'green', 'yellow']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>255_1_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 1, 0, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>255_1_2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "      <td>['red', 'green', 'yellow']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>255_1_4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 2, 0, 5]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>262_1_1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 4, 3]</td>\n",
       "      <td>['red', 'green', 'yellow']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>262_1_2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 2, 4, 1]</td>\n",
       "      <td>['green', 'yellow', 'purple', 'red']</td>\n",
       "      <td>['#69BD9D' '#E2DC92' '#724F94' '#BE575F']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>262_1_4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 0, 1, 4]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>262_1_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[3, 5, 4, 1]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>h_lesion</td>\n",
       "      <td>262_1_6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1, 0, 4, 2, 5]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>learning</td>\n",
       "      <td>268_1_2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[4, 1, 5, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>learning</td>\n",
       "      <td>269_1_1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 2, 4]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>learning</td>\n",
       "      <td>269_1_2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2, 1, 5, 3, 0]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>learning</td>\n",
       "      <td>269_1_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 3, 4, 2]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>learning</td>\n",
       "      <td>270_1_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[5, 1, 3, 4, 2]</td>\n",
       "      <td>['green', 'red', 'blue', 'purple', 'gold']</td>\n",
       "      <td>['#69BD9D' '#BE575F' '#1C79B5' '#724F94' '#C69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>learning</td>\n",
       "      <td>270_1_3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 0, 1, 4, 5]</td>\n",
       "      <td>['red', 'green', 'yellow', 'blue', 'purple']</td>\n",
       "      <td>['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       group        mir  dominant_task_seqs  other_task_seqs  \\\n",
       "0     expert    148_2_2                 4.0              0.0   \n",
       "1     expert    136_1_3                 4.0              0.0   \n",
       "2     expert    136_1_4                 5.0              0.0   \n",
       "3     expert    149_1_1                 4.0              0.0   \n",
       "4     expert    149_1_2                 5.0              1.0   \n",
       "5     expert    149_1_3                 NaN              NaN   \n",
       "6     expert    162_1_3                 3.0              1.0   \n",
       "7     expert    178_1_4                 4.0              1.0   \n",
       "8     expert    178_1_5                 3.0              1.0   \n",
       "9     expert    178_1_6                 4.0              1.0   \n",
       "10    expert    178_1_7                 4.0              1.0   \n",
       "11    expert    178_1_8                 4.0              0.0   \n",
       "12    expert    178_1_9                 3.0              3.0   \n",
       "13    expert    178_2_1                 3.0              0.0   \n",
       "14    expert    178_2_2                 3.0              1.0   \n",
       "15    expert    178_2_3                 4.0              1.0   \n",
       "16    expert    178_2_4                 4.0              0.0   \n",
       "17    expert   268_1_10                 3.0              NaN   \n",
       "18    expert    269_1_4                 4.0              1.0   \n",
       "19    expert    269_1_5                 3.0              1.0   \n",
       "20    expert    269_1_6                 3.0              1.0   \n",
       "21    expert    269_1_7                 4.0              0.0   \n",
       "22    expert    270_1_5                 4.0              0.0   \n",
       "23    expert    270_1_6                 4.0              0.0   \n",
       "24    expert    270_1_7                 3.0              0.0   \n",
       "25        TRN   238_1_2                 NaN              NaN   \n",
       "26        TRN   238_1_4                 4.0              1.0   \n",
       "27        TRN   238_1_5                 3.0              1.0   \n",
       "28        TRN   238_1_6                 2.0              3.0   \n",
       "29  h_lesion    256_1_1                 3.0              0.0   \n",
       "30  h_lesion    255_1_1                 3.0              1.0   \n",
       "31  h_lesion    255_1_2                 3.0              0.0   \n",
       "32  h_lesion    255_1_4                 3.0              1.0   \n",
       "33  h_lesion    262_1_1                 3.0              0.0   \n",
       "34  h_lesion    262_1_2                 3.0              1.0   \n",
       "35  h_lesion    262_1_4                 4.0              0.0   \n",
       "36  h_lesion    262_1_5                 4.0              0.0   \n",
       "37  h_lesion    262_1_6                 4.0              1.0   \n",
       "38   learning   268_1_2                 4.0              0.0   \n",
       "39   learning   269_1_1                 4.0              0.0   \n",
       "40   learning   269_1_2                 5.0              0.0   \n",
       "41   learning   269_1_3                 4.0              0.0   \n",
       "42   learning   270_1_1                 5.0              0.0   \n",
       "43   learning   270_1_3                 4.0              1.0   \n",
       "\n",
       "             seq_order                                  color_order_names  \\\n",
       "0         [0, 3, 5, 1]                 ['red', 'green', 'yellow', 'blue']   \n",
       "1         [2, 0, 3, 1]                 ['red', 'green', 'yellow', 'blue']   \n",
       "2      [0, 2, 5, 4, 3]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "3         [5, 0, 4, 1]                 ['red', 'green', 'yellow', 'blue']   \n",
       "4      [5, 0, 4, 3, 2]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "5      [5, 0, 4, 3, 2]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "6            [5, 0, 3]                         ['red', 'green', 'yellow']   \n",
       "7      [4, 1, 2, 3, 0]       ['yellow', 'blue', 'red', 'green', 'purple']   \n",
       "8         [5, 1, 4, 0]                 ['red', 'green', 'yellow', 'blue']   \n",
       "9      [0, 1, 3, 5, 2]       ['red', 'yellow', 'blue', 'purple', 'green']   \n",
       "10     [4, 3, 0, 5, 2]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "11        [3, 5, 2, 4]                 ['red', 'green', 'yellow', 'blue']   \n",
       "12  [3, 1, 0, 2, 4, 5]  ['green', 'yellow', 'blue', 'gold', 'purple', ...   \n",
       "13           [4, 1, 2]                       ['yellow', 'blue', 'purple']   \n",
       "14        [1, 4, 3, 5]                 ['red', 'green', 'yellow', 'blue']   \n",
       "15     [1, 5, 0, 4, 2]      ['yellow', 'blue', 'purple', 'gold', 'green']   \n",
       "16        [5, 3, 4, 2]                 ['red', 'green', 'yellow', 'blue']   \n",
       "17                   0                                                  0   \n",
       "18     [3, 1, 5, 0, 2]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "19        [1, 5, 0, 4]                 ['red', 'yellow', 'green', 'blue']   \n",
       "20        [5, 2, 3, 0]               ['red', 'yellow', 'green', 'purple']   \n",
       "21        [4, 0, 5, 2]                 ['red', 'green', 'yellow', 'blue']   \n",
       "22        [5, 0, 3, 2]                 ['red', 'green', 'yellow', 'blue']   \n",
       "23        [4, 0, 5, 1]                 ['red', 'green', 'yellow', 'blue']   \n",
       "24           [2, 0, 3]                           ['green', 'red', 'blue']   \n",
       "25                   0                                                  0   \n",
       "26     [0, 1, 3, 4, 2]         ['red', 'green', 'yellow', 'gold', 'blue']   \n",
       "27        [0, 3, 1, 4]                 ['red', 'green', 'yellow', 'blue']   \n",
       "28     [0, 3, 4, 1, 5]       ['red', 'green', 'blue', 'yellow', 'purple']   \n",
       "29           [0, 4, 2]                         ['red', 'green', 'yellow']   \n",
       "30        [3, 1, 0, 2]                 ['red', 'green', 'yellow', 'blue']   \n",
       "31           [2, 1, 3]                         ['red', 'green', 'yellow']   \n",
       "32        [3, 2, 0, 5]                 ['red', 'green', 'yellow', 'blue']   \n",
       "33           [0, 4, 3]                         ['red', 'green', 'yellow']   \n",
       "34        [3, 2, 4, 1]               ['green', 'yellow', 'purple', 'red']   \n",
       "35        [3, 0, 1, 4]                 ['red', 'green', 'yellow', 'blue']   \n",
       "36        [3, 5, 4, 1]                 ['red', 'green', 'yellow', 'blue']   \n",
       "37     [1, 0, 4, 2, 5]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "38        [4, 1, 5, 2]                 ['red', 'green', 'yellow', 'blue']   \n",
       "39        [1, 0, 2, 4]                 ['red', 'green', 'yellow', 'blue']   \n",
       "40     [2, 1, 5, 3, 0]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "41        [5, 3, 4, 2]                 ['red', 'green', 'yellow', 'blue']   \n",
       "42     [5, 1, 3, 4, 2]         ['green', 'red', 'blue', 'purple', 'gold']   \n",
       "43     [3, 0, 1, 4, 5]       ['red', 'green', 'yellow', 'blue', 'purple']   \n",
       "\n",
       "                                    color_order_codes  \n",
       "0           ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "1           ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "2   ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "3           ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "4   ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "5   ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "6                     ['#BE575F' '#69BD9D' '#E2DC92']  \n",
       "7   ['#E2DC92' '#1C79B5' '#BE575F' '#69BD9D' '#724...  \n",
       "8           ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "9   ['#BE575F' '#E2DC92' '#1C79B5' '#724F94' '#69B...  \n",
       "10  ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "11          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "12  ['#69BD9D' '#E2DC92' '#1C79B5' '#C6963E' '#724...  \n",
       "13                    ['#E2DC92' '#1C79B5' '#724F94']  \n",
       "14          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "15  ['#E2DC92' '#1C79B5' '#724F94' '#C6963E' '#69B...  \n",
       "16          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "17                                                  0  \n",
       "18  ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "19          ['#BE575F' '#E2DC92' '#69BD9D' '#1C79B5']  \n",
       "20          ['#BE575F' '#E2DC92' '#69BD9D' '#724F94']  \n",
       "21          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "22          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "23          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "24                    ['#69BD9D' '#BE575F' '#1C79B5']  \n",
       "25                                                  0  \n",
       "26  ['#BE575F' '#69BD9D' '#E2DC92' '#C6963E' '#1C7...  \n",
       "27          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "28  ['#BE575F' '#69BD9D' '#1C79B5' '#E2DC92' '#724...  \n",
       "29                    ['#BE575F' '#69BD9D' '#E2DC92']  \n",
       "30          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "31                    ['#BE575F' '#69BD9D' '#E2DC92']  \n",
       "32          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "33                    ['#BE575F' '#69BD9D' '#E2DC92']  \n",
       "34          ['#69BD9D' '#E2DC92' '#724F94' '#BE575F']  \n",
       "35          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "36          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "37  ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "38          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "39          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "40  ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  \n",
       "41          ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5']  \n",
       "42  ['#69BD9D' '#BE575F' '#1C79B5' '#724F94' '#C69...  \n",
       "43  ['#BE575F' '#69BD9D' '#E2DC92' '#1C79B5' '#724...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old data in path and make new folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mouse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmouse\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mouse' is not defined"
     ]
    }
   ],
   "source": [
    "mouse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict):\n",
    "#     # if session in one of the groups (and define which)   \n",
    "#     if mouse in list(expert_mice) + list(hlesion_mice) + list(learning_mice):\n",
    "#         if mouse in expert_mice:\n",
    "#             var_dict['expert'] += [1]\n",
    "#             var_dict['hlesion'] += [0]\n",
    "#             var_dict['learning'] += [0]               \n",
    "#         elif mouse in hlesion_mice:                \n",
    "#             var_dict['expert'] += [0]\n",
    "#             var_dict['hlesion'] += [1]\n",
    "#             var_dict['learning'] += [0]   \n",
    "#         elif mouse in learning_mice:                \n",
    "#             var_dict['expert'] += [0]\n",
    "#             var_dict['hlesion'] += [0]\n",
    "#             var_dict['learning'] += [1]   \n",
    "#     return var_dict\n",
    "\n",
    "# def get_time_span(dat_path,pp_file,mouse):\n",
    "#     with open(dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json', 'r') as file:\n",
    "#         params = json.load(file)\n",
    "#     time_spans = params['time_span']\n",
    "#     return time_spans\n",
    "\n",
    "# def find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start):\n",
    "#     current_mouse_path = []\n",
    "#     for run_index,pp_file in enumerate(os.listdir(sleep_ppseq_path)):\n",
    "#         if not 'sleep_time_points' in pp_file:\n",
    "#             # current mouse\n",
    "#             mouse = '_'.join(pp_file.split('_')[0:3])    \n",
    "\n",
    "#             if mouse in useable_mirs:\n",
    "                    \n",
    "#                     # asign to experimental group in var_dict\n",
    "#                     var_dict = assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict)\n",
    "\n",
    "#                     # load in sleep start time and time span\n",
    "#                     var_dict['current_sleep_start'] += sleep_start[mouse]\n",
    "#                     var_dict['time_spans'] += get_time_span(sleep_ppseq_path,pp_file,mouse)\n",
    "\n",
    "#                     # set path to processed files \n",
    "#                     current_mouse_path += [sleep_ppseq_path + pp_file + '\\\\analysis_output\\\\']\n",
    "#                     var_dict['mirs'] += [mouse]\n",
    "#     return current_mouse_path,var_dict\n",
    "\n",
    "\n",
    "# def make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only):\n",
    "#     ## filter this data\n",
    "#     if sequential_filter == True: \n",
    "#         sequential_condition = data.ordering_classification == 'sequential'\n",
    "#     else:\n",
    "#         sequential_condition = np.array([True]*len(data.ordering_classification))\n",
    "\n",
    "#     if sleep_filters_on == True:\n",
    "#         if nrem_filter == True: \n",
    "#             nrem_condition = data.nrem_events == 1\n",
    "#         else:\n",
    "#             nrem_condition = np.array([False]*len(data.nrem_events))\n",
    "\n",
    "#         if rem_filter == True: \n",
    "#             rem_condition = data.rem_events == 1\n",
    "#         else:\n",
    "#             rem_condition = np.array([False]*len(data.rem_events))\n",
    "\n",
    "#         if background_only == True:\n",
    "#             rem_condition = data.rem_events == 0\n",
    "#             nrem_condition = data.nrem_events == 0\n",
    "\n",
    "#     else:\n",
    "#         nrem_condition = np.array([True]*len(data))\n",
    "#         rem_condition = np.array([True]*len(data))\n",
    "        \n",
    "#     # filter is set up so that any true will carry forward \n",
    "#     filter_mask = sequential_condition * (nrem_condition + rem_condition)\n",
    "        \n",
    "#     return filter_mask\n",
    "\n",
    "# def determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path):\n",
    "#     # if sleep_filters_on is false, use all chunk time\n",
    "#     if sleep_filters_on == False:\n",
    "#         mins = np.diff(chunk_time)[0]\n",
    "#     else:\n",
    "#         # load in state times\n",
    "#         rem_state_times = np.load(path + 'rem_state_times.npy')\n",
    "#         nrem_state_times = np.load(path + 'nrem_state_times.npy')\n",
    "#         if len(rem_state_times) > 0:\n",
    "#             tot_rem = sum(np.diff(rem_state_times))[0]\n",
    "#         else:\n",
    "#             tot_rem = 0\n",
    "#         if len(nrem_state_times) > 0:\n",
    "#             tot_nrem = sum(np.diff(nrem_state_times))[0]\n",
    "#         else:\n",
    "#             tot_nrem = 0\n",
    "\n",
    "#         # if background then use all non rem and non nrem times\n",
    "#         if background_only:\n",
    "#             mins = np.diff(chunk_time)[0] - (tot_rem+tot_nrem)\n",
    "#         else:\n",
    "#             # if both, use both \n",
    "#             if nrem_filter == True and rem_filter == True:\n",
    "#                 mins = tot_rem+tot_nrem\n",
    "#             elif nrem_filter == True and rem_filter == False:\n",
    "#                 mins = tot_nrem\n",
    "#             elif nrem_filter == False and rem_filter == True:\n",
    "#                 mins = tot_rem\n",
    "#     # convert to mins            \n",
    "#     mins = mins/60\n",
    "    \n",
    "#     return mins\n",
    "\n",
    "# def cluster_events(start_times, end_times, threshold):\n",
    "#     clusters = []\n",
    "#     for i in range(len(start_times)):\n",
    "#         event_added = False\n",
    "#         for cluster in clusters:\n",
    "#             for index in cluster:\n",
    "#                 if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "#                     cluster.append(i)\n",
    "#                     event_added = True\n",
    "#                     break\n",
    "#             if event_added:\n",
    "#                 break\n",
    "#         if not event_added:\n",
    "#             clusters.append([i])\n",
    "#     return clusters\n",
    "\n",
    "# def relative_dict(input_dict):\n",
    "#     total_sum = sum(input_dict.values())\n",
    "#     relative_dict = {key: value / total_sum for key, value in input_dict.items()}\n",
    "#     return relative_dict\n",
    "\n",
    "# def refind_cluster_events(filtered_chunk_data,event_proximity_filter):\n",
    "    \n",
    "#     ### ignore the origonal clusterg rosp and remake them: \n",
    "#     start_times = filtered_chunk_data.first_spike_time.values\n",
    "#     end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "#     clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "#     cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "#     for index,cluster in enumerate(clustered_events):\n",
    "#         for item in cluster:\n",
    "#             cluster_group[item] = int(index)\n",
    "#     filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "    \n",
    "#     return filtered_chunk_data\n",
    "\n",
    "\n",
    "\n",
    "# def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "#     # if first one in dominant check for ordering:\n",
    "#     if pair[0] in dominant and pair[-1] in dominant:\n",
    "#         if pair_in_sequence(pair,dominant):\n",
    "#             return('ordered')\n",
    "#         elif pair_in_sequence(pair,dominant[::-1]):\n",
    "#             return('reverse')\n",
    "#         elif pair[-1] == pair[0]:\n",
    "#             return('repeat')\n",
    "#         elif pair[-1] in dominant:\n",
    "#             return('misordered') \n",
    "#     # if its not these  options then check if it could be in the extra task seqs\n",
    "#     elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "#         for item in other:\n",
    "#             if pair[0] in  (dominant + [item]):\n",
    "#                 if pair_in_sequence(pair,(dominant + [item])):\n",
    "#                     return('ordered')\n",
    "#                 elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "#                     return('reverse')\n",
    "#                 elif pair[-1] == pair[0]:\n",
    "#                     return('repeat')\n",
    "#                 elif pair[-1] in (dominant + [item]):\n",
    "#                     return('misordered')  \n",
    "#         # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "#         if pair[0] in other and pair[-1] in other:\n",
    "#             if not pair[-1] == pair[0]: \n",
    "#                 return('ordered')\n",
    "#     else:\n",
    "#         # if item 1 is in but item 2 isnt then task to other \n",
    "#         if pair[0] in  (dominant + other):\n",
    "#             if not pair[-1] in  (dominant + other):\n",
    "#                 return('task to other')\n",
    "#         # if item 2 is in but item 1 isnt then other to task \n",
    "#         elif not pair[0] in  (dominant + other):\n",
    "#             if pair[-1] in  (dominant + other):\n",
    "#                 return('other to task')\n",
    "#             else:\n",
    "#                 return('other')\n",
    "#     return print('ERROR!')\n",
    "\n",
    "# def pair_in_sequence(pair, sequence):\n",
    "#     for i in range(len(sequence) - 1):\n",
    "#         if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "#             return True\n",
    "#         # because its ciruclar:\n",
    "#         elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def calculate_ordering_amounts(meaned_order,dominant,other_):\n",
    "#     ordered = 0\n",
    "#     misordered = 0\n",
    "#     other = 0\n",
    "#     for cluster in meaned_order:\n",
    "#         for ind,item in enumerate(cluster):\n",
    "#             if not ind == len(cluster)-1:\n",
    "#                 pair = [item,cluster[ind+1]]\n",
    "#                 outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "#                 if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "#                     ordered += 1\n",
    "#                 elif outcome == 'misordered':\n",
    "#                     misordered += 1\n",
    "#                 else:\n",
    "#                     other +=1\n",
    "#     return ordered,misordered,other\n",
    "\n",
    "# def all_motifs_proportion_coactive(multi_cluster_df):\n",
    "#     motif_motif_coative_events = []\n",
    "#     for seq_type in range(1,7):\n",
    "#         motif_cluster_groups = multi_cluster_df[multi_cluster_df['cluster_seq_type'] == seq_type].new_cluster_group\n",
    "#         if not len(motif_cluster_groups) == 0:\n",
    "#             coative_motif_events = len(motif_cluster_groups)\n",
    "#         else:\n",
    "#             coative_motif_events = 0\n",
    "#         motif_motif_coative_events += [coative_motif_events]\n",
    "#     return motif_motif_coative_events\n",
    "\n",
    "# def motif_by_motif_ordering(meaned_order,real_order,dominant,other_):\n",
    "\n",
    "#     all_motifs_fs_task_related_ordered = []\n",
    "#     all_motifs_fs_task_related_misordered = []\n",
    "#     all_motifs_fs_task_related_other = []\n",
    "\n",
    "#     for motif_type in range(1,7):\n",
    "#         ordered = 0\n",
    "#         misordered = 0\n",
    "#         other = 0\n",
    "        \n",
    "#         if motif_type in real_order:\n",
    "#             for cluster in meaned_order:\n",
    "#                 for ind,item in enumerate(cluster):\n",
    "#                     if not ind == len(cluster)-1:\n",
    "#                         pair = [item,cluster[ind+1]]\n",
    "#                         if motif_type in pair:\n",
    "#                             outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "#                             if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "#                                 ordered += 1\n",
    "#                             elif outcome == 'misordered':\n",
    "#                                 misordered += 1\n",
    "#                             else:\n",
    "#                                 other +=1  \n",
    "                                \n",
    "#             all_motifs_fs_task_related_ordered += [ordered]\n",
    "#             all_motifs_fs_task_related_misordered += [misordered]\n",
    "#             all_motifs_fs_task_related_other += [other]\n",
    "#         else:\n",
    "#             all_motifs_fs_task_related_ordered += ['nan']\n",
    "#             all_motifs_fs_task_related_misordered += ['nan']\n",
    "#             all_motifs_fs_task_related_other += ['nan']\n",
    "\n",
    "#     return all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other\n",
    "\n",
    "# def coactive_rate(filtered_chunk_data):\n",
    "#     # how mnay coacitve in chunk: \n",
    "#     current_coactive_freqs_chunk = {}\n",
    "#     for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "#         num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "#         if num in current_coactive_freqs_chunk:\n",
    "#             current_coactive_freqs_chunk[num] += 1\n",
    "#         else:\n",
    "#             current_coactive_freqs_chunk[num] = 1\n",
    "            \n",
    "#     # total single events that are cocaitve with at least one other\n",
    "#     cocative_total = 0\n",
    "#     overall_total = 0\n",
    "#     for item in list(current_coactive_freqs_chunk):\n",
    "#         if item > 1:\n",
    "#             cocative_total += current_coactive_freqs_chunk[item]\n",
    "#         overall_total += current_coactive_freqs_chunk[item] * item\n",
    "\n",
    "#     # coactive_lengths (only coactive, ignore single events)\n",
    "#     coactive_len_per_chunk =[]\n",
    "#     for item in current_coactive_freqs_chunk:\n",
    "#         if item > 1:\n",
    "#             coactive_len_per_chunk += current_coactive_freqs_chunk[item] * [item]\n",
    "\n",
    "\n",
    "#     return cocative_total,coactive_len_per_chunk,overall_total\n",
    "\n",
    "# def empty_chunk_vars():\n",
    "#     ## set chunk vars \n",
    "#     chunk_vars = {\"chunk_reactivations\" : [],\n",
    "#                 \"chunk_mins\" : [],\n",
    "#                 \"chunk_motif_type_reactivations\" : [],\n",
    "#                 \"mean_spikes_per_event\" : [],\n",
    "#                 \"motif_by_motif_mean_spikes_per_event\" : [],\n",
    "#                 \"mean_units_per_event\" : [],\n",
    "#                 \"motif_by_motif_mean_units_per_event\" : [],\n",
    "#                 \"chunk_event_lengths\" : [],\n",
    "#                 \"motif_event_lenghts\" : [],\n",
    "#                 \"total_single_events_coacitvely_paired\" : [],\n",
    "#                 \"coactive_lenghts\" : [],\n",
    "#                 \"overall_total_coactive_or_single_cluster_events\" : [],\n",
    "#                 \"meaned_order_task_related_ordered\":[],\n",
    "#                 \"meaned_order_task_related_misordered\":[],\n",
    "#                 \"meaned_order_task_related_other\":[],\n",
    "#                 \"fs_order_task_related_ordered\":[],\n",
    "#                 \"fs_order_task_related_misordered\":[],\n",
    "#                 \"fs_order_task_related_other\":[],\n",
    "#                 \"normalised_task_related_total\":[],\n",
    "#                 \"normalised_non_task_related_total\":[],\n",
    "#                 \"all_motifs_total_coactive\":[],\n",
    "#                 \"meaned_ordering_all_motifs_task_related_ordered\":[],\n",
    "#                 \"meaned_ordering_all_motifs_task_related_misordered\":[],\n",
    "#                 \"meaned_ordering_all_motifs_task_related_other\":[],\n",
    "#                 \"fs_ordering_all_motifs_task_related_ordered\":[],\n",
    "#                 \"fs_ordering_all_motifs_task_related_misordered\":[],\n",
    "#                 \"fs_ordering_all_motifs_task_related_other\":[],\n",
    "                \n",
    "#                 \"normalised_task_related_total\":[],\n",
    "#                 \"normalised_non_task_related_total\":[],\n",
    "                                \n",
    "                \n",
    "\n",
    "#     }\n",
    "#     return chunk_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ordering of coactive?\n",
    "if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "    multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "else:\n",
    "    multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "# pull out sequence order for current mouse\n",
    "seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "real_order = np.array(seq_order)+1\n",
    "\n",
    "#deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "if not len(real_order) == num_dominant_seqs:\n",
    "    dominant = list(real_order[0:num_dominant_seqs])\n",
    "    other_ = list(real_order[num_dominant_seqs::])\n",
    "else:\n",
    "    dominant = list(real_order)\n",
    "    other_ = []\n",
    "    \n",
    "# orderng amounts for mean ordering - this calculated for each pair in the chunk \n",
    "ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)\n",
    "chunk_vars['meaned_order_task_related_ordered'] += [ordered]\n",
    "chunk_vars['meaned_order_task_related_misordered'] += [misordered]\n",
    "chunk_vars['meaned_order_task_related_other'] += [other]\n",
    "\n",
    "# orderng amounts for first spike ordering\n",
    "ordered,misordered,other = calculate_ordering_amounts(fs_order,dominant,other_)\n",
    "chunk_vars['fs_order_task_related_ordered'] += [ordered]\n",
    "chunk_vars['fs_order_task_related_misordered'] += [misordered]\n",
    "chunk_vars['fs_order_task_related_other'] += [other]\n",
    "### motif by motif:\n",
    "# does one motif appear more in coactive?\n",
    "if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "    all_motifs_total_coactive = all_motifs_proportion_coactive(multi_cluster_df)\n",
    "    chunk_vars['all_motifs_total_coactive'] += [all_motifs_total_coactive]\n",
    "else:\n",
    "    chunk_vars['all_motifs_total_coactive'] += [[0,0,0,0,0,0]]\n",
    "\n",
    "# does one motif appeaer more ordered? \n",
    "# # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "# meaned ordering \n",
    "all_motifs_meaned_ordering_task_related_ordered,all_motifs_meaned_ordering_task_related_misordered,all_motifs_meaned_ordering_task_related_other = motif_by_motif_ordering(meaned_order,real_order,dominant,other_)\n",
    "chunk_vars['meaned_ordering_all_motifs_task_related_ordered'] += [all_motifs_meaned_ordering_task_related_ordered]\n",
    "chunk_vars['meaned_ordering_all_motifs_task_related_misordered'] += [all_motifs_meaned_ordering_task_related_misordered]\n",
    "chunk_vars['meaned_ordering_all_motifs_task_related_other'] += [all_motifs_meaned_ordering_task_related_other]\n",
    "\n",
    "# first spike ordering \n",
    "all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other = motif_by_motif_ordering(fs_order,real_order,dominant,other_)\n",
    "chunk_vars['fs_ordering_all_motifs_task_related_ordered'] += [all_motifs_fs_task_related_ordered]\n",
    "chunk_vars['fs_ordering_all_motifs_task_related_misordered'] += [all_motifs_fs_task_related_misordered]\n",
    "chunk_vars['fs_ordering_all_motifs_task_related_other'] += [all_motifs_fs_task_related_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: 10    [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...\nName: cluster_spike_times, dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_chunk_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster_spike_times\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:109\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:83\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:74\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[1;32m---> 74\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\ast.py:71\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     70\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string: 10    [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...\nName: cluster_spike_times, dtype: object"
     ]
    }
   ],
   "source": [
    "len(literal_eval(seq_chunk_data['cluster_spike_times']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0714285714285716, 1.18125, 1.2828282828282829, 0.6363636363636364]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_proportion_compared_to_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_reg_data[seq_reg_data. warp_factor > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_reg_data[seq_reg_data. warp_factor <= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3888888888888889"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ##################################### av. spikes involved\n",
    "            chunk_vars['mean_spikes_per_event'] += [[len(item) for item in filtered_chunk_data.cluster_spike_times]]\n",
    "            # per motif\n",
    "            motif_by_motif_mean_spikes_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_spikes_per_event += [[len(item) for item in motif_data.cluster_spike_times]]\n",
    "            chunk_vars['motif_by_motif_mean_spikes_per_event'] += [motif_by_motif_mean_spikes_per_event]  \n",
    "                    \n",
    "            ########################################## average units involved \n",
    "            chunk_vars['mean_units_per_event'] += [[len(np.unique(ast.literal_eval(item))) for item in filtered_chunk_data.cluster_neurons]]\n",
    "            motif_by_motif_mean_units_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_units_per_event += [[len(np.unique(ast.literal_eval(item))) for item in motif_data.cluster_neurons]]\n",
    "            chunk_vars['motif_by_motif_mean_units_per_event'] += [motif_by_motif_mean_units_per_event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_seq_event_rate_per_min = seq_event_per_min/sum(seq_event_per_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(nan)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_per_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_42616\\2415183231.py:3: RuntimeWarning: Mean of empty slice\n",
      "  seq_event_lens += [np.nanmean([item for sublist in [item[i] for item in chunk_event_lens] for item in sublist])]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(nan),\n",
       " np.float64(0.12021244979920481),\n",
       " np.float64(nan),\n",
       " np.float64(0.027006666666651784)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for sublist in [item[i] for item in chunk_event_lens] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_event_lens[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(nan),\n",
       " np.float64(0.12021244979920481),\n",
       " np.float64(nan),\n",
       " np.float64(0.027006666666651784)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_event_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.0531999999999754),\n",
       " np.float64(0.0142000000000166),\n",
       " np.float64(0.0228999999999928),\n",
       " np.float64(0.0275000000000034),\n",
       " np.float64(0.0101999999999975),\n",
       " np.float64(0.0584999999999809),\n",
       " np.float64(0.0175999999999874),\n",
       " np.float64(0.0537000000000205),\n",
       " np.float64(0.0224000000000046),\n",
       " np.float64(0.0378000000000042),\n",
       " np.float64(0.0768999999999664),\n",
       " np.float64(0.0511999999999943),\n",
       " np.float64(0.0912999999999897),\n",
       " np.float64(0.1779999999999972),\n",
       " np.float64(0.0810999999999921),\n",
       " np.float64(0.0736999999999739),\n",
       " np.float64(0.0464000000000055),\n",
       " np.float64(0.0212000000000216),\n",
       " np.float64(0.0372999999999592),\n",
       " np.float64(0.031499999999994),\n",
       " np.float64(0.1111000000000217),\n",
       " np.float64(0.016900000000021),\n",
       " np.float64(0.0271999999999934),\n",
       " np.float64(0.0158000000000129),\n",
       " np.float64(0.0059999999999718),\n",
       " np.float64(0.1580000000000154),\n",
       " np.float64(0.0815000000000054),\n",
       " np.float64(0.0780000000000313),\n",
       " np.float64(0.0147999999999797),\n",
       " np.float64(0.0375999999999976),\n",
       " np.float64(0.0280999999999949),\n",
       " np.float64(0.0262999999999919),\n",
       " np.float64(0.0237000000000193),\n",
       " np.float64(0.0219999999999913),\n",
       " np.float64(0.0620000000000118),\n",
       " np.float64(0.0371000000000094),\n",
       " np.float64(0.0258000000000038),\n",
       " np.float64(0.032100000000014),\n",
       " np.float64(0.1311999999999784),\n",
       " np.float64(0.1332999999999629),\n",
       " np.float64(0.0561000000000149),\n",
       " np.float64(0.2062999999999988),\n",
       " np.float64(0.1087999999999738),\n",
       " np.float64(0.0156000000000062),\n",
       " np.float64(0.1582000000000221),\n",
       " np.float64(0.1193000000000097),\n",
       " np.float64(0.1193000000000097),\n",
       " np.float64(0.1456000000000017),\n",
       " np.float64(0.010499999999979),\n",
       " np.float64(0.0058999999999969),\n",
       " np.float64(0.1538000000000465),\n",
       " np.float64(0.1433999999999855),\n",
       " np.float64(0.0838999999999714),\n",
       " np.float64(0.0867999999999824),\n",
       " np.float64(0.0520999999999958),\n",
       " np.float64(0.0274),\n",
       " np.float64(0.0435999999999694),\n",
       " np.float64(0.0596000000000458),\n",
       " np.float64(0.1582999999999401),\n",
       " np.float64(0.1046000000000049),\n",
       " np.float64(0.0854000000000496),\n",
       " np.float64(0.0730999999999539),\n",
       " np.float64(0.1480000000000245),\n",
       " np.float64(0.0208999999999832),\n",
       " np.float64(0.1388000000000602),\n",
       " np.float64(0.1023999999999887),\n",
       " np.float64(0.1317000000000234),\n",
       " np.float64(0.0965999999999667),\n",
       " np.float64(0.0236999999999625),\n",
       " np.float64(0.0248000000000274),\n",
       " np.float64(0.0311000000000376),\n",
       " np.float64(0.0658000000000811),\n",
       " np.float64(0.0878999999999905),\n",
       " np.float64(0.0965999999999667),\n",
       " np.float64(0.0670000000000072),\n",
       " np.float64(0.041300000000092),\n",
       " np.float64(0.043700000000058),\n",
       " np.float64(0.129099999999994),\n",
       " np.float64(0.0942000000000007),\n",
       " np.float64(0.1669000000000551),\n",
       " np.float64(0.0855999999999994),\n",
       " np.float64(0.0386999999999488),\n",
       " np.float64(0.0833999999999832),\n",
       " np.float64(0.5354999999999563),\n",
       " np.float64(0.0986000000000331),\n",
       " np.float64(0.1539000000000214),\n",
       " np.float64(0.0053000000000338),\n",
       " np.float64(0.0587999999997919),\n",
       " np.float64(0.0993000000000847),\n",
       " np.float64(0.0866000000000895),\n",
       " np.float64(0.0418999999999414),\n",
       " np.float64(0.1591000000000804),\n",
       " np.float64(0.256800000000112),\n",
       " np.float64(0.125300000000152),\n",
       " np.float64(0.0327999999999519),\n",
       " np.float64(0.0498999999999796),\n",
       " np.float64(0.6065000000000964),\n",
       " np.float64(0.1545999999998457),\n",
       " np.float64(0.200800000000072),\n",
       " np.float64(0.0936000000001513),\n",
       " np.float64(0.0394000000001142),\n",
       " np.float64(0.0355000000001837),\n",
       " np.float64(0.1489999999998872),\n",
       " np.float64(0.0246999999999388),\n",
       " np.float64(0.0911000000000967),\n",
       " np.float64(0.1231000000000221),\n",
       " np.float64(0.0773999999998977),\n",
       " np.float64(0.4834000000000742),\n",
       " np.float64(0.0619999999998981),\n",
       " np.float64(0.0783999999998741),\n",
       " np.float64(0.1000999999998839),\n",
       " np.float64(0.1903000000002066),\n",
       " np.float64(0.0953999999999268),\n",
       " np.float64(0.0853999999999359),\n",
       " np.float64(0.0487000000000534),\n",
       " np.float64(0.0376000000001113),\n",
       " np.float64(0.0369000000000596),\n",
       " np.float64(0.2733000000000629),\n",
       " np.float64(0.4319000000000415),\n",
       " np.float64(0.1432000000002062),\n",
       " np.float64(0.1204000000000178),\n",
       " np.float64(0.1488999999999123),\n",
       " np.float64(0.2213999999999032),\n",
       " np.float64(0.1581999999998515),\n",
       " np.float64(0.3344999999999345),\n",
       " np.float64(0.5784000000001015),\n",
       " np.float64(0.1930999999999585),\n",
       " np.float64(0.1204000000000178),\n",
       " np.float64(0.1488999999999123),\n",
       " np.float64(0.2213999999999032),\n",
       " np.float64(0.1581999999998515),\n",
       " np.float64(0.3344999999999345),\n",
       " np.float64(0.5784000000001015),\n",
       " np.float64(0.1930999999999585),\n",
       " np.float64(0.0444999999999708),\n",
       " np.float64(0.0861999999999625),\n",
       " np.float64(0.2193999999999505),\n",
       " np.float64(0.2319999999999709),\n",
       " np.float64(0.0415000000000418),\n",
       " np.float64(0.0733000000000174),\n",
       " np.float64(0.0205999999998311),\n",
       " np.float64(0.1259000000000014),\n",
       " np.float64(0.0288000000000465),\n",
       " np.float64(0.1245000000001255),\n",
       " np.float64(0.1140000000000327),\n",
       " np.float64(0.1501000000000658),\n",
       " np.float64(0.0630000000001018),\n",
       " np.float64(0.1251999999999498),\n",
       " np.float64(0.1275000000000545),\n",
       " np.float64(0.3152000000000043),\n",
       " np.float64(0.0262000000000171),\n",
       " np.float64(0.0234000000000378),\n",
       " np.float64(0.1475000000000363),\n",
       " np.float64(0.019399999999905),\n",
       " np.float64(0.0209999999999581),\n",
       " np.float64(0.2235000000000582),\n",
       " np.float64(0.3517999999999119),\n",
       " np.float64(0.3517999999999119),\n",
       " np.float64(0.501299999999901),\n",
       " np.float64(0.0361000000000331),\n",
       " np.float64(0.0237999999999374),\n",
       " np.float64(0.058999999999969),\n",
       " np.float64(0.0184999999999035),\n",
       " np.float64(0.0277000000000953),\n",
       " np.float64(0.0907999999999447),\n",
       " np.float64(0.0715999999999894),\n",
       " np.float64(0.0220999999999094),\n",
       " np.float64(0.0202999999999065),\n",
       " np.float64(0.1224999999999454),\n",
       " np.float64(0.1225000000001728),\n",
       " np.float64(0.1503999999999905),\n",
       " np.float64(0.1054000000001451),\n",
       " np.float64(0.1157000000000607),\n",
       " np.float64(0.4699000000000524),\n",
       " np.float64(0.0465000000001509),\n",
       " np.float64(0.0176000000001295),\n",
       " np.float64(0.0819000000001324),\n",
       " np.float64(0.1490999999998621),\n",
       " np.float64(0.0975000000000818),\n",
       " np.float64(0.369799999999941),\n",
       " np.float64(0.0289000000000214),\n",
       " np.float64(0.0979999999999563),\n",
       " np.float64(0.0936999999998988),\n",
       " np.float64(0.1756999999997788),\n",
       " np.float64(0.0886000000000422),\n",
       " np.float64(0.1255000000001018),\n",
       " np.float64(0.0172999999999774),\n",
       " np.float64(0.1014999999999872),\n",
       " np.float64(0.0316000000000258),\n",
       " np.float64(0.3821000000000367),\n",
       " np.float64(0.0475999999998748),\n",
       " np.float64(0.0502000000001316),\n",
       " np.float64(0.2530999999999039),\n",
       " np.float64(0.0530999999998584),\n",
       " np.float64(0.022200000000339),\n",
       " np.float64(0.1485999999999876),\n",
       " np.float64(0.1932999999999083),\n",
       " np.float64(0.0279999999997926),\n",
       " np.float64(0.1061000000004241),\n",
       " np.float64(0.1054000000001451),\n",
       " np.float64(0.1157000000000607),\n",
       " np.float64(0.4699000000000524),\n",
       " np.float64(0.0465000000001509),\n",
       " np.float64(0.0176000000001295),\n",
       " np.float64(0.0819000000001324),\n",
       " np.float64(0.1490999999998621),\n",
       " np.float64(0.0975000000000818),\n",
       " np.float64(0.369799999999941),\n",
       " np.float64(0.0289000000000214),\n",
       " np.float64(0.0979999999999563),\n",
       " np.float64(0.0936999999998988),\n",
       " np.float64(0.1756999999997788),\n",
       " np.float64(0.0886000000000422),\n",
       " np.float64(0.1255000000001018),\n",
       " np.float64(0.0172999999999774),\n",
       " np.float64(0.1014999999999872),\n",
       " np.float64(0.0316000000000258),\n",
       " np.float64(0.3821000000000367),\n",
       " np.float64(0.0475999999998748),\n",
       " np.float64(0.0502000000001316),\n",
       " np.float64(0.2530999999999039),\n",
       " np.float64(0.0530999999998584),\n",
       " np.float64(0.022200000000339),\n",
       " np.float64(0.1485999999999876),\n",
       " np.float64(0.1932999999999083),\n",
       " np.float64(0.0279999999997926),\n",
       " np.float64(0.1061000000004241),\n",
       " np.float64(0.0766000000003259),\n",
       " np.float64(0.1075000000000727),\n",
       " np.float64(0.5864999999998872),\n",
       " np.float64(0.0677000000000589),\n",
       " np.float64(0.8315999999999804),\n",
       " np.float64(0.0230999999998857),\n",
       " np.float64(0.1329000000000633),\n",
       " np.float64(0.1424000000001797),\n",
       " np.float64(0.1260999999999512),\n",
       " np.float64(0.0353000000000065),\n",
       " np.float64(0.1008999999999105),\n",
       " np.float64(0.0688999999997577),\n",
       " np.float64(0.0348999999996522),\n",
       " np.float64(0.009200000000419),\n",
       " np.float64(0.4668000000001484),\n",
       " np.float64(0.0347000000001571),\n",
       " np.float64(0.0185000000001309),\n",
       " np.float64(0.2443000000002939),\n",
       " np.float64(0.3778999999999541),\n",
       " np.float64(0.0277000000000953),\n",
       " np.float64(0.3022999999998319),\n",
       " np.float64(0.1104000000000269)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_chunk_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [np.float64(0.0531999999999754),\n",
       "  np.float64(0.0142000000000166),\n",
       "  np.float64(0.0228999999999928),\n",
       "  np.float64(0.0275000000000034),\n",
       "  np.float64(0.0101999999999975),\n",
       "  np.float64(0.0584999999999809),\n",
       "  np.float64(0.0175999999999874),\n",
       "  np.float64(0.0537000000000205),\n",
       "  np.float64(0.0224000000000046),\n",
       "  np.float64(0.0378000000000042),\n",
       "  np.float64(0.0768999999999664),\n",
       "  np.float64(0.0511999999999943),\n",
       "  np.float64(0.0912999999999897),\n",
       "  np.float64(0.1779999999999972),\n",
       "  np.float64(0.0810999999999921),\n",
       "  np.float64(0.0736999999999739),\n",
       "  np.float64(0.0464000000000055),\n",
       "  np.float64(0.0212000000000216),\n",
       "  np.float64(0.0372999999999592),\n",
       "  np.float64(0.031499999999994),\n",
       "  np.float64(0.1111000000000217),\n",
       "  np.float64(0.016900000000021),\n",
       "  np.float64(0.0271999999999934),\n",
       "  np.float64(0.0158000000000129),\n",
       "  np.float64(0.0059999999999718),\n",
       "  np.float64(0.1580000000000154),\n",
       "  np.float64(0.0815000000000054),\n",
       "  np.float64(0.0780000000000313),\n",
       "  np.float64(0.0147999999999797),\n",
       "  np.float64(0.0375999999999976),\n",
       "  np.float64(0.0280999999999949),\n",
       "  np.float64(0.0262999999999919),\n",
       "  np.float64(0.0237000000000193),\n",
       "  np.float64(0.0219999999999913),\n",
       "  np.float64(0.0620000000000118),\n",
       "  np.float64(0.0371000000000094),\n",
       "  np.float64(0.0258000000000038),\n",
       "  np.float64(0.032100000000014),\n",
       "  np.float64(0.1311999999999784),\n",
       "  np.float64(0.1332999999999629),\n",
       "  np.float64(0.0561000000000149),\n",
       "  np.float64(0.2062999999999988),\n",
       "  np.float64(0.1087999999999738),\n",
       "  np.float64(0.0156000000000062),\n",
       "  np.float64(0.1582000000000221),\n",
       "  np.float64(0.1193000000000097),\n",
       "  np.float64(0.1193000000000097),\n",
       "  np.float64(0.1456000000000017),\n",
       "  np.float64(0.010499999999979),\n",
       "  np.float64(0.0058999999999969),\n",
       "  np.float64(0.1538000000000465),\n",
       "  np.float64(0.1433999999999855),\n",
       "  np.float64(0.0838999999999714),\n",
       "  np.float64(0.0867999999999824),\n",
       "  np.float64(0.0520999999999958),\n",
       "  np.float64(0.0274),\n",
       "  np.float64(0.0435999999999694),\n",
       "  np.float64(0.0596000000000458),\n",
       "  np.float64(0.1582999999999401),\n",
       "  np.float64(0.1046000000000049),\n",
       "  np.float64(0.0854000000000496),\n",
       "  np.float64(0.0730999999999539),\n",
       "  np.float64(0.1480000000000245),\n",
       "  np.float64(0.0208999999999832),\n",
       "  np.float64(0.1388000000000602),\n",
       "  np.float64(0.1023999999999887),\n",
       "  np.float64(0.1317000000000234),\n",
       "  np.float64(0.0965999999999667),\n",
       "  np.float64(0.0236999999999625),\n",
       "  np.float64(0.0248000000000274),\n",
       "  np.float64(0.0311000000000376),\n",
       "  np.float64(0.0658000000000811),\n",
       "  np.float64(0.0878999999999905),\n",
       "  np.float64(0.0965999999999667),\n",
       "  np.float64(0.0670000000000072),\n",
       "  np.float64(0.041300000000092),\n",
       "  np.float64(0.043700000000058),\n",
       "  np.float64(0.129099999999994),\n",
       "  np.float64(0.0942000000000007),\n",
       "  np.float64(0.1669000000000551),\n",
       "  np.float64(0.0855999999999994),\n",
       "  np.float64(0.0386999999999488),\n",
       "  np.float64(0.0833999999999832)],\n",
       " [],\n",
       " [np.float64(0.0059000000000537),\n",
       "  np.float64(0.0984000000000264),\n",
       "  np.float64(0.0595000000000141),\n",
       "  np.float64(9.999999997489796e-05)]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_event_lens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_rpm\n",
    "chunk_event_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_type</th>\n",
       "      <th>regression_line</th>\n",
       "      <th>filt_rel_spike_times</th>\n",
       "      <th>slope</th>\n",
       "      <th>reactivation_ID</th>\n",
       "      <th>warp_factor</th>\n",
       "      <th>mouse</th>\n",
       "      <th>awake_rel_occurance_times</th>\n",
       "      <th>task_involved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.4539199  0.51027892 0.4342718  0.48546027 0...</td>\n",
       "      <td>[0.0448 0.0557 0.041  0.0509 0.     0.0257 0.0...</td>\n",
       "      <td>5.170553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.443771</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.73537798 0.73537798 0.49940425 0.2811263  0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.62004643 0.39491982 0.37708801 0.23109006 0...</td>\n",
       "      <td>[0.     0.0202 0.0218 0.0349 0.0084 0.0204 0.0...</td>\n",
       "      <td>-11.144881</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.111981</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.87948413 0.30634616 0.30634616 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.86585369 0.69513829 0.3929259  0.42906835 0...</td>\n",
       "      <td>[0.     0.0222 0.0615 0.0568 0.0814 0.1038]</td>\n",
       "      <td>-7.689883</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.147243</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.87948413 0.21668602 0.23829101 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[ 0.49788529  0.20586617  0.19078031 -0.184211...</td>\n",
       "      <td>[0.     0.0271 0.0285 0.0633 0.0258 0.0096 0.0...</td>\n",
       "      <td>-10.775613</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-3.008870</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.04104176 0.         0.         0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.42389077 0.53777263 0.31965993 0.29263711 0...</td>\n",
       "      <td>[0.0059 0.     0.0113 0.0127 0.0246 0.0127 0.0...</td>\n",
       "      <td>-19.302009</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-5.389692</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.87948413 0.30634616 0.07390482 0.07390482 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.59548911 1.01292746 0.40594303 0.42396903 0...</td>\n",
       "      <td>[0.1806 0.5627 0.0071 0.0236 0.     0.0152 0.0...</td>\n",
       "      <td>1.092485</td>\n",
       "      <td>641.0</td>\n",
       "      <td>1.141124</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.9164957  0.9164957  0.1243793  0.02768056 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.15433603 0.91594778 0.17987242 0.16329616 0...</td>\n",
       "      <td>[0.17   0.     0.1643 0.168  0.1686 0.17   0.1...</td>\n",
       "      <td>-4.480069</td>\n",
       "      <td>644.0</td>\n",
       "      <td>-4.679531</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.45344187 0.9164957  0.02768056 0.02768056 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.5179733  0.33377115 0.86055995 0.85963789 0...</td>\n",
       "      <td>[0.8174 1.2569 0.     0.0022 0.0041 0.0065 0.1...</td>\n",
       "      <td>-0.419118</td>\n",
       "      <td>645.0</td>\n",
       "      <td>-0.437777</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.45344187 0.45344187 0.9164957  0.9164957  0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[0.68308293 0.43199044 0.46632788 0.2946407  0...</td>\n",
       "      <td>[0.1086 0.0384 0.048  0.     0.0222 0.0299 0.0...</td>\n",
       "      <td>3.576816</td>\n",
       "      <td>653.0</td>\n",
       "      <td>3.736063</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.9164957  0.08428787 0.02768056 0.53141627 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>6.0</td>\n",
       "      <td>[ 0.94971534  0.90342477  0.87826009  0.090388...</td>\n",
       "      <td>[0.     0.0149 0.023  0.2766 0.2809 0.2902 0.3...</td>\n",
       "      <td>-3.106750</td>\n",
       "      <td>657.0</td>\n",
       "      <td>-3.245069</td>\n",
       "      <td>136_1_3</td>\n",
       "      <td>[0.9164957  0.9164957  0.9164957  0.02768056 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      seq_type                                    regression_line  \\\n",
       "0          1.0  [0.4539199  0.51027892 0.4342718  0.48546027 0...   \n",
       "1          1.0  [0.62004643 0.39491982 0.37708801 0.23109006 0...   \n",
       "2          1.0  [0.86585369 0.69513829 0.3929259  0.42906835 0...   \n",
       "3          1.0  [ 0.49788529  0.20586617  0.19078031 -0.184211...   \n",
       "4          1.0  [0.42389077 0.53777263 0.31965993 0.29263711 0...   \n",
       "...        ...                                                ...   \n",
       "1201       6.0  [0.59548911 1.01292746 0.40594303 0.42396903 0...   \n",
       "1202       6.0  [0.15433603 0.91594778 0.17987242 0.16329616 0...   \n",
       "1203       6.0  [0.5179733  0.33377115 0.86055995 0.85963789 0...   \n",
       "1204       6.0  [0.68308293 0.43199044 0.46632788 0.2946407  0...   \n",
       "1205       6.0  [ 0.94971534  0.90342477  0.87826009  0.090388...   \n",
       "\n",
       "                                   filt_rel_spike_times      slope  \\\n",
       "0     [0.0448 0.0557 0.041  0.0509 0.     0.0257 0.0...   5.170553   \n",
       "1     [0.     0.0202 0.0218 0.0349 0.0084 0.0204 0.0... -11.144881   \n",
       "2           [0.     0.0222 0.0615 0.0568 0.0814 0.1038]  -7.689883   \n",
       "3     [0.     0.0271 0.0285 0.0633 0.0258 0.0096 0.0... -10.775613   \n",
       "4     [0.0059 0.     0.0113 0.0127 0.0246 0.0127 0.0... -19.302009   \n",
       "...                                                 ...        ...   \n",
       "1201  [0.1806 0.5627 0.0071 0.0236 0.     0.0152 0.0...   1.092485   \n",
       "1202  [0.17   0.     0.1643 0.168  0.1686 0.17   0.1...  -4.480069   \n",
       "1203  [0.8174 1.2569 0.     0.0022 0.0041 0.0065 0.1...  -0.419118   \n",
       "1204  [0.1086 0.0384 0.048  0.     0.0222 0.0299 0.0...   3.576816   \n",
       "1205  [0.     0.0149 0.023  0.2766 0.2809 0.2902 0.3...  -3.106750   \n",
       "\n",
       "      reactivation_ID  warp_factor    mouse  \\\n",
       "0                 0.0     1.443771  136_1_3   \n",
       "1                 9.0    -3.111981  136_1_3   \n",
       "2                10.0    -2.147243  136_1_3   \n",
       "3                11.0    -3.008870  136_1_3   \n",
       "4                17.0    -5.389692  136_1_3   \n",
       "...               ...          ...      ...   \n",
       "1201            641.0     1.141124  136_1_3   \n",
       "1202            644.0    -4.679531  136_1_3   \n",
       "1203            645.0    -0.437777  136_1_3   \n",
       "1204            653.0     3.736063  136_1_3   \n",
       "1205            657.0    -3.245069  136_1_3   \n",
       "\n",
       "                              awake_rel_occurance_times  task_involved  \n",
       "0     [0.73537798 0.73537798 0.49940425 0.2811263  0...            1.0  \n",
       "1     [0.87948413 0.87948413 0.30634616 0.30634616 0...            1.0  \n",
       "2     [0.87948413 0.87948413 0.21668602 0.23829101 0...            1.0  \n",
       "3     [0.87948413 0.04104176 0.         0.         0...            1.0  \n",
       "4     [0.87948413 0.30634616 0.07390482 0.07390482 0...            1.0  \n",
       "...                                                 ...            ...  \n",
       "1201  [0.9164957  0.9164957  0.1243793  0.02768056 0...            0.0  \n",
       "1202  [0.45344187 0.9164957  0.02768056 0.02768056 0...            0.0  \n",
       "1203  [0.45344187 0.45344187 0.9164957  0.9164957  0...            0.0  \n",
       "1204  [0.9164957  0.08428787 0.02768056 0.53141627 0...            0.0  \n",
       "1205  [0.9164957  0.9164957  0.9164957  0.02768056 0...            0.0  \n",
       "\n",
       "[1206 rows x 9 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.])], [array([1.])], [array([0.])], [array([0.1])]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_rpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_event_lens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.]), array([8.3]), array([0.]), array([0.4])],\n",
       " [array([0.]), array([5.85]), array([0.]), array([0.375])],\n",
       " [array([0.]), array([1.]), array([0.]), array([0.1])]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_rpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2629.961</td>\n",
       "      <td>2629.9265</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>2629.961</td>\n",
       "      <td>[2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...</td>\n",
       "      <td>[45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....</td>\n",
       "      <td>[ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_seq_type  num_spikes  num_neurons  first_spike_time  \\\n",
       "10    243                 2          20     2629.961         2629.9265   \n",
       "\n",
       "    event_length  last_spike_time  \\\n",
       "10        0.0345         2629.961   \n",
       "\n",
       "                                  cluster_spike_times  \\\n",
       "10  [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...   \n",
       "\n",
       "                                      cluster_neurons  \\\n",
       "10  [45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....   \n",
       "\n",
       "                                 spike_plotting_order  coactive_cluster_group  \\\n",
       "10  [ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...                   229.0   \n",
       "\n",
       "   ordering_classification  rem_events  nrem_events  \n",
       "10              sequential           0            0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>2596.9095</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>[2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...</td>\n",
       "      <td>[39.0, 102.0, 103.0, 104.0, 104.0, 104.0]</td>\n",
       "      <td>[84. 62. 61. 60. 60. 60.]</td>\n",
       "      <td>120.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>2610.7146</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>[2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...</td>\n",
       "      <td>[82.0, 96.0, 102.0, 104.0, 104.0, 107.0]</td>\n",
       "      <td>[33. 57. 62. 60. 60. 82.]</td>\n",
       "      <td>125.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>2650.6180</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>[2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...</td>\n",
       "      <td>[90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...</td>\n",
       "      <td>[68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>2654.9429</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>[2654.9429, 2654.9522, 2654.9514, 2654.958, 26...</td>\n",
       "      <td>[30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...</td>\n",
       "      <td>[67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]</td>\n",
       "      <td>145.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>2669.6668</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>[2669.6843, 2669.6847, 2669.678, 2669.6778, 26...</td>\n",
       "      <td>[90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...</td>\n",
       "      <td>[ 68.  58. 115.  64.  64.  56.  61.  60.]</td>\n",
       "      <td>149.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>2675.3538</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>[2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...</td>\n",
       "      <td>[30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...</td>\n",
       "      <td>[67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>2686.4794</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>[2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...</td>\n",
       "      <td>[40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...</td>\n",
       "      <td>[59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>2705.4055</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>[2705.418, 2705.4073, 2705.4135, 2705.4151, 27...</td>\n",
       "      <td>[82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...</td>\n",
       "      <td>[ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>2716.7483</td>\n",
       "      <td>0.3023</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>[2716.7483, 2716.9622, 2716.9666, 2716.986, 27...</td>\n",
       "      <td>[101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...</td>\n",
       "      <td>[56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]</td>\n",
       "      <td>166.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>2725.4187</td>\n",
       "      <td>0.1104</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>[2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...</td>\n",
       "      <td>[35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....</td>\n",
       "      <td>[ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>2629.9265</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>[2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...</td>\n",
       "      <td>[45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....</td>\n",
       "      <td>[ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>2610.2360</td>\n",
       "      <td>1.0350</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>[2610.3344, 2610.678, 2610.8732, 2611.271, 261...</td>\n",
       "      <td>[1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...</td>\n",
       "      <td>[ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>2700.6185</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>[2700.6185, 2700.685, 2700.7036, 2700.7084, 27...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...</td>\n",
       "      <td>159.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_seq_type  num_spikes  num_neurons  first_spike_time  \\\n",
       "0     120                 1           6    2596.9444         2596.9095   \n",
       "1     125                 1           6    2610.7238         2610.7146   \n",
       "2     143                 1          29    2651.0848         2650.6180   \n",
       "3     145                 1          11    2654.9776         2654.9429   \n",
       "4     149                 1           8    2669.6853         2669.6668   \n",
       "5     151                 1          13    2675.5981         2675.3538   \n",
       "6     155                 1          23    2686.8573         2686.4794   \n",
       "7     162                 1          12    2705.4332         2705.4055   \n",
       "8     166                 1          11    2717.0506         2716.7483   \n",
       "9     169                 1          36    2725.5291         2725.4187   \n",
       "10    243                 2          20    2629.9610         2629.9265   \n",
       "11    260                 3          22    2611.2710         2610.2360   \n",
       "12    264                 3          12    2700.7282         2700.6185   \n",
       "\n",
       "    event_length  last_spike_time  \\\n",
       "0         0.0349        2596.9444   \n",
       "1         0.0092        2610.7238   \n",
       "2         0.4668        2651.0848   \n",
       "3         0.0347        2654.9776   \n",
       "4         0.0185        2669.6853   \n",
       "5         0.2443        2675.5981   \n",
       "6         0.3779        2686.8573   \n",
       "7         0.0277        2705.4332   \n",
       "8         0.3023        2717.0506   \n",
       "9         0.1104        2725.5291   \n",
       "10        0.0345        2629.9610   \n",
       "11        1.0350        2611.2710   \n",
       "12        0.1097        2700.7282   \n",
       "\n",
       "                                  cluster_spike_times  \\\n",
       "0   [2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...   \n",
       "1   [2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...   \n",
       "2   [2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...   \n",
       "3   [2654.9429, 2654.9522, 2654.9514, 2654.958, 26...   \n",
       "4   [2669.6843, 2669.6847, 2669.678, 2669.6778, 26...   \n",
       "5   [2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...   \n",
       "6   [2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...   \n",
       "7   [2705.418, 2705.4073, 2705.4135, 2705.4151, 27...   \n",
       "8   [2716.7483, 2716.9622, 2716.9666, 2716.986, 27...   \n",
       "9   [2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...   \n",
       "10  [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...   \n",
       "11  [2610.3344, 2610.678, 2610.8732, 2611.271, 261...   \n",
       "12  [2700.6185, 2700.685, 2700.7036, 2700.7084, 27...   \n",
       "\n",
       "                                      cluster_neurons  \\\n",
       "0           [39.0, 102.0, 103.0, 104.0, 104.0, 104.0]   \n",
       "1            [82.0, 96.0, 102.0, 104.0, 104.0, 107.0]   \n",
       "2   [90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...   \n",
       "3   [30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...   \n",
       "4   [90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...   \n",
       "5   [30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...   \n",
       "6   [40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...   \n",
       "7   [82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...   \n",
       "8   [101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...   \n",
       "9   [35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....   \n",
       "10  [45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....   \n",
       "11  [1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...   \n",
       "12  [1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                 spike_plotting_order  coactive_cluster_group  \\\n",
       "0                           [84. 62. 61. 60. 60. 60.]                   120.0   \n",
       "1                           [33. 57. 62. 60. 60. 82.]                   125.0   \n",
       "2   [68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...                   143.0   \n",
       "3       [67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]                   145.0   \n",
       "4           [ 68.  58. 115.  64.  64.  56.  61.  60.]                   149.0   \n",
       "5   [67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...                   151.0   \n",
       "6   [59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...                   155.0   \n",
       "7   [ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...                   162.0   \n",
       "8       [56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]                   166.0   \n",
       "9   [ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...                   169.0   \n",
       "10  [ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...                   229.0   \n",
       "11  [ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...                   124.0   \n",
       "12  [ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...                   159.0   \n",
       "\n",
       "   ordering_classification  rem_events  nrem_events  \n",
       "0               sequential           0            0  \n",
       "1               sequential           0            0  \n",
       "2               sequential           1            0  \n",
       "3               sequential           1            0  \n",
       "4               sequential           1            0  \n",
       "5               sequential           1            0  \n",
       "6               sequential           1            0  \n",
       "7               sequential           0            0  \n",
       "8               sequential           0            0  \n",
       "9               sequential           0            0  \n",
       "10              sequential           0            0  \n",
       "11              sequential           0            0  \n",
       "12              sequential           0            0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10]),)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>2596.9095</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>2596.9444</td>\n",
       "      <td>[2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...</td>\n",
       "      <td>[39.0, 102.0, 103.0, 104.0, 104.0, 104.0]</td>\n",
       "      <td>[84. 62. 61. 60. 60. 60.]</td>\n",
       "      <td>120.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>2610.7146</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>2610.7238</td>\n",
       "      <td>[2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...</td>\n",
       "      <td>[82.0, 96.0, 102.0, 104.0, 104.0, 107.0]</td>\n",
       "      <td>[33. 57. 62. 60. 60. 82.]</td>\n",
       "      <td>125.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>2650.6180</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>2651.0848</td>\n",
       "      <td>[2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...</td>\n",
       "      <td>[90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...</td>\n",
       "      <td>[68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>2654.9429</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>2654.9776</td>\n",
       "      <td>[2654.9429, 2654.9522, 2654.9514, 2654.958, 26...</td>\n",
       "      <td>[30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...</td>\n",
       "      <td>[67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]</td>\n",
       "      <td>145.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>2669.6668</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>2669.6853</td>\n",
       "      <td>[2669.6843, 2669.6847, 2669.678, 2669.6778, 26...</td>\n",
       "      <td>[90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...</td>\n",
       "      <td>[ 68.  58. 115.  64.  64.  56.  61.  60.]</td>\n",
       "      <td>149.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>2675.3538</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>2675.5981</td>\n",
       "      <td>[2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...</td>\n",
       "      <td>[30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...</td>\n",
       "      <td>[67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>2686.4794</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>2686.8573</td>\n",
       "      <td>[2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...</td>\n",
       "      <td>[40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...</td>\n",
       "      <td>[59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>2705.4055</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>2705.4332</td>\n",
       "      <td>[2705.418, 2705.4073, 2705.4135, 2705.4151, 27...</td>\n",
       "      <td>[82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...</td>\n",
       "      <td>[ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>2716.7483</td>\n",
       "      <td>0.3023</td>\n",
       "      <td>2717.0506</td>\n",
       "      <td>[2716.7483, 2716.9622, 2716.9666, 2716.986, 27...</td>\n",
       "      <td>[101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...</td>\n",
       "      <td>[56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]</td>\n",
       "      <td>166.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>2725.4187</td>\n",
       "      <td>0.1104</td>\n",
       "      <td>2725.5291</td>\n",
       "      <td>[2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...</td>\n",
       "      <td>[35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....</td>\n",
       "      <td>[ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>243</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>2629.9265</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>2629.9610</td>\n",
       "      <td>[2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...</td>\n",
       "      <td>[45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....</td>\n",
       "      <td>[ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...</td>\n",
       "      <td>229.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>260</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>2610.2360</td>\n",
       "      <td>1.0350</td>\n",
       "      <td>2611.2710</td>\n",
       "      <td>[2610.3344, 2610.678, 2610.8732, 2611.271, 261...</td>\n",
       "      <td>[1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...</td>\n",
       "      <td>[ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>2700.6185</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>2700.7282</td>\n",
       "      <td>[2700.6185, 2700.685, 2700.7036, 2700.7084, 27...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...</td>\n",
       "      <td>159.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  cluster_seq_type  num_spikes  num_neurons  first_spike_time  \\\n",
       "0     120                 1           6    2596.9444         2596.9095   \n",
       "1     125                 1           6    2610.7238         2610.7146   \n",
       "2     143                 1          29    2651.0848         2650.6180   \n",
       "3     145                 1          11    2654.9776         2654.9429   \n",
       "4     149                 1           8    2669.6853         2669.6668   \n",
       "5     151                 1          13    2675.5981         2675.3538   \n",
       "6     155                 1          23    2686.8573         2686.4794   \n",
       "7     162                 1          12    2705.4332         2705.4055   \n",
       "8     166                 1          11    2717.0506         2716.7483   \n",
       "9     169                 1          36    2725.5291         2725.4187   \n",
       "10    243                 2          20    2629.9610         2629.9265   \n",
       "11    260                 3          22    2611.2710         2610.2360   \n",
       "12    264                 3          12    2700.7282         2700.6185   \n",
       "\n",
       "    event_length  last_spike_time  \\\n",
       "0         0.0349        2596.9444   \n",
       "1         0.0092        2610.7238   \n",
       "2         0.4668        2651.0848   \n",
       "3         0.0347        2654.9776   \n",
       "4         0.0185        2669.6853   \n",
       "5         0.2443        2675.5981   \n",
       "6         0.3779        2686.8573   \n",
       "7         0.0277        2705.4332   \n",
       "8         0.3023        2717.0506   \n",
       "9         0.1104        2725.5291   \n",
       "10        0.0345        2629.9610   \n",
       "11        1.0350        2611.2710   \n",
       "12        0.1097        2700.7282   \n",
       "\n",
       "                                  cluster_spike_times  \\\n",
       "0   [2596.9141, 2596.9186, 2596.9418, 2596.9095, 2...   \n",
       "1   [2610.7192, 2610.7146, 2610.7207, 2610.7208, 2...   \n",
       "2   [2651.0711, 2651.0848, 2650.7495, 2650.6406, 2...   \n",
       "3   [2654.9429, 2654.9522, 2654.9514, 2654.958, 26...   \n",
       "4   [2669.6843, 2669.6847, 2669.678, 2669.6778, 26...   \n",
       "5   [2675.3538, 2675.5296, 2675.4724, 2675.5379, 2...   \n",
       "6   [2686.4794, 2686.4908, 2686.5422, 2686.6195, 2...   \n",
       "7   [2705.418, 2705.4073, 2705.4135, 2705.4151, 27...   \n",
       "8   [2716.7483, 2716.9622, 2716.9666, 2716.986, 27...   \n",
       "9   [2725.4542, 2725.4187, 2725.4283, 2725.4507, 2...   \n",
       "10  [2629.9385, 2629.9392, 2629.9454, 2629.9278, 2...   \n",
       "11  [2610.3344, 2610.678, 2610.8732, 2611.271, 261...   \n",
       "12  [2700.6185, 2700.685, 2700.7036, 2700.7084, 27...   \n",
       "\n",
       "                                      cluster_neurons  \\\n",
       "0           [39.0, 102.0, 103.0, 104.0, 104.0, 104.0]   \n",
       "1            [82.0, 96.0, 102.0, 104.0, 104.0, 107.0]   \n",
       "2   [90.0, 90.0, 96.0, 98.0, 98.0, 98.0, 100.0, 10...   \n",
       "3   [30.0, 30.0, 90.0, 92.0, 92.0, 100.0, 104.0, 1...   \n",
       "4   [90.0, 98.0, 99.0, 100.0, 100.0, 101.0, 103.0,...   \n",
       "5   [30.0, 39.0, 82.0, 101.0, 101.0, 102.0, 102.0,...   \n",
       "6   [40.0, 40.0, 40.0, 40.0, 40.0, 90.0, 90.0, 101...   \n",
       "7   [82.0, 90.0, 90.0, 90.0, 90.0, 99.0, 100.0, 10...   \n",
       "8   [101.0, 102.0, 102.0, 102.0, 102.0, 102.0, 102...   \n",
       "9   [35.0, 90.0, 90.0, 90.0, 90.0, 96.0, 98.0, 99....   \n",
       "10  [45.0, 49.0, 49.0, 51.0, 51.0, 51.0, 51.0, 52....   \n",
       "11  [1.0, 2.0, 2.0, 40.0, 45.0, 45.0, 45.0, 45.0, ...   \n",
       "12  [1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                 spike_plotting_order  coactive_cluster_group  \\\n",
       "0                           [84. 62. 61. 60. 60. 60.]                   120.0   \n",
       "1                           [33. 57. 62. 60. 60. 82.]                   125.0   \n",
       "2   [68. 68. 57. 58. 58. 58. 64. 61. 61. 61. 61. 6...                   143.0   \n",
       "3       [67. 67. 68. 75. 75. 64. 60. 60. 83. 83. 82.]                   145.0   \n",
       "4           [ 68.  58. 115.  64.  64.  56.  61.  60.]                   149.0   \n",
       "5   [67. 84. 33. 56. 56. 62. 62. 62. 62. 62. 61. 6...                   151.0   \n",
       "6   [59. 59. 59. 59. 59. 68. 68. 56. 62. 62. 62. 6...                   155.0   \n",
       "7   [ 33.  68.  68.  68.  68. 115.  64.  64.  64. ...                   162.0   \n",
       "8       [56. 62. 62. 62. 62. 62. 62. 62. 61. 61. 61.]                   166.0   \n",
       "9   [ 69.  68.  68.  68.  68.  57.  58. 115.  56. ...                   169.0   \n",
       "10  [ 99.  98.  98. 102. 102. 102. 102.  94.  94. ...                   229.0   \n",
       "11  [ 47.  48.  48.  59.  99.  99.  99.  99.  92. ...                   124.0   \n",
       "12  [ 47.  47.  47.  47.  47.  46.  46.  46.  46. ...                   159.0   \n",
       "\n",
       "   ordering_classification  rem_events  nrem_events  \n",
       "0               sequential           0            0  \n",
       "1               sequential           0            0  \n",
       "2               sequential           1            0  \n",
       "3               sequential           1            0  \n",
       "4               sequential           1            0  \n",
       "5               sequential           1            0  \n",
       "6               sequential           1            0  \n",
       "7               sequential           0            0  \n",
       "8               sequential           0            0  \n",
       "9               sequential           0            0  \n",
       "10              sequential           0            0  \n",
       "11              sequential           0            0  \n",
       "12              sequential           0            0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(11)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_id in seq_order:\n",
    "    np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(array([10]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:173\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(array([10]),)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfiltered_chunk_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_chunk_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_seq_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3824\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3824\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\envs\\analysis_main\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6072\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   6068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   6069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   6070\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   6071\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 6072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (array([10]),)"
     ]
    }
   ],
   "source": [
    "filtered_chunk_data[np.where(filtered_chunk_data.cluster_seq_type == seq_id+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 3, 1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, mouse in enumerate(list(all_mice_dict)):\n",
    "    replay_data_found = False\n",
    "    print('----------')\n",
    "    print(mouse)\n",
    "    animals += [mouse]\n",
    "    print(index)\n",
    "    if '_final_analysis_output' in os.listdir(all_mice_dict[mouse]['sleep_path']):\n",
    "\n",
    "        # load in replay data \n",
    "        for run_index,pp_file in enumerate(os.listdir(dat_path)):\n",
    "            if mouse_file in pp_file:\n",
    "                print(mouse_file)\n",
    "                # set path to processed files \n",
    "                current_mouse_path = dat_path + pp_file + '\\\\_final_analysis_output'\n",
    "                print('replay data path found: ' + current_mouse_path)\n",
    "                replay_data_found = True\n",
    "                \n",
    "                # load in sleep start time \n",
    "                current_sleep_start = sleep_start[mouse]\n",
    "                params_file = dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json'\n",
    "                with open(params_file, 'r') as file:\n",
    "                    params = json.load(file)\n",
    "                time_spans = params['time_span']\n",
    "                            \n",
    "                break\n",
    "\n",
    "    if not replay_data_found:\n",
    "        print('No replay data found for ' + mouse_file)\n",
    "        continue\n",
    "    \n",
    "    # rpm\n",
    "    chunk_rpm= []\n",
    "    # event lengths\n",
    "    chunk_event_lens = []\n",
    "    # decay \n",
    "    chunk_binned_rate,chunk_bins_relative_so = [],[]\n",
    "    # coactive freqs\n",
    "    coactive_freqs_chunk  = {}\n",
    "    # ordered vs misordered\n",
    "    chunk_ordered_misordered_proportions = []\n",
    "    # task vs non task related events\n",
    "    nontask_task_chunk = []\n",
    "    # awake seq by seq neuron involvements\n",
    "    chunk_mouse_neuron_rel_awake_positions_reverse = []\n",
    "    chunk_mouse_neuron_rel_reverse_replay_positions = []\n",
    "    chunk_mouse_neuron_rel_awake_positions_forwards = []\n",
    "    chunk_mouse_neuron_rel_forward_replay_positions = []\n",
    "    \n",
    "    ## loop across all chunk files ################################\n",
    "    for chunk_number,file in enumerate(os.listdir(current_mouse_path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            current_data_path = current_mouse_path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(current_data_path + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(current_data_path + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            ###### FILTERING AND MASKING ##################################################################''\n",
    "            ## filter this data for sequential ordering\n",
    "            sequential_condition = data.ordering_classification == 'sequential'\n",
    "            # filter is set up so that any true will carry forward \n",
    "            filtered_chunk_data = data[sequential_condition].reset_index()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_ppseq_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\paper_submission\\post_sleep\\\\\"\n",
    "out_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\behaviour_to_replay\\processed_data\\\\\" \n",
    "\n",
    "useable_mirs  = [ '149_1_2']\n",
    "# done '136_1_3', '136_1_4', '149_1_1', '149_1_2', '149_1_3','178_1_4', '178_1_5', '178_1_6', '178_1_7','178_1_8','178_1_9', '178_2_1', '178_2_2', '178_2_4', '268_1_10','269_1_4',\n",
    "#['ap5R_1_1','ap5R_1_2','ap5R_1_3','seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11','seq008_1_3']\n",
    "\n",
    "# load in sleep time points\n",
    "sleep_time_point_df = pd.read_csv(sleep_ppseq_path + 'sleep_time_points.csv')\n",
    "# decide when sleep started\n",
    "sleep_start = {}\n",
    "for index,value in enumerate(sleep_time_point_df.approx_sleep_start.values):\n",
    "    mouse = sleep_time_point_df.mir.values[index]\n",
    "    sleep_start[mouse] = value\n",
    "# define mice/sessions in each group    \n",
    "expert_mice = sleep_time_point_df[sleep_time_point_df.group == 'expert'].mir.values\n",
    "hlesion_mice = sleep_time_point_df[sleep_time_point_df.group == 'h_lesion'].mir.values\n",
    "learning_mice = sleep_time_point_df[sleep_time_point_df.group == 'learning'].mir.values\n",
    "\n",
    "\n",
    "var_dict = {'expert':[],'hlesion':[],'learning' :[],'mirs':[],'current_sleep_start':[], 'time_spans':[]}\n",
    "\n",
    "#Load in seq order data \n",
    "sequence_order_df = pd.read_csv(sleep_ppseq_path+\"sequence_order.csv\")\n",
    "# get all the relevant path name and some other data \n",
    "current_mouse_path,var_dict = find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start)\n",
    "# loop across each mouse path:\n",
    "for loop_index, path in enumerate(current_mouse_path):\n",
    "    # create empty chunk vars dict\n",
    "    chunk_vars = empty_chunk_vars()\n",
    "    \n",
    "    print(f\"run index: {loop_index}, processing {var_dict['mirs'][loop_index]}\")\n",
    "\n",
    "    ## loop across all chunk files\n",
    "    for chunk_index, file in enumerate(os.listdir(path)):\n",
    "        if 'chunk' in file:\n",
    "            print(file)\n",
    "            path_ = path + '\\\\' + file + '\\\\'\n",
    "            chunk_time = np.load(path_ + 'chunk_time_interval.npy')\n",
    "            data = pd.read_csv(path_ + 'filtered_replay_clusters_df.csv')\n",
    "            \n",
    "            # filter based on the sequential/rem-nrem conditions set above\n",
    "            filter_mask = make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only)\n",
    "            filtered_chunk_data = data[filter_mask].reset_index()\n",
    "            \n",
    "            # how many reactivations found\n",
    "            reactivations_found = len(filtered_chunk_data)\n",
    "            print(reactivations_found)\n",
    "            \n",
    "            ####################################### chunk rate per minute: (# this one depends on rem/nrem filter... )\n",
    "            mins = determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path_)\n",
    "            if mins > 0:\n",
    "                chunk_vars['chunk_reactivations'] += [reactivations_found] \n",
    "                chunk_vars['chunk_mins'] += [mins]     \n",
    "                \n",
    "            ####################################### replay rate per motif type\n",
    "            all_motif_type_reactivations = []\n",
    "            all_motif_type_reactivations_min = []\n",
    "            all_motif_type_relative_proportion = []\n",
    "            for seq_type in range(1,7):\n",
    "                motif_type_reactivations = [len(np.where(filtered_chunk_data.cluster_seq_type.values == seq_type)[0])][0]\n",
    "                all_motif_type_reactivations += [motif_type_reactivations]\n",
    "            chunk_vars['chunk_motif_type_reactivations'] += [all_motif_type_reactivations]\n",
    "            \n",
    "            ##################################### av. spikes involved\n",
    "            chunk_vars['mean_spikes_per_event'] += [[len(item) for item in filtered_chunk_data.cluster_spike_times]]\n",
    "            # per motif\n",
    "            motif_by_motif_mean_spikes_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_spikes_per_event += [[len(item) for item in motif_data.cluster_spike_times]]\n",
    "            chunk_vars['motif_by_motif_mean_spikes_per_event'] += [motif_by_motif_mean_spikes_per_event]  \n",
    "                    \n",
    "            ########################################## average units involved \n",
    "            chunk_vars['mean_units_per_event'] += [[len(np.unique(ast.literal_eval(item))) for item in filtered_chunk_data.cluster_neurons]]\n",
    "            motif_by_motif_mean_units_per_event = []\n",
    "            for motif_number in range(1,7):\n",
    "                motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "                motif_by_motif_mean_units_per_event += [[len(np.unique(ast.literal_eval(item))) for item in motif_data.cluster_neurons]]\n",
    "            chunk_vars['motif_by_motif_mean_units_per_event'] += [motif_by_motif_mean_units_per_event]\n",
    "            \n",
    "            ########################################### replay length overall \n",
    "            chunk_vars['chunk_event_lengths'] += [filtered_chunk_data.event_length.values]\n",
    "            \n",
    "            ########################################### replay length per motif \n",
    "            motif_event_lenghts = []\n",
    "            for i in range(1,7):\n",
    "                motif_event_lenghts += [filtered_chunk_data[filtered_chunk_data.cluster_seq_type == i].event_length.values]\n",
    "            chunk_vars['motif_event_lenghts'] += [motif_event_lenghts]\n",
    "            \n",
    "            ########################################### coactive rate overall\n",
    "            event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "            # refind the clusters\n",
    "            filtered_chunk_data  = refind_cluster_events(filtered_chunk_data,event_proximity_filter)\n",
    "            # how many single evtns coactivly paired? average coactive rate? proportion of global events coactive? \n",
    "            cocative_total,coactive_len_per_chunk,overall_total = coactive_rate(filtered_chunk_data)\n",
    "            chunk_vars['total_single_events_coacitvely_paired'] += [cocative_total]\n",
    "            chunk_vars['coactive_lenghts'] += [coactive_len_per_chunk]\n",
    "            chunk_vars['overall_total_coactive_or_single_cluster_events'] += [overall_total]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # ordering of coactive?\n",
    "            if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "                multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "            else:\n",
    "                multi_cluster_df,meaned_order,fs_order = [],[],[]\n",
    "\n",
    "            # pull out sequence order for current mouse\n",
    "            seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "            num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "            real_order = np.array(seq_order)+1\n",
    "\n",
    "            #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "            if not len(real_order) == num_dominant_seqs:\n",
    "                dominant = list(real_order[0:num_dominant_seqs])\n",
    "                other_ = list(real_order[num_dominant_seqs::])\n",
    "            else:\n",
    "                dominant = list(real_order)\n",
    "                other_ = []\n",
    "                \n",
    "            # orderng amounts for mean ordering - this calculated for each pair in the chunk \n",
    "            ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)\n",
    "            chunk_vars['meaned_order_task_related_ordered'] += [ordered]\n",
    "            chunk_vars['meaned_order_task_related_misordered'] += [misordered]\n",
    "            chunk_vars['meaned_order_task_related_other'] += [other]\n",
    "            \n",
    "            # orderng amounts for first spike ordering\n",
    "            ordered,misordered,other = calculate_ordering_amounts(fs_order,dominant,other_)\n",
    "            chunk_vars['fs_order_task_related_ordered'] += [ordered]\n",
    "            chunk_vars['fs_order_task_related_misordered'] += [misordered]\n",
    "            chunk_vars['fs_order_task_related_other'] += [other]\n",
    "            ### motif by motif:\n",
    "            # does one motif appear more in coactive?\n",
    "            if not chunk_vars['total_single_events_coacitvely_paired'][chunk_index] == 0:\n",
    "                all_motifs_total_coactive = all_motifs_proportion_coactive(multi_cluster_df)\n",
    "                chunk_vars['all_motifs_total_coactive'] += [all_motifs_total_coactive]\n",
    "            else:\n",
    "                chunk_vars['all_motifs_total_coactive'] += [[0,0,0,0,0,0]]\n",
    "            \n",
    "            # does one motif appeaer more ordered? \n",
    "            # # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "            # meaned ordering \n",
    "            all_motifs_meaned_ordering_task_related_ordered,all_motifs_meaned_ordering_task_related_misordered,all_motifs_meaned_ordering_task_related_other = motif_by_motif_ordering(meaned_order,real_order,dominant,other_)\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_ordered'] += [all_motifs_meaned_ordering_task_related_ordered]\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_misordered'] += [all_motifs_meaned_ordering_task_related_misordered]\n",
    "            chunk_vars['meaned_ordering_all_motifs_task_related_other'] += [all_motifs_meaned_ordering_task_related_other]\n",
    "            \n",
    "            # first spike ordering \n",
    "            all_motifs_fs_task_related_ordered,all_motifs_fs_task_related_misordered,all_motifs_fs_task_related_other = motif_by_motif_ordering(fs_order,real_order,dominant,other_)\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_ordered'] += [all_motifs_fs_task_related_ordered]\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_misordered'] += [all_motifs_fs_task_related_misordered]\n",
    "            chunk_vars['fs_ordering_all_motifs_task_related_other'] += [all_motifs_fs_task_related_other]\n",
    "\n",
    "            ########################################### task related vs other rate\n",
    "            task_seqs = np.array(seq_order)+1\n",
    "            # mask each condition\n",
    "            mask = np.isin(filtered_chunk_data.cluster_seq_type.values, task_seqs)\n",
    "            opposite_mask = ~mask\n",
    "            task_related = filtered_chunk_data[mask]\n",
    "            non_task_related = filtered_chunk_data[opposite_mask]\n",
    "\n",
    "            #  task v nontask overallrate\n",
    "            chunk_vars['normalised_task_related_total'] += [len(task_related)/len(task_seqs)]\n",
    "            if len(task_seqs) == 6:\n",
    "                chunk_vars['normalised_non_task_related_total'] += [0]\n",
    "            else:\n",
    "                chunk_vars['normalised_non_task_related_total'] += [len(non_task_related)/(6-len(task_seqs))]\n",
    "            \n",
    "            ### extra stuff to add in:\n",
    "            \n",
    "            # same but motif by motif\n",
    "            \n",
    "            # number of spikes task related \n",
    "            # number of units task related\n",
    "            \n",
    "            # # coative rate \n",
    "            # task_related_number = 0\n",
    "            # non_task_related_number = 0\n",
    "            # for coactive_ in meaned_order:\n",
    "            #     for motif_item in coactive_:\n",
    "            #         if motif_item in task_seqs:\n",
    "            #             task_related_number += 1\n",
    "            #         else:\n",
    "            #             non_task_related_number += 1\n",
    "            # # make it relative:\n",
    "            # task_related_number = task_related_number/len(task_seqs) \n",
    "            # non_task_related_number = non_task_related_number/(6-len(task_seqs))\n",
    "            # proportion_coacitve_event_that_are_task_related = task_related_number/(task_related_number+non_task_related_number)\n",
    "            # chunk_vars['proportion_coacitve_event_that_are_task_related'] = proportion_coacitve_event_that_are_task_related\n",
    "            \n",
    "            # # motif coactive rate for task and non task \n",
    "            # # task\n",
    "            # task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "            # task_proportion_single_events_coacitvely_paired,task_av_coactive_len_per_chunk,task_proporiton_of_events_coactive = coactive_rate(task_related)\n",
    "            # # non task:\n",
    "            # non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "            # nontask_proportion_single_events_coacitvely_paired,nontask_av_coactive_len_per_chunk,nontask_proporiton_of_events_coactive = coactive_rate(non_task_related)\n",
    "\n",
    "            # replay length\n",
    "            #task v non task\n",
    "            # motif by motif  \n",
    "            # spikes involved\n",
    "            \n",
    "            # save out to newly made place\n",
    "            \n",
    "        ###now do averages for each chunk and save out to a new file\n",
    "        ########## Calculate averages across chunks/ combine across chunks for each data variable\n",
    "        out_vars = {}\n",
    "\n",
    "        # 1 overall event rate \n",
    "        if sum(chunk_vars['chunk_reactivations']) == 0:\n",
    "            out_vars['event_rpm'] = 0\n",
    "        else:\n",
    "            out_vars['event_rpm'] = sum(chunk_vars['chunk_reactivations'])/sum(chunk_vars['chunk_mins'])\n",
    "\n",
    "        # 2 motif by motif event rate \n",
    "        out_vars['motif_event_rpm'] = np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)/sum(chunk_vars['chunk_mins'])\n",
    "\n",
    "        # 3 motif by motif proportion of all events\n",
    "        out_vars['motif_relative_event_proportion'] = np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)/sum(np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0))\n",
    "\n",
    "        # 4 spikes per replay event\n",
    "        chunk_spikes_per_event = chunk_vars['mean_spikes_per_event']\n",
    "        spikes_per_event = [item for sublist in chunk_spikes_per_event for item in sublist]\n",
    "        out_vars['spikes_per_event'] = spikes_per_event\n",
    "\n",
    "        # 5 motif by motif spikes per replay event\n",
    "        motif_by_motif_spikes_per_event = []\n",
    "        for seq in range(1,7):\n",
    "            motif_spikes_per_event = []\n",
    "            for chunk_ in chunk_vars['motif_by_motif_mean_spikes_per_event']:\n",
    "                motif_spikes_per_event +=chunk_[seq-1]\n",
    "            motif_by_motif_spikes_per_event += [motif_spikes_per_event]\n",
    "        out_vars['motif_by_motif_spikes_per_event'] = motif_by_motif_spikes_per_event\n",
    "\n",
    "        # 6 units per event\n",
    "        chunk_units_per_event = chunk_vars['mean_units_per_event']\n",
    "        units_per_event = [item for sublist in chunk_units_per_event for item in sublist]\n",
    "        out_vars['units_per_event'] = units_per_event\n",
    "\n",
    "        # 7 motif by motif units per replay event\n",
    "        motif_by_motif_units_per_event = []\n",
    "        for seq in range(1,7):\n",
    "            motif_units_per_event = []\n",
    "            for chunk_ in chunk_vars['motif_by_motif_mean_units_per_event']:\n",
    "                motif_units_per_event +=chunk_[seq-1]\n",
    "            motif_by_motif_units_per_event += [motif_units_per_event]\n",
    "        out_vars['motif_by_motif_units_per_event'] = motif_by_motif_units_per_event\n",
    "\n",
    "        # 8 event lengths\n",
    "        chunk_event_lengths = chunk_vars['chunk_event_lengths']\n",
    "        event_lengths = [item for sublist in chunk_event_lengths for item in sublist]\n",
    "        out_vars['event_lengths'] = event_lengths\n",
    "\n",
    "        # 9 motif by motif event lengths\n",
    "        motif_by_motif_event_lengths = []\n",
    "        for seq in range(1,7):\n",
    "            motif_event_lengths = []\n",
    "            for chunk_ in chunk_vars['motif_event_lenghts']:\n",
    "                motif_event_lengths +=list(chunk_[seq-1])\n",
    "            motif_by_motif_event_lengths += [motif_event_lengths]\n",
    "        out_vars['motif_by_motif_event_lengths'] = motif_by_motif_event_lengths\n",
    "\n",
    "        # 10 coactive rate (proportion of single events coacitvly paired)\n",
    "        if sum(chunk_vars['total_single_events_coacitvely_paired']) == 0:\n",
    "            out_vars['proportion_single_events_coactivly_paired'] = 0\n",
    "        else:\n",
    "            out_vars['proportion_single_events_coactivly_paired'] = sum(chunk_vars['total_single_events_coacitvely_paired'])/sum(chunk_vars['overall_total_coactive_or_single_cluster_events'])\n",
    "\n",
    "\n",
    "        # 11 number of motifs in each coative group\n",
    "        out_vars['coactive_group_lengths'] = [item for sublist in chunk_vars['coactive_lenghts']  for item in sublist]\n",
    "\n",
    "        # 12 meaned order\n",
    "        # ordered proporiton out of all \n",
    "        ordered = sum(chunk_vars['meaned_order_task_related_ordered'])\n",
    "        misordered = sum(chunk_vars['meaned_order_task_related_misordered'])\n",
    "        other = sum(chunk_vars['meaned_order_task_related_other'])\n",
    "        if ordered+misordered+other == 0:\n",
    "            out_vars['meaned_order_overall_ordered_prop'] = 0\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            out_vars['meaned_order_task_related_ordered_prop'] = 0\n",
    "            # other proportion\n",
    "            out_vars['meaned_order_overall_other_prop'] = 0\n",
    "        else:\n",
    "            out_vars['meaned_order_overall_ordered_prop'] = ordered/(ordered+misordered+other)\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            if ordered+misordered == 0:\n",
    "                out_vars['meaned_order_task_related_ordered_prop'] = 0\n",
    "            else:\n",
    "                out_vars['meaned_order_task_related_ordered_prop'] = ordered/(ordered+misordered)\n",
    "            # other proportion\n",
    "            out_vars['meaned_order_overall_other_prop'] = other/(ordered+misordered+other)\n",
    "\n",
    "        # 13 first spike order\n",
    "        # ordered proporiton out of all \n",
    "        ordered = sum(chunk_vars['fs_order_task_related_ordered'])\n",
    "        misordered = sum(chunk_vars['fs_order_task_related_misordered'])\n",
    "        other = sum(chunk_vars['fs_order_task_related_other'])\n",
    "        if ordered+misordered+other == 0:\n",
    "            out_vars['fs_order_overall_ordered_prop'] = 0\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            out_vars['fs_order_task_related_ordered_prop'] = 0\n",
    "            # other proportion\n",
    "            out_vars['fs_order_overall_other_prop'] = 0\n",
    "        else:\n",
    "            out_vars['fs_order_overall_ordered_prop'] = ordered/(ordered+misordered+other)\n",
    "            # ordered proporito out of taks related (ordered/misodered)\n",
    "            if ordered+misordered == 0:\n",
    "                out_vars['fs_order_task_related_ordered_prop'] = 0\n",
    "            else:\n",
    "                out_vars['fs_order_task_related_ordered_prop'] = ordered/(ordered+misordered)\n",
    "            # other proportion\n",
    "            out_vars['fs_order_overall_other_prop'] = other/(ordered+misordered+other)\n",
    "\n",
    "        # 14 motif by motif proportion coactive, what proprotion of each motif is coactive\n",
    "        out_vars['motif_proportion_coactive'] = np.sum(chunk_vars['all_motifs_total_coactive'],axis = 0)/np.sum(chunk_vars['chunk_motif_type_reactivations'],axis = 0)\n",
    "\n",
    "        # 15 does one motif appeaer more ordered? Proportion of moitfs that were in ordered events for coactive task related motifs\n",
    "        # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "        ordered_prop = []\n",
    "        for seq in range(1,7):\n",
    "            ordered = []\n",
    "            misordered = []\n",
    "            other  = []\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_ordered']:\n",
    "                ordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_misordered']:\n",
    "                misordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['meaned_ordering_all_motifs_task_related_other']:\n",
    "                other += [chunk[seq-1]]\n",
    "            if not 'nan' in ordered and (sum(ordered)+sum(misordered)+sum(other)) > 0:\n",
    "                ordered_prop += [sum(ordered)/(sum(ordered)+sum(misordered)+sum(other))]\n",
    "            else:\n",
    "                ordered_prop += ['nan']\n",
    "        out_vars['motif_meaned_ordering_ordered_prop_out_of_all_task_related'] = ordered_prop\n",
    "\n",
    "        # 16 same but for first spike ordering\n",
    "        ordered_prop = []\n",
    "        for seq in range(1,7):\n",
    "            ordered = []\n",
    "            misordered = []\n",
    "            other  = []\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_ordered']:\n",
    "                ordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_misordered']:\n",
    "                misordered += [chunk[seq-1]]\n",
    "            for chunk in chunk_vars['fs_ordering_all_motifs_task_related_other']:\n",
    "                other += [chunk[seq-1]]\n",
    "            if not 'nan' in ordered and (sum(ordered)+sum(misordered)+sum(other)) > 0:\n",
    "                ordered_prop += [sum(ordered)/(sum(ordered)+sum(misordered)+sum(other))]\n",
    "            else:\n",
    "                ordered_prop += ['nan']\n",
    "        out_vars['motif_fs_ordering_ordered_prop_out_of_all_task_related'] = ordered_prop\n",
    "        \n",
    "        if sum(chunk_vars['normalised_non_task_related_total']) == 0:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = sum(chunk_vars['normalised_task_related_total'])\n",
    "        elif sum(chunk_vars['normalised_task_related_total']) == 0:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = 0\n",
    "        else:\n",
    "            out_vars['Ratio_task_related_to_non_task_related_events'] = sum(chunk_vars['normalised_task_related_total'])/sum(chunk_vars['normalised_non_task_related_total'])\n",
    "\n",
    "\n",
    "        ####### SAVE OUT THE DATA \n",
    "\n",
    "        try: \n",
    "            int(var_dict['mirs'][loop_index].split('_')[0])\n",
    "            current_save_path = out_path + 'EJT' + var_dict['mirs'][loop_index] + '\\\\' + save_var + '\\\\'\n",
    "        except:\n",
    "            current_save_path = out_path + var_dict['mirs'][loop_index] + '\\\\' + save_var + '\\\\'\n",
    "            \n",
    "\n",
    "        ## if the path doesnt exist, make a new dir \n",
    "        if not os.path.exists(current_save_path):\n",
    "            os.makedirs(current_save_path)\n",
    "            \n",
    "        # convert all np arrays to list\n",
    "        for key, value in out_vars.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                out_vars[key] = value.tolist()\n",
    "\n",
    "        ### save out the out_vars dict\n",
    "        with open(current_save_path + 'replay_data_variables.json', 'w') as file:\n",
    "            json.dump(out_vars, file)\n",
    "            \n",
    "        # convert all np arrays to list\n",
    "        for key, value in var_dict.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                var_dict[key] = value.tolist()\n",
    "            \n",
    "        ### save out the var_dict \n",
    "        with open(current_save_path + 'general_mouse_info.json', 'w') as file:\n",
    "            json.dump(var_dict, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_mouse_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
