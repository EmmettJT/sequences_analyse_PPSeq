{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict):\n",
    "    # if session in one of the groups (and define which)   \n",
    "    if mouse in list(expert_mice) + list(hlesion_mice) + list(learning_mice):\n",
    "        if mouse in expert_mice:\n",
    "            var_dict['expert'] += [1]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [0]               \n",
    "        elif mouse in hlesion_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [1]\n",
    "            var_dict['learning'] += [0]   \n",
    "        elif mouse in learning_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [1]   \n",
    "    return var_dict\n",
    "\n",
    "def get_time_span(dat_path,pp_file,mouse):\n",
    "    with open(dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json', 'r') as file:\n",
    "        params = json.load(file)\n",
    "    time_spans = params['time_span']\n",
    "    return time_spans\n",
    "\n",
    "def find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start):\n",
    "    current_mouse_path = []\n",
    "    for run_index,pp_file in enumerate(os.listdir(sleep_ppseq_path)):\n",
    "        if not 'sleep_time_points' in pp_file:\n",
    "            # current mouse\n",
    "            mouse = '_'.join(pp_file.split('_')[0:3])    \n",
    "\n",
    "            if mouse in useable_mirs:\n",
    "                    #print out progress\n",
    "                    print(f\"run index: {run_index}, processing {mouse}\")\n",
    "                    \n",
    "                    # asign to experimental group in var_dict\n",
    "                    var_dict = assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict)\n",
    "\n",
    "                    # load in sleep start time and time span\n",
    "                    var_dict['current_sleep_start'] += sleep_start[mouse]\n",
    "                    var_dict['time_spans'] += get_time_span(sleep_ppseq_path,pp_file,mouse)\n",
    "\n",
    "                    # set path to processed files \n",
    "                    current_mouse_path += [sleep_ppseq_path + pp_file + '\\\\analysis_output\\\\']\n",
    "                    var_dict['mirs'] += [mouse]\n",
    "    return current_mouse_path,var_dict\n",
    "\n",
    "\n",
    "def make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only):\n",
    "    ## filter this data\n",
    "    if sequential_filter == True: \n",
    "        sequential_condition = data.ordering_classification == 'sequential'\n",
    "    else:\n",
    "        sequential_condition = np.array([True]*len(data.ordering_classification))\n",
    "\n",
    "    if sleep_filters_on == True:\n",
    "        if nrem_filter == True: \n",
    "            nrem_condition = data.nrem_events == 1\n",
    "        else:\n",
    "            nrem_condition = np.array([False]*len(data.nrem_events))\n",
    "\n",
    "        if rem_filter == True: \n",
    "            rem_condition = data.rem_events == 1\n",
    "        else:\n",
    "            rem_condition = np.array([False]*len(data.rem_events))\n",
    "\n",
    "        if background_only == True:\n",
    "            rem_condition = data.rem_events == 0\n",
    "            nrem_condition = data.nrem_events == 0\n",
    "\n",
    "    else:\n",
    "        nrem_condition = np.array([True]*len(data))\n",
    "        rem_condition = np.array([True]*len(data))\n",
    "        \n",
    "    # filter is set up so that any true will carry forward \n",
    "    filter_mask = sequential_condition * (nrem_condition + rem_condition)\n",
    "        \n",
    "    return filter_mask\n",
    "\n",
    "def determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path):\n",
    "    # if sleep_filters_on is false, use all chunk time\n",
    "    if sleep_filters_on == False:\n",
    "        mins = np.diff(chunk_time)[0]\n",
    "    else:\n",
    "        # load in state times\n",
    "        rem_state_times = np.load(path + 'rem_state_times.npy')\n",
    "        nrem_state_times = np.load(path + 'nrem_state_times.npy')\n",
    "        if len(rem_state_times) > 0:\n",
    "            tot_rem = sum(np.diff(rem_state_times))[0]\n",
    "        else:\n",
    "            tot_rem = 0\n",
    "        if len(nrem_state_times) > 0:\n",
    "            tot_nrem = sum(np.diff(nrem_state_times))[0]\n",
    "        else:\n",
    "            tot_nrem = 0\n",
    "\n",
    "        # if background then use all non rem and non nrem times\n",
    "        if background_only:\n",
    "            mins = np.diff(chunk_time)[0] - (tot_rem+tot_nrem)\n",
    "        else:\n",
    "            # if both, use both \n",
    "            if nrem_filter == True and rem_filter == True:\n",
    "                mins = tot_rem+tot_nrem\n",
    "            elif nrem_filter == True and rem_filter == False:\n",
    "                mins = tot_nrem\n",
    "            elif nrem_filter == False and rem_filter == True:\n",
    "                mins = tot_rem\n",
    "    # convert to mins            \n",
    "    mins = mins/60\n",
    "    \n",
    "    return mins\n",
    "\n",
    "def empty_chunk_vars():\n",
    "    ## set chunk vars \n",
    "    chunk_vars = {\"chunk_rpm\": [],           \n",
    "    \"chunk_motif_type_reactivations\" :[],\n",
    "    \"chunk_motif_type_reactivations_min\" :[],\n",
    "    \"chunk_motif_type_relative_proportion\" :[],\n",
    "    \"chunk_event_lengths\":[],\n",
    "    \"motif_event_lenghts\":[],\n",
    "    #2\n",
    "    # \"chunk_binned_rate\": [],\n",
    "    # \"chunk_bins_relative_so\": [],\n",
    "    # #3\n",
    "    # \"chunk_event_lens\": [],\n",
    "    # #4\n",
    "    # \"coactive_freqs_chunk\": {},\n",
    "    # \"chunk_total_nontask_task_related_events\": [],\n",
    "    # \"total_events\": 0,\n",
    "    # \"chunk_task_num_spikes\": [],\n",
    "    # \"chunk_nontask_num_spikes\": [],\n",
    "    # \"chunk_task_e_len\": [],\n",
    "    # \"chunk_nontask_e_len\": [],\n",
    "    # #5\n",
    "    # \"chunk_summed_amounts\": [],\n",
    "    # \"chunk_ordered_sum\": 0,\n",
    "    # \"chunk_coactive_total\": 0,\n",
    "    # \"task_related\": 0,\n",
    "    # \"non_task_related\": 0\n",
    "    }\n",
    "    return chunk_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in sleep ppseq data - the replay data\n",
    "\n",
    "# replay rate \n",
    "# replay length \n",
    "# motif rate\n",
    "# decay rate? \n",
    "\n",
    "\n",
    "## then run this for a load of mice, make a new file for the plots\n",
    "\n",
    "\n",
    "## for the osciallation analysis, look into the LFP preprocess file...make sure it makes sense in terms of the alignment. \n",
    "## do some lfp processing, then start the osciallation analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this filtering gives...\n",
      " - only sequential events\n",
      "and only those which are in\n",
      " - nrem\n",
      " - rem\n"
     ]
    }
   ],
   "source": [
    "# seq filter takes presedence, if its on: only sequential events, if it is off: all events \n",
    "sequential_filter = True\n",
    "## master switch - turns all sleep filters on/off (if you want all evets turn this off)\n",
    "sleep_filters_on = True\n",
    "# these filters refer to seq one above, and both can be true at the same time. \n",
    "nrem_filter = True\n",
    "rem_filter = True\n",
    "# set this as true (along with the sleep filter one) to override the other two an djust take the background \n",
    "background_only = False\n",
    "\n",
    "\n",
    "## sanity checker / set save path:\n",
    "print('this filtering gives...')\n",
    "if sequential_filter == True:\n",
    "    print(' - only sequential events')\n",
    "    save_var = 'sequential_no_sleep_selected'\n",
    "    type_var = 'sequential'\n",
    "else:\n",
    "    print('- all events')\n",
    "    save_var = 'all_events_no_sleep_selected'\n",
    "    type_var = 'all_events'\n",
    "if sleep_filters_on == True:\n",
    "    if not background_only:\n",
    "        print('and only those which are in')\n",
    "        if nrem_filter == True:\n",
    "            print(' - nrem')\n",
    "            save_var = type_var+'_NREM_sleep'\n",
    "        if rem_filter == True:\n",
    "            print(' - rem')\n",
    "            save_var = type_var+'_REM_sleep'\n",
    "        if nrem_filter == True and rem_filter == True:\n",
    "            save_var = type_var+'_NREM_and_REM_sleep'\n",
    "        \n",
    "    else:\n",
    "        print('and only those which are not in rem/nrem')\n",
    "        save_var = type_var + '_OTHER_nonsleep'\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run index: 3, processing 149_1_1\n",
      "run index: 7, processing 178_1_7\n"
     ]
    }
   ],
   "source": [
    "sleep_ppseq_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\paper_submission\\post_sleep\\\\\"\n",
    "out_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\behaviour_to_replay\\processed_data\\\\\" + save_var + '\\\\'\n",
    "\n",
    "useable_mirs = ['178_1_7','149_1_1']\n",
    "\n",
    "# load in sleep time points\n",
    "sleep_time_point_df = pd.read_csv(sleep_ppseq_path + 'sleep_time_points.csv')\n",
    "# decide when sleep started\n",
    "sleep_start = {}\n",
    "for index,value in enumerate(sleep_time_point_df.approx_sleep_start.values):\n",
    "    mouse = sleep_time_point_df.mir.values[index]\n",
    "    sleep_start[mouse] = value\n",
    "# define mice/sessions in each group    \n",
    "expert_mice = sleep_time_point_df[sleep_time_point_df.group == 'expert'].mir.values\n",
    "hlesion_mice = sleep_time_point_df[sleep_time_point_df.group == 'h_lesion'].mir.values\n",
    "learning_mice = sleep_time_point_df[sleep_time_point_df.group == 'learning'].mir.values\n",
    "\n",
    "# ## set up empty vars \n",
    "# mirs = []\n",
    "\n",
    "var_dict = {'expert':[],'hlesion':[],'learning' :[],'mirs':[],'current_sleep_start':[], 'time_spans':[]}\n",
    "\n",
    "# # 1\n",
    "# reactivations_per_min = []\n",
    "# # 2\n",
    "# event_rate_binned = []\n",
    "# er_bins_relative_to_so = []\n",
    "# # 3\n",
    "# event_lens = []\n",
    "# # 4\n",
    "# av_coactive_len_per_chunk = []\n",
    "\n",
    "# e_coactive_freqs_counts = {}\n",
    "# hl_coactive_freqs_counts = {}\n",
    "# l_coactive_freqs_counts = {}\n",
    "\n",
    "# all_total_events = []\n",
    "# rel_task_nontask = []\n",
    "# chunks_task_nontask = []\n",
    "\n",
    "# task_nontask_num_spikes = []\n",
    "# task_nontask_e_len = []\n",
    "\n",
    "# chunk_expert = []\n",
    "# chunk_mid_time_post_onset = []\n",
    "# #5 \n",
    "# mouse_summed_amounts = []\n",
    "# ordered_sum = []\n",
    "# ordered_misordered_total = []\n",
    "\n",
    "\n",
    "# warps = []\n",
    "\n",
    "\n",
    "# get all the relevant path name and some other data \n",
    "current_mouse_path,var_dict = find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start)\n",
    "# loop across each mouse path:\n",
    "for index, path in enumerate(current_mouse_path):\n",
    "    # create empty chunk vars dict\n",
    "    chunk_vars = empty_chunk_vars()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk1_8300to9300\n",
      "577\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## loop across all chunk files\n",
    "for file in os.listdir(path):\n",
    "    if 'chunk' in file:\n",
    "        print(file)\n",
    "        path_ = path + '\\\\' + file + '\\\\'\n",
    "        chunk_time = np.load(path_ + 'chunk_time_interval.npy')\n",
    "        data = pd.read_csv(path_ + 'filtered_replay_clusters_df.csv')\n",
    "        \n",
    "        # filter based on the sequential/rem-nrem conditions set above\n",
    "        filter_mask = make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only)\n",
    "        filtered_chunk_data = data[filter_mask].reset_index()\n",
    "        \n",
    "        # how many reactivations found\n",
    "        reactivations_found = len(filtered_chunk_data)\n",
    "        print(reactivations_found)\n",
    "        \n",
    "        ####################################### chunk rate per minute: (# this one depends on rem/nrem filter... )\n",
    "        mins = determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path_)\n",
    "        if mins > 0:\n",
    "            chunk_vars['chunk_rpm'] += [reactivations_found/mins]    \n",
    "            \n",
    "        ####################################### replay rate per motif type\n",
    "        all_motif_type_reactivations = []\n",
    "        all_motif_type_reactivations_min = []\n",
    "        all_motif_type_relative_proportion = []\n",
    "        for seq_type in range(1,7):\n",
    "            motif_type_reactivations = [len(np.where(filtered_chunk_data.cluster_seq_type.values == seq_type)[0])][0]\n",
    "            motif_type_reactivations_min = (motif_type_reactivations / mins)\n",
    "            relative_motif_proportion = motif_type_reactivations / len(filtered_chunk_data.cluster_seq_type.values)\n",
    "            all_motif_type_reactivations += [motif_type_reactivations]\n",
    "            all_motif_type_reactivations_min += [motif_type_reactivations_min]\n",
    "            all_motif_type_relative_proportion += [motif_type_reactivations]\n",
    "        chunk_vars['chunk_motif_type_reactivations'] = all_motif_type_reactivations\n",
    "        chunk_vars['chunk_motif_type_reactivations_min'] = all_motif_type_reactivations_min\n",
    "        chunk_vars['chunk_motif_type_relative_proportion'] = all_motif_type_relative_proportion\n",
    "        \n",
    "        ########################################### replay length overall \n",
    "        chunk_vars['chunk_event_lengths'] = filtered_chunk_data.event_length.values\n",
    "        \n",
    "        ########################################### replay length per motif \n",
    "        motif_event_lenghts = []\n",
    "        for i in range(1,7):\n",
    "            motif_event_lenghts += [filtered_chunk_data[filtered_chunk_data.cluster_seq_type == i].event_length.values]\n",
    "        chunk_vars['motif_event_lenghts'] = motif_event_lenghts\n",
    "        \n",
    "        ########################################### coactive rate overall\n",
    "        \n",
    "        \n",
    "                \n",
    "        ########################################### task related vs other rate\n",
    "        # load task related seqs\n",
    "        task_seqs = np.load(path_ + 'task_order_seqs.npy')+1\n",
    "        # mask each condition\n",
    "        mask = np.isin(filtered_chunk_data.cluster_seq_type.values, task_seqs)\n",
    "        opposite_mask = ~mask\n",
    "\n",
    "        task_related = filtered_chunk_data[mask]\n",
    "        non_task_related = filtered_chunk_data[opposite_mask]\n",
    "\n",
    "        # rate\n",
    "        # replay length\n",
    "        # coative rate \n",
    "                \n",
    "\n",
    "        \n",
    "        # motif coactive rate\n",
    "        \n",
    "        # save out to newly made place\n",
    "\n",
    "            \n",
    "\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def relative_dict(input_dict):\n",
    "    total_sum = sum(input_dict.values())\n",
    "    relative_dict = {key: value / total_sum for key, value in input_dict.items()}\n",
    "    return relative_dict\n",
    "\n",
    "def refind_cluster_events(filtered_chunk_data,event_proximity_filter):\n",
    "    \n",
    "    ### ignore the origonal clusterg rosp and remake them: \n",
    "    start_times = filtered_chunk_data.first_spike_time.values\n",
    "    end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "    clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "    cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "    for index,cluster in enumerate(clustered_events):\n",
    "        for item in cluster:\n",
    "            cluster_group[item] = int(index)\n",
    "    filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "    \n",
    "    return filtered_chunk_data\n",
    "\n",
    "########## coactive rate\n",
    "event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "# refind the clusters\n",
    "filtered_chunk_data  = refind_cluster_events(filtered_chunk_data,event_proximity_filter)\n",
    "\n",
    "# how many coactive?\n",
    "\n",
    "# average number in coactive? \n",
    "\n",
    "# ordering of coactive?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# work out how mnay coacitve in chunk: \n",
    "current_coactive_freqs_chunk = {}\n",
    "for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "    num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "    if num in current_coactive_freqs_chunk:\n",
    "        current_coactive_freqs_chunk[num] += 1\n",
    "    else:\n",
    "        current_coactive_freqs_chunk[num] = 1\n",
    "        \n",
    "avs =[]\n",
    "for item in current_coactive_freqs_chunk:\n",
    "    if item > 1:\n",
    "        avs += current_coactive_freqs_chunk[item] * [item]\n",
    "# av_coactive_len_per_chunk += [np.mean(avs)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# make it relative:\n",
    "current_coactive_freqs_chunk = relative_dict(current_coactive_freqs_chunk)\n",
    "\n",
    "coactive_freqs_keys = list(current_coactive_freqs_chunk.keys())\n",
    "rel_coactive_freqs = list(current_coactive_freqs_chunk.values())\n",
    "for index,item in enumerate(rel_coactive_freqs):\n",
    "    num = int(coactive_freqs_keys[index])\n",
    "    if num in coactive_freqs_chunk:\n",
    "        coactive_freqs_chunk[num] += [item]\n",
    "    else:\n",
    "        coactive_freqs_chunk[num] = [item]\n",
    "\n",
    "\n",
    "task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "\n",
    "chunk_task_num_spikes+=list(task_events.num_spikes)\n",
    "chunk_nontask_num_spikes+=list(non_task_events.num_spikes)\n",
    "chunk_task_e_len+=list(task_events.event_length)\n",
    "chunk_nontask_e_len+=list(non_task_events.event_length)\n",
    "\n",
    "\n",
    "# 5 ##############################################################################\n",
    "\n",
    "############################################## split into multi clusters and process\n",
    "\n",
    "multi_cluster_df = pd.DataFrame({'cluster_seq_type':[],\n",
    "    'num_spikes':[],\n",
    "    'num_neurons':[],\n",
    "    'first_spike_time':[],\n",
    "    'event_length':[],\n",
    "    'last_spike_time':[],\n",
    "    'cluster_spike_times':[],\n",
    "    'cluster_neurons':[],\n",
    "    'spike_plotting_order':[],\n",
    "    'coactive_cluster_group':[],\n",
    "    'new_cluster_group':[],\n",
    "    'cluster_order_first_spike_defined':[],\n",
    "    'cluster_order_mean_weighted_spikes_defined':[],\n",
    "    'pairs_mean_ordering':[],\n",
    "    'catagories_mean_ordering':[],\n",
    "    'pairs_fs_ordering':[],\n",
    "    'catagories_fs_ordering':[],\n",
    "    'real_sequence_order':[]})\n",
    "meaned_order = []\n",
    "fs_order = []\n",
    "event_times = []\n",
    "multi_cluster_df\n",
    "count = 0\n",
    "for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "    group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "    current_cluster = filtered_chunk_data[group_mask]\n",
    "    if len(current_cluster) > 1:\n",
    "        means = []\n",
    "        event_types = []\n",
    "        fs_orders = []\n",
    "        for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "            event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "            # calculate event order based on spike time weighted mean\n",
    "            means += [np.mean(ast.literal_eval(events))]\n",
    "            # calculate order based on first spike time:\n",
    "            fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "        # order by mean time:    \n",
    "        meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "        # order by first spike:\n",
    "        fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "        event_times += [fs_orders]\n",
    "\n",
    "        current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "        current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "        current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "        if count == 0:\n",
    "            multi_cluster_df = current_cluster.copy()\n",
    "        else:\n",
    "            # Concatenate the DataFrames vertically (row-wise)\n",
    "            multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "            # Reset the index if needed\n",
    "            multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "############################################## Load in seq order data \n",
    "\n",
    "awake_PP_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\\"\n",
    "\n",
    "for index_,M_I_R in enumerate(os.listdir(awake_PP_path)):\n",
    "    if not M_I_R == 'not_suitable':\n",
    "        mir = '_'.join(M_I_R.split('_')[0:3])\n",
    "        if mir == mouse:\n",
    "            c_path = awake_PP_path + M_I_R + r\"\\analysis_output\\reordered_recolored\\\\\" \n",
    "\n",
    "sequence_order_df = pd.read_csv(awake_PP_path+\"sequence_order.csv\")\n",
    "\n",
    "import ast\n",
    "seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == mouse].seq_order.values[0])\n",
    "num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == mouse].dominant_task_seqs)\n",
    "\n",
    "############################################## calculate catagory breakdown\n",
    "\n",
    "if len(multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "\n",
    "    real_order = list(np.array(seq_order)+1)\n",
    "\n",
    "    # # mean ordering first : \n",
    "    if len(real_order) > 3: # 3 will always be ordered so exclude\n",
    "        relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "        summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "    #     labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "    #     fig, ax = plt.subplots()\n",
    "    #     ax.bar(labels,summed_amounts)\n",
    "    #     ax.set_title('catagory occurances (seqs ordered by mean spike time)')\n",
    "\n",
    "    #     SaveFig('catagory occurances_1___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "        all_pair_outcomes_todf = []\n",
    "        all_pairs_todf = []\n",
    "        for group in multi_cluster_df.new_cluster_group.unique():\n",
    "            group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "            group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "            all_pairs = []\n",
    "            all_pair_outcomes = []\n",
    "            for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                all_pairs += [pair_]\n",
    "                all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "            all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "            all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "        multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "        multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "        multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "        chunk_summed_amounts += [list(np.array(summed_amounts)/sum(summed_amounts))]\n",
    "\n",
    "        chunk_ordered_sum += sum(summed_amounts[0:3])\n",
    "        chunk_coactive_total += sum(summed_amounts[0:4])\n",
    "    else:\n",
    "        print('only 3 seqs')\n",
    "\n",
    "    \n",
    "    \n",
    "#                             print(chunk_summed_amounts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>78.9772</td>\n",
       "      <td>78.9622</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>78.9772</td>\n",
       "      <td>[78.9769, 78.9622, 78.9755, 78.9757, 78.977, 7...</td>\n",
       "      <td>[79.0, 149.0, 163.0, 163.0, 206.0, 208.0, 208.0]</td>\n",
       "      <td>[147. 129.  72.  72. 145. 148. 148.]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>93.5430</td>\n",
       "      <td>93.4474</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>93.5430</td>\n",
       "      <td>[93.4474, 93.4562, 93.4626, 93.4476, 93.4564, ...</td>\n",
       "      <td>[40.0, 40.0, 40.0, 42.0, 42.0, 42.0, 48.0, 48....</td>\n",
       "      <td>[140. 140. 140. 143. 143. 143. 142. 142. 115. ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>114.2586</td>\n",
       "      <td>114.2036</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>114.2586</td>\n",
       "      <td>[114.2374, 114.2375, 114.2511, 114.2512, 114.2...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 206.0, 210.0]</td>\n",
       "      <td>[138. 138. 138. 138. 138. 145. 146.]</td>\n",
       "      <td>15.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>115.0579</td>\n",
       "      <td>115.0205</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>115.0579</td>\n",
       "      <td>[115.0206, 115.0207, 115.0207, 115.0208, 115.0...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 149.0, 149...</td>\n",
       "      <td>[138. 138. 138. 138. 138. 137. 137. 129. 129. ...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>119.7683</td>\n",
       "      <td>119.7589</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>119.7683</td>\n",
       "      <td>[119.7589, 119.768, 119.7682, 119.7683, 119.7679]</td>\n",
       "      <td>[21.0, 40.0, 42.0, 45.0, 48.0]</td>\n",
       "      <td>[116. 140. 143. 139. 142.]</td>\n",
       "      <td>18.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1911</td>\n",
       "      <td>1911</td>\n",
       "      <td>1911</td>\n",
       "      <td>1911</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>962.7765</td>\n",
       "      <td>962.7655</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>962.7765</td>\n",
       "      <td>[962.7655, 962.7667, 962.7715, 962.7764, 962.7...</td>\n",
       "      <td>[15.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 148...</td>\n",
       "      <td>[ 96. 105. 105. 105. 106. 106. 106. 182. 182.]</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>968.9051</td>\n",
       "      <td>968.8036</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>968.9051</td>\n",
       "      <td>[968.8868, 968.8956, 968.8036, 968.8827, 968.8...</td>\n",
       "      <td>[15.0, 15.0, 101.0, 170.0, 170.0, 170.0, 170.0...</td>\n",
       "      <td>[ 96.  96. 117.  97.  97.  97.  97. 242. 242.]</td>\n",
       "      <td>978.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1914</td>\n",
       "      <td>1914</td>\n",
       "      <td>1914</td>\n",
       "      <td>1914</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>969.7686</td>\n",
       "      <td>969.7467</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>969.7686</td>\n",
       "      <td>[969.7686, 969.7686, 969.7583, 969.7526, 969.7...</td>\n",
       "      <td>[68.0, 70.0, 200.0, 204.0, 212.0]</td>\n",
       "      <td>[100.  99. 101. 248. 111.]</td>\n",
       "      <td>980.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1916</td>\n",
       "      <td>1916</td>\n",
       "      <td>1916</td>\n",
       "      <td>1916</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>982.2133</td>\n",
       "      <td>982.1402</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>982.2133</td>\n",
       "      <td>[982.2066, 982.2067, 982.2069, 982.1887, 982.1...</td>\n",
       "      <td>[8.0, 10.0, 10.0, 62.0, 68.0, 70.0, 82.0, 82.0...</td>\n",
       "      <td>[212. 213. 213. 175. 100.  99. 108. 108. 108. ...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>994.5238</td>\n",
       "      <td>994.5073</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>994.5238</td>\n",
       "      <td>[994.5198, 994.52, 994.5202, 994.511, 994.5173...</td>\n",
       "      <td>[8.0, 10.0, 10.0, 24.0, 24.0, 58.0, 70.0, 86.0...</td>\n",
       "      <td>[212. 213. 213.  95.  95.  13.  99. 185. 185. ...</td>\n",
       "      <td>133.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  cluster_seq_type  \\\n",
       "0        9             9             9           9                 1   \n",
       "1       10            10            10          10                 1   \n",
       "2       17            17            17          17                 1   \n",
       "3       18            18            18          18                 1   \n",
       "4       21            21            21          21                 1   \n",
       "..     ...           ...           ...         ...               ...   \n",
       "572   1911          1911          1911        1911                 6   \n",
       "573   1913          1913          1913        1913                 6   \n",
       "574   1914          1914          1914        1914                 6   \n",
       "575   1916          1916          1916        1916                 6   \n",
       "576   1918          1918          1918        1918                 6   \n",
       "\n",
       "     num_spikes  num_neurons  first_spike_time  event_length  last_spike_time  \\\n",
       "0             7      78.9772           78.9622        0.0150          78.9772   \n",
       "1            15      93.5430           93.4474        0.0956          93.5430   \n",
       "2             7     114.2586          114.2036        0.0550         114.2586   \n",
       "3            11     115.0579          115.0205        0.0374         115.0579   \n",
       "4             5     119.7683          119.7589        0.0094         119.7683   \n",
       "..          ...          ...               ...           ...              ...   \n",
       "572           9     962.7765          962.7655        0.0110         962.7765   \n",
       "573           9     968.9051          968.8036        0.1015         968.9051   \n",
       "574           5     969.7686          969.7467        0.0219         969.7686   \n",
       "575          20     982.2133          982.1402        0.0731         982.2133   \n",
       "576          18     994.5238          994.5073        0.0165         994.5238   \n",
       "\n",
       "                                   cluster_spike_times  \\\n",
       "0    [78.9769, 78.9622, 78.9755, 78.9757, 78.977, 7...   \n",
       "1    [93.4474, 93.4562, 93.4626, 93.4476, 93.4564, ...   \n",
       "2    [114.2374, 114.2375, 114.2511, 114.2512, 114.2...   \n",
       "3    [115.0206, 115.0207, 115.0207, 115.0208, 115.0...   \n",
       "4    [119.7589, 119.768, 119.7682, 119.7683, 119.7679]   \n",
       "..                                                 ...   \n",
       "572  [962.7655, 962.7667, 962.7715, 962.7764, 962.7...   \n",
       "573  [968.8868, 968.8956, 968.8036, 968.8827, 968.8...   \n",
       "574  [969.7686, 969.7686, 969.7583, 969.7526, 969.7...   \n",
       "575  [982.2066, 982.2067, 982.2069, 982.1887, 982.1...   \n",
       "576  [994.5198, 994.52, 994.5202, 994.511, 994.5173...   \n",
       "\n",
       "                                       cluster_neurons  \\\n",
       "0     [79.0, 149.0, 163.0, 163.0, 206.0, 208.0, 208.0]   \n",
       "1    [40.0, 40.0, 40.0, 42.0, 42.0, 42.0, 48.0, 48....   \n",
       "2              [6.0, 6.0, 6.0, 6.0, 6.0, 206.0, 210.0]   \n",
       "3    [6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 149.0, 149...   \n",
       "4                       [21.0, 40.0, 42.0, 45.0, 48.0]   \n",
       "..                                                 ...   \n",
       "572  [15.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 148...   \n",
       "573  [15.0, 15.0, 101.0, 170.0, 170.0, 170.0, 170.0...   \n",
       "574                  [68.0, 70.0, 200.0, 204.0, 212.0]   \n",
       "575  [8.0, 10.0, 10.0, 62.0, 68.0, 70.0, 82.0, 82.0...   \n",
       "576  [8.0, 10.0, 10.0, 24.0, 24.0, 58.0, 70.0, 86.0...   \n",
       "\n",
       "                                  spike_plotting_order  \\\n",
       "0                 [147. 129.  72.  72. 145. 148. 148.]   \n",
       "1    [140. 140. 140. 143. 143. 143. 142. 142. 115. ...   \n",
       "2                 [138. 138. 138. 138. 138. 145. 146.]   \n",
       "3    [138. 138. 138. 138. 138. 137. 137. 129. 129. ...   \n",
       "4                           [116. 140. 143. 139. 142.]   \n",
       "..                                                 ...   \n",
       "572     [ 96. 105. 105. 105. 106. 106. 106. 182. 182.]   \n",
       "573     [ 96.  96. 117.  97.  97.  97.  97. 242. 242.]   \n",
       "574                         [100.  99. 101. 248. 111.]   \n",
       "575  [212. 213. 213. 175. 100.  99. 108. 108. 108. ...   \n",
       "576  [212. 213. 213.  95.  95.  13.  99. 185. 185. ...   \n",
       "\n",
       "     coactive_cluster_group ordering_classification  rem_events  nrem_events  \n",
       "0                       8.0              sequential           0            1  \n",
       "1                       9.0              sequential           0            1  \n",
       "2                      15.0              sequential           0            1  \n",
       "3                      16.0              sequential           0            1  \n",
       "4                      18.0              sequential           0            1  \n",
       "..                      ...                     ...         ...          ...  \n",
       "572                  1078.0              sequential           0            1  \n",
       "573                   978.0              sequential           0            1  \n",
       "574                   980.0              sequential           0            1  \n",
       "575                   128.0              sequential           0            1  \n",
       "576                   133.0              sequential           0            1  \n",
       "\n",
       "[577 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                        #2################################\n",
    "\n",
    "#                             current_sleep_start = sleep_start[mouse] - 400\n",
    "                            chunk_number = int(file.split('_')[0][-1])\n",
    "                            start_offset = ([0]+list(np.cumsum(np.diff(time_spans))))[chunk_number-1]\n",
    "\n",
    "\n",
    "                            # take away cumulative chunk offset - this gives time in terms of chunk\n",
    "                            f_spike_times = filtered_chunk_data.first_spike_time.values - start_offset\n",
    "                            # add on ephys time that chunk started - so its in ephys timestamps \n",
    "                            f_spike_times = f_spike_times + chunk_time[0]\n",
    "\n",
    "                            # now make relative to sleep start time\n",
    "                            f_spike_times_relative_to_so = f_spike_times - current_sleep_start \n",
    "                            # do the same but for rem and nrem start\n",
    "\n",
    "                            # filter out anything that happened before sleep onset\n",
    "                            f_spike_times_relative_to_so = f_spike_times_relative_to_so[f_spike_times_relative_to_so > 0]\n",
    "\n",
    "                            ## calculate rate over time:\n",
    "                            time_data = pd.Series(f_spike_times_relative_to_so)\n",
    "                            if len(time_data) > 0:\n",
    "#                                 # Calculate the number of bins required # 5 minute bins\n",
    "#                                 num_bins = int((time_data.max() - time_data.min()) // 40 + 1)\n",
    "#                                 # Create bins and count the occurrences in each bin\n",
    "#                                 chunk_event_rate, chunk_relative_time_bins = np.histogram(time_data, bins=num_bins)\n",
    "#                                 #remove extra final bin and convert to mins\n",
    "#                                 chunk_relative_time_bins = chunk_relative_time_bins[0:-1]/60\n",
    "\n",
    "                                # Calculate the number of bins required # 20s bins\n",
    "                            #     num_bins = int((time_data.max() - time_data.min()) // 40 + 1)\n",
    "                                if time_data.max() - time_data.min() > 19:\n",
    "                                    num_bins = int((time_data.max() - time_data.min())//20)\n",
    "                                    # Create bins and count the occurrences in each bin\n",
    "                                    chunk_event_rate, chunk_relative_time_bins = np.histogram(time_data, bins=num_bins)\n",
    "                                    #remove extra final bin and convert to mins\n",
    "                                    chunk_relative_time_bins = chunk_relative_time_bins[0:-1]/60\n",
    "\n",
    "\n",
    "                                    chunk_binned_rate += [list((chunk_event_rate*3).astype(float))] # *3 because its per 20s so we want it per minute )\n",
    "                                    chunk_bins_relative_so += [list(chunk_relative_time_bins.astype(float))]\n",
    "\n",
    "                        \n",
    "                        #3########################################################\n",
    "\n",
    "                        chunk_event_lens += list(filtered_chunk_data.event_length.values)\n",
    "\n",
    "                        #4 ################################################# coactive stuff -300ms = coactive\n",
    "                        event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "\n",
    "                        task_seqs = np.load(current_data_path + 'task_order_seqs.npy')+1\n",
    "            \n",
    "                        for motif_type in filtered_chunk_data.cluster_seq_type:\n",
    "                            if motif_type in task_seqs:\n",
    "                                task_related += 1\n",
    "                            else:\n",
    "                                non_task_related += 1\n",
    "        \n",
    "                        total_events += len(filtered_chunk_data.cluster_seq_type)\n",
    "\n",
    "                        # normalise by number of each type: \n",
    "#                         if (6-len(task_seqs)) == 0:\n",
    "#                             chunk_total_nontask_task_related_events += [[non_task_related,(task_related/len(task_seqs))]]\n",
    "#                         else:\n",
    "#                             chunk_total_nontask_task_related_events += [[non_task_related/(6-len(task_seqs)),(task_related/len(task_seqs))]]\n",
    "\n",
    "                        chunk_mid_time_post_onset += [((sum(chunk_time)/2)-current_sleep_start)]\n",
    "\n",
    "                        ### ignore the origonal clusterg rosp and remake them: \n",
    "                        start_times = filtered_chunk_data.first_spike_time.values\n",
    "                        end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "                        clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "                        cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "                        for index,cluster in enumerate(clustered_events):\n",
    "                            for item in cluster:\n",
    "                                cluster_group[item] = int(index)\n",
    "                        filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "\n",
    "                        # work out how mnay coacitve in chunk: \n",
    "                        current_coactive_freqs_chunk = {}\n",
    "                        for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "                            num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "                            if num in current_coactive_freqs_chunk:\n",
    "                                current_coactive_freqs_chunk[num] += 1\n",
    "                            else:\n",
    "                                current_coactive_freqs_chunk[num] = 1\n",
    "\n",
    "                        avs =[]\n",
    "                        for item in current_coactive_freqs_chunk:\n",
    "                            avs += current_coactive_freqs_chunk[item] * [item]\n",
    "                        av_coactive_len_per_chunk += [np.mean(avs)]\n",
    "                        if mouse in expert_mice:\n",
    "                            chunk_expert += [1]\n",
    "                        elif mouse in hlesion_mice:\n",
    "                            chunk_expert += [2]\n",
    "                        elif mouse in learning_mice:\n",
    "                            chunk_expert += [3]\n",
    "\n",
    "\n",
    "                        # make it relative:\n",
    "                        current_coactive_freqs_chunk = relative_dict(current_coactive_freqs_chunk)\n",
    "\n",
    "                        coactive_freqs_keys = list(current_coactive_freqs_chunk.keys())\n",
    "                        rel_coactive_freqs = list(current_coactive_freqs_chunk.values())\n",
    "                        for index,item in enumerate(rel_coactive_freqs):\n",
    "                            num = int(coactive_freqs_keys[index])\n",
    "                            if num in coactive_freqs_chunk:\n",
    "                                coactive_freqs_chunk[num] += [item]\n",
    "                            else:\n",
    "                                coactive_freqs_chunk[num] = [item]\n",
    "\n",
    "\n",
    "                        task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "                        non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "\n",
    "                        chunk_task_num_spikes+=list(task_events.num_spikes)\n",
    "                        chunk_nontask_num_spikes+=list(non_task_events.num_spikes)\n",
    "                        chunk_task_e_len+=list(task_events.event_length)\n",
    "                        chunk_nontask_e_len+=list(non_task_events.event_length)\n",
    "\n",
    "\n",
    "                        # 5 ##############################################################################\n",
    "\n",
    "                        ############################################## split into multi clusters and process\n",
    "\n",
    "                        multi_cluster_df = pd.DataFrame({'cluster_seq_type':[],\n",
    "                         'num_spikes':[],\n",
    "                         'num_neurons':[],\n",
    "                         'first_spike_time':[],\n",
    "                         'event_length':[],\n",
    "                         'last_spike_time':[],\n",
    "                         'cluster_spike_times':[],\n",
    "                         'cluster_neurons':[],\n",
    "                         'spike_plotting_order':[],\n",
    "                         'coactive_cluster_group':[],\n",
    "                         'new_cluster_group':[],\n",
    "                         'cluster_order_first_spike_defined':[],\n",
    "                         'cluster_order_mean_weighted_spikes_defined':[],\n",
    "                         'pairs_mean_ordering':[],\n",
    "                         'catagories_mean_ordering':[],\n",
    "                         'pairs_fs_ordering':[],\n",
    "                         'catagories_fs_ordering':[],\n",
    "                         'real_sequence_order':[]})\n",
    "                        meaned_order = []\n",
    "                        fs_order = []\n",
    "                        event_times = []\n",
    "                        multi_cluster_df\n",
    "                        count = 0\n",
    "                        for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "                            group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "                            current_cluster = filtered_chunk_data[group_mask]\n",
    "                            if len(current_cluster) > 1:\n",
    "                                means = []\n",
    "                                event_types = []\n",
    "                                fs_orders = []\n",
    "                                for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                                    event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                                    # calculate event order based on spike time weighted mean\n",
    "                                    means += [np.mean(ast.literal_eval(events))]\n",
    "                                    # calculate order based on first spike time:\n",
    "                                    fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "                                # order by mean time:    \n",
    "                                meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "                                # order by first spike:\n",
    "                                fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "                                event_times += [fs_orders]\n",
    "\n",
    "                                current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "                                current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "                                current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "                                if count == 0:\n",
    "                                    multi_cluster_df = current_cluster.copy()\n",
    "                                else:\n",
    "                                    # Concatenate the DataFrames vertically (row-wise)\n",
    "                                    multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                                    # Reset the index if needed\n",
    "                                    multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "                                count += 1\n",
    "\n",
    "                        ############################################## Load in seq order data \n",
    "\n",
    "                        awake_PP_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\\"\n",
    "\n",
    "                        for index_,M_I_R in enumerate(os.listdir(awake_PP_path)):\n",
    "                            if not M_I_R == 'not_suitable':\n",
    "                                mir = '_'.join(M_I_R.split('_')[0:3])\n",
    "                                if mir == mouse:\n",
    "                                    c_path = awake_PP_path + M_I_R + r\"\\analysis_output\\reordered_recolored\\\\\" \n",
    "\n",
    "                        sequence_order_df = pd.read_csv(awake_PP_path+\"sequence_order.csv\")\n",
    "\n",
    "                        import ast\n",
    "                        seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == mouse].seq_order.values[0])\n",
    "                        num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == mouse].dominant_task_seqs)\n",
    "\n",
    "                        ############################################## calculate catagory breakdown\n",
    "\n",
    "                        if len(multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "\n",
    "                            real_order = list(np.array(seq_order)+1)\n",
    "\n",
    "                            # # mean ordering first : \n",
    "                            if len(real_order) > 3: # 3 will always be ordered so exclude\n",
    "                                relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "                                summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "                            #     labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "                            #     fig, ax = plt.subplots()\n",
    "                            #     ax.bar(labels,summed_amounts)\n",
    "                            #     ax.set_title('catagory occurances (seqs ordered by mean spike time)')\n",
    "\n",
    "                            #     SaveFig('catagory occurances_1___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                                all_pair_outcomes_todf = []\n",
    "                                all_pairs_todf = []\n",
    "                                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                                    group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                                    group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                                    all_pairs = []\n",
    "                                    all_pair_outcomes = []\n",
    "                                    for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                                        all_pairs += [pair_]\n",
    "                                        all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "                                    all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "                                    all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "                                multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "                                multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "                                multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "                                chunk_summed_amounts += [list(np.array(summed_amounts)/sum(summed_amounts))]\n",
    "\n",
    "                                chunk_ordered_sum += sum(summed_amounts[0:3])\n",
    "                                chunk_coactive_total += sum(summed_amounts[0:4])\n",
    "                            else:\n",
    "                                print('only 3 seqs')\n",
    "\n",
    "                            \n",
    "                            \n",
    "#                             print(chunk_summed_amounts)\n",
    "                            \n",
    "        \n",
    "                # outside of chunk loop ################################################\n",
    "                \n",
    "                # changed how i do this, now task freq is worke dout by adding up instances across all chunks and lookig at the proportion rather than averageing across chunks \n",
    "                if (6-len(task_seqs)) == 0:\n",
    "                    chunk_total_nontask_task_related_events += [[non_task_related,(task_related/len(task_seqs))]]\n",
    "                else:\n",
    "                    chunk_total_nontask_task_related_events += [[non_task_related/(6-len(task_seqs)),(task_related/len(task_seqs))]]      \n",
    "\n",
    "                ### add to animal vars\n",
    "                #1\n",
    "                reactivations_per_min += [np.mean(chunk_rpm)]\n",
    "                if np.mean(chunk_rpm) < 3:\n",
    "                    print('!!!!!')\n",
    "                #2\n",
    "                event_rate_binned +=[chunk_binned_rate]\n",
    "                er_bins_relative_to_so +=[chunk_bins_relative_so]\n",
    "                #3\n",
    "                event_lens += [chunk_event_lens]\n",
    "\n",
    "\n",
    "                #4 #########    \n",
    "                relative = []\n",
    "                totals = [sum(item) for item in chunk_total_nontask_task_related_events]\n",
    "                for i,item in enumerate(chunk_total_nontask_task_related_events):\n",
    "                    relative += [list(np.array(item)/totals[i])]\n",
    "\n",
    "                all_total_events += [total_events]\n",
    "\n",
    "                num_task_order_seqs = len(np.load(current_data_path+ 'task_order_seqs.npy')+1)\n",
    "\n",
    "                rel_task_nontask += [[np.mean(conactinate_nth_items(relative)[1]),np.mean(conactinate_nth_items(relative)[0])]]\n",
    "\n",
    "                chunks_task_nontask += conactinate_nth_items(relative)[1]\n",
    "\n",
    "                for item in coactive_freqs_chunk:\n",
    "                    if mouse in expert_mice:\n",
    "                        if item in e_coactive_freqs_counts:\n",
    "                            e_coactive_freqs_counts[item] += [np.mean(coactive_freqs_chunk[item])]\n",
    "                        else:\n",
    "                            e_coactive_freqs_counts[item] = [np.mean(coactive_freqs_chunk[item])]\n",
    "                    elif mouse in hlesion_mice:\n",
    "                        if item in hl_coactive_freqs_counts:\n",
    "                            hl_coactive_freqs_counts[item] += [np.mean(coactive_freqs_chunk[item])]\n",
    "                        else:\n",
    "                            hl_coactive_freqs_counts[item] = [np.mean(coactive_freqs_chunk[item])]\n",
    "                    elif mouse in learning_mice:\n",
    "                        if item in l_coactive_freqs_counts:\n",
    "                            l_coactive_freqs_counts[item] += [np.mean(coactive_freqs_chunk[item])]\n",
    "                        else:\n",
    "                            l_coactive_freqs_counts[item] = [np.mean(coactive_freqs_chunk[item])]\n",
    "\n",
    "\n",
    "\n",
    "                task_nontask_num_spikes+= [[np.mean(chunk_task_num_spikes),np.mean(chunk_nontask_num_spikes)]]\n",
    "                task_nontask_e_len+= [[np.mean(chunk_task_e_len),np.mean(chunk_nontask_e_len)]]\n",
    "\n",
    "                #5 #############\n",
    "\n",
    "                if len(chunk_summed_amounts) > 0:\n",
    "                    c_summed_amounts = []\n",
    "                    for item in conactinate_nth_items(chunk_summed_amounts):\n",
    "                        c_summed_amounts +=[np.mean(item)]\n",
    "                    mouse_summed_amounts += [c_summed_amounts]\n",
    "                else:\n",
    "                    mouse_summed_amounts += [[]]\n",
    "                    \n",
    "                    \n",
    "                ordered_sum += [chunk_ordered_sum]\n",
    "                ordered_misordered_total += [chunk_coactive_total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'EJT'in mir:\n",
    "    c_mir = mir.split('T')[-1]\n",
    "else:\n",
    "    c_mir = mir\n",
    "full_sleep_path = None\n",
    "for ppsleep_file in os.listdir(sleep_ppseq_path):\n",
    "    if c_mir in ppsleep_file:\n",
    "        'print sleep file found'\n",
    "        full_sleep_path = os.path.join(sleep_ppseq_path,ppsleep_file + '/analysis_output')\n",
    "if full_sleep_path is None:\n",
    "    raise Exception(f\"no sleep file found for {mir}\")\n",
    "\n",
    "os.listdir(full_sleep_path)\n",
    "\n",
    "chunk_paths = []\n",
    "for file in os.listdir(full_sleep_path):\n",
    "    if 'chunk' in file:\n",
    "        chunk_paths += [os.path.join(full_sleep_path,file)]     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
