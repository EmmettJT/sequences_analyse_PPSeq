{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast \n",
    "\n",
    "def assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict):\n",
    "    # if session in one of the groups (and define which)   \n",
    "    if mouse in list(expert_mice) + list(hlesion_mice) + list(learning_mice):\n",
    "        if mouse in expert_mice:\n",
    "            var_dict['expert'] += [1]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [0]               \n",
    "        elif mouse in hlesion_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [1]\n",
    "            var_dict['learning'] += [0]   \n",
    "        elif mouse in learning_mice:                \n",
    "            var_dict['expert'] += [0]\n",
    "            var_dict['hlesion'] += [0]\n",
    "            var_dict['learning'] += [1]   \n",
    "    return var_dict\n",
    "\n",
    "def get_time_span(dat_path,pp_file,mouse):\n",
    "    with open(dat_path + pp_file + r'\\trainingData\\\\' + 'params_' + mouse + '.json', 'r') as file:\n",
    "        params = json.load(file)\n",
    "    time_spans = params['time_span']\n",
    "    return time_spans\n",
    "\n",
    "def find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start):\n",
    "    current_mouse_path = []\n",
    "    for run_index,pp_file in enumerate(os.listdir(sleep_ppseq_path)):\n",
    "        if not 'sleep_time_points' in pp_file:\n",
    "            # current mouse\n",
    "            mouse = '_'.join(pp_file.split('_')[0:3])    \n",
    "\n",
    "            if mouse in useable_mirs:\n",
    "                    #print out progress\n",
    "                    print(f\"run index: {run_index}, processing {mouse}\")\n",
    "                    \n",
    "                    # asign to experimental group in var_dict\n",
    "                    var_dict = assign_to_group(mouse,expert_mice,hlesion_mice,learning_mice,var_dict)\n",
    "\n",
    "                    # load in sleep start time and time span\n",
    "                    var_dict['current_sleep_start'] += sleep_start[mouse]\n",
    "                    var_dict['time_spans'] += get_time_span(sleep_ppseq_path,pp_file,mouse)\n",
    "\n",
    "                    # set path to processed files \n",
    "                    current_mouse_path += [sleep_ppseq_path + pp_file + '\\\\analysis_output\\\\']\n",
    "                    var_dict['mirs'] += [mouse]\n",
    "    return current_mouse_path,var_dict\n",
    "\n",
    "\n",
    "def make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only):\n",
    "    ## filter this data\n",
    "    if sequential_filter == True: \n",
    "        sequential_condition = data.ordering_classification == 'sequential'\n",
    "    else:\n",
    "        sequential_condition = np.array([True]*len(data.ordering_classification))\n",
    "\n",
    "    if sleep_filters_on == True:\n",
    "        if nrem_filter == True: \n",
    "            nrem_condition = data.nrem_events == 1\n",
    "        else:\n",
    "            nrem_condition = np.array([False]*len(data.nrem_events))\n",
    "\n",
    "        if rem_filter == True: \n",
    "            rem_condition = data.rem_events == 1\n",
    "        else:\n",
    "            rem_condition = np.array([False]*len(data.rem_events))\n",
    "\n",
    "        if background_only == True:\n",
    "            rem_condition = data.rem_events == 0\n",
    "            nrem_condition = data.nrem_events == 0\n",
    "\n",
    "    else:\n",
    "        nrem_condition = np.array([True]*len(data))\n",
    "        rem_condition = np.array([True]*len(data))\n",
    "        \n",
    "    # filter is set up so that any true will carry forward \n",
    "    filter_mask = sequential_condition * (nrem_condition + rem_condition)\n",
    "        \n",
    "    return filter_mask\n",
    "\n",
    "def determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path):\n",
    "    # if sleep_filters_on is false, use all chunk time\n",
    "    if sleep_filters_on == False:\n",
    "        mins = np.diff(chunk_time)[0]\n",
    "    else:\n",
    "        # load in state times\n",
    "        rem_state_times = np.load(path + 'rem_state_times.npy')\n",
    "        nrem_state_times = np.load(path + 'nrem_state_times.npy')\n",
    "        if len(rem_state_times) > 0:\n",
    "            tot_rem = sum(np.diff(rem_state_times))[0]\n",
    "        else:\n",
    "            tot_rem = 0\n",
    "        if len(nrem_state_times) > 0:\n",
    "            tot_nrem = sum(np.diff(nrem_state_times))[0]\n",
    "        else:\n",
    "            tot_nrem = 0\n",
    "\n",
    "        # if background then use all non rem and non nrem times\n",
    "        if background_only:\n",
    "            mins = np.diff(chunk_time)[0] - (tot_rem+tot_nrem)\n",
    "        else:\n",
    "            # if both, use both \n",
    "            if nrem_filter == True and rem_filter == True:\n",
    "                mins = tot_rem+tot_nrem\n",
    "            elif nrem_filter == True and rem_filter == False:\n",
    "                mins = tot_nrem\n",
    "            elif nrem_filter == False and rem_filter == True:\n",
    "                mins = tot_rem\n",
    "    # convert to mins            \n",
    "    mins = mins/60\n",
    "    \n",
    "    return mins\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def relative_dict(input_dict):\n",
    "    total_sum = sum(input_dict.values())\n",
    "    relative_dict = {key: value / total_sum for key, value in input_dict.items()}\n",
    "    return relative_dict\n",
    "\n",
    "def refind_cluster_events(filtered_chunk_data,event_proximity_filter):\n",
    "    \n",
    "    ### ignore the origonal clusterg rosp and remake them: \n",
    "    start_times = filtered_chunk_data.first_spike_time.values\n",
    "    end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "    clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "    cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "    for index,cluster in enumerate(clustered_events):\n",
    "        for item in cluster:\n",
    "            cluster_group[item] = int(index)\n",
    "    filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "    \n",
    "    return filtered_chunk_data\n",
    "\n",
    "def coactive_rate(filtered_chunk_data):\n",
    "    # work out how mnay coacitve in chunk: \n",
    "    current_coactive_freqs_chunk = {}\n",
    "    for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "        num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "        if num in current_coactive_freqs_chunk:\n",
    "            current_coactive_freqs_chunk[num] += 1\n",
    "        else:\n",
    "            current_coactive_freqs_chunk[num] = 1\n",
    "            \n",
    "    # proportion of all single events that are cocaitve with at least one other\n",
    "    cocative_total = 0\n",
    "    for item in list(current_coactive_freqs_chunk):\n",
    "        if item > 1:\n",
    "            cocative_total += current_coactive_freqs_chunk[item] * item \n",
    "    proportion_single_events_coacitvely_paired = cocative_total/(cocative_total + current_coactive_freqs_chunk[1])\n",
    "\n",
    "    # av_coactive_length (only coactive, ignore single events)\n",
    "    avs =[]\n",
    "    for item in current_coactive_freqs_chunk:\n",
    "        if item > 1:\n",
    "            avs += current_coactive_freqs_chunk[item] * [item]\n",
    "    av_coactive_len_per_chunk = np.mean(avs)\n",
    "\n",
    "    # proportion of events that are coactive (counting coative evetns as one)\n",
    "    proporiton_of_events_coactive = len(avs) / (len(avs) + current_coactive_freqs_chunk[1])\n",
    "\n",
    "    return proportion_single_events_coacitvely_paired,av_coactive_len_per_chunk,proporiton_of_events_coactive\n",
    "\n",
    "def create_multicluster_dataframe(filtered_chunk_data):\n",
    "    meaned_order = []\n",
    "    fs_order = []\n",
    "    event_times = []\n",
    "    count = 0\n",
    "    for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "        group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "        current_cluster = filtered_chunk_data[group_mask].copy()\n",
    "        if len(current_cluster) > 1:\n",
    "            means = []\n",
    "            event_types = []\n",
    "            fs_orders = []\n",
    "            for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                # calculate event order based on spike time weighted mean\n",
    "                means += [np.mean(ast.literal_eval(events))]\n",
    "                # calculate order based on first spike time:\n",
    "                fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "            # order by mean time:    \n",
    "            meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "            # order by first spike:\n",
    "            fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "            event_times += [fs_orders]\n",
    "\n",
    "            current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "            current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "            current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "            if count == 0:\n",
    "                multi_cluster_df = current_cluster.copy()\n",
    "            else:\n",
    "                # Concatenate the DataFrames vertically (row-wise)\n",
    "                multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                # Reset the index if needed\n",
    "                multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "            count += 1\n",
    "    return multi_cluster_df,meaned_order,fs_order\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_ordering_amounts(meaned_order,dominant,other_):\n",
    "    ordered = 0\n",
    "    misordered = 0\n",
    "    other = 0\n",
    "    for cluster in meaned_order:\n",
    "        for ind,item in enumerate(cluster):\n",
    "            if not ind == len(cluster)-1:\n",
    "                pair = [item,cluster[ind+1]]\n",
    "                outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                    ordered += 1\n",
    "                elif outcome == 'misordered':\n",
    "                    misordered += 1\n",
    "                else:\n",
    "                    other +=1\n",
    "    return ordered,misordered,other\n",
    "\n",
    "def all_motifs_proportion_coactive(multi_cluster_df,all_motif_type_reactivations):\n",
    "    motif_prop_coative = []\n",
    "    for seq_type in range(1,7):\n",
    "        motif_cluster_groups = multi_cluster_df[multi_cluster_df['cluster_seq_type'] == seq_type].new_cluster_group\n",
    "        if not len(motif_cluster_groups) == 0:\n",
    "            coative_motif_events = len(motif_cluster_groups)\n",
    "            all_motif_events = all_motif_type_reactivations[seq_type-1]\n",
    "            motif_prop_coative += [coative_motif_events/all_motif_events]\n",
    "        else:\n",
    "            motif_prop_coative += [0]\n",
    "    return motif_prop_coative\n",
    "\n",
    "def motif_by_motif_ordering(meaned_order,real_order,dominant,other_):\n",
    "\n",
    "    all_motifs_fs_task_related_ordered_prop = []\n",
    "    all_motifs_fs_ordered_proportion_all = []\n",
    "    all_motifs_fs_other_proportion = []\n",
    "\n",
    "    for motif_type in range(1,7):\n",
    "        ordered = 0\n",
    "        misordered = 0\n",
    "        other = 0\n",
    "        \n",
    "        if motif_type in real_order:\n",
    "            for cluster in meaned_order:\n",
    "                for ind,item in enumerate(cluster):\n",
    "                    if not ind == len(cluster)-1:\n",
    "                        pair = [item,cluster[ind+1]]\n",
    "                        if motif_type in pair:\n",
    "                            outcome = logic_machine_for_pair_catagorisation(pair,dominant,other_)\n",
    "                            if outcome in ['ordered', 'repeat', 'reverse']:\n",
    "                                ordered += 1\n",
    "                            elif outcome == 'misordered':\n",
    "                                misordered += 1\n",
    "                            else:\n",
    "                                other +=1  \n",
    "                                \n",
    "            if not (ordered+misordered) == 0:\n",
    "                task_related_ordered_prop = ordered/(ordered+misordered)\n",
    "            else:\n",
    "                task_related_ordered_prop = 0\n",
    "            if not (ordered+misordered+other) == 0:\n",
    "                ordered_proportion_all = ordered/(ordered+misordered+other)\n",
    "                other_proportion = other/(ordered+misordered+other)\n",
    "            else:\n",
    "                ordered_proportion_all = 0\n",
    "                other_proportion = 0\n",
    "        else:\n",
    "            task_related_ordered_prop = 'nan'\n",
    "            ordered_proportion_all = 'nan'\n",
    "            other_proportion = 'nan'\n",
    "        \n",
    "        all_motifs_fs_task_related_ordered_prop += [task_related_ordered_prop]\n",
    "        all_motifs_fs_ordered_proportion_all += [ordered_proportion_all]\n",
    "        all_motifs_fs_other_proportion += [other_proportion]\n",
    "        \n",
    "    return all_motifs_fs_task_related_ordered_prop,all_motifs_fs_ordered_proportion_all,all_motifs_fs_other_proportion\n",
    "\n",
    "def empty_chunk_vars():\n",
    "    ## set chunk vars \n",
    "    chunk_vars = {\"chunk_rpm\": [],           \n",
    "    \"chunk_motif_type_reactivations\" :[],\n",
    "    \"chunk_motif_type_reactivations_min\" :[],\n",
    "    \"chunk_motif_type_relative_proportion\" :[],\n",
    "    \"chunk_event_lengths\":[],\n",
    "    \"motif_event_lenghts\":[],\n",
    "    \"proportion_single_events_coacitvely_paired\":[],\n",
    "    \"av_coactive_len_per_chunk\":[],\n",
    "    \"proporiton_of_events_coactive\":[],\n",
    "    \"meaned_order_task_related_ordered_prop\":[],\n",
    "    \"meaned_order_ordered_proportion_all\":[],\n",
    "    \"meaned_order_other_proportion\":[],\n",
    "    \"fs_order_task_related_ordered_prop\":[],\n",
    "    \"fs_order_ordered_proportion_all\":[],\n",
    "    \"fs_order_other_proportion\":[],\n",
    "    \"all_motifs_proportion_coactive\":[],\n",
    "    \"meaned_ordering_all_motifs_task_related_ordered_prop\":[],\n",
    "    \"meaned_ordering_all_motifs_ordered_proportion_all\":[],\n",
    "    \"meaned_ordering_all_motifs_other_proportion\":[],\n",
    "    \"fs_ordering_all_motifs_task_related_ordered_prop\":[],\n",
    "    \"fs_ordering_all_motifs_ordered_proportion_all\":[],\n",
    "    \"fs_ordering_all_motifs_other_proportion\":[],\n",
    "    \"ratio_task_to_nontask\":[],\n",
    "    \"proportion_coacitve_event_that_are_task_related\":[]\n",
    "    }\n",
    "    return chunk_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in sleep ppseq data - the replay data\n",
    "\n",
    "# replay rate \n",
    "# replay length \n",
    "# motif rate\n",
    "# decay rate? \n",
    "\n",
    "\n",
    "## then run this for a load of mice, make a new file for the plots\n",
    "\n",
    "\n",
    "## for the osciallation analysis, look into the LFP preprocess file...make sure it makes sense in terms of the alignment. \n",
    "## do some lfp processing, then start the osciallation analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay processing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this filtering gives...\n",
      " - only sequential events\n",
      "and only those which are in\n",
      " - nrem\n",
      " - rem\n"
     ]
    }
   ],
   "source": [
    "# seq filter takes presedence, if its on: only sequential events, if it is off: all events \n",
    "sequential_filter = True\n",
    "## master switch - turns all sleep filters on/off (if you want all evets turn this off)\n",
    "sleep_filters_on = True\n",
    "# these filters refer to seq one above, and both can be true at the same time. \n",
    "nrem_filter = True\n",
    "rem_filter = True\n",
    "# set this as true (along with the sleep filter one) to override the other two an djust take the background \n",
    "background_only = False\n",
    "\n",
    "\n",
    "## sanity checker / set save path:\n",
    "print('this filtering gives...')\n",
    "if sequential_filter == True:\n",
    "    print(' - only sequential events')\n",
    "    save_var = 'sequential_no_sleep_selected'\n",
    "    type_var = 'sequential'\n",
    "else:\n",
    "    print('- all events')\n",
    "    save_var = 'all_events_no_sleep_selected'\n",
    "    type_var = 'all_events'\n",
    "if sleep_filters_on == True:\n",
    "    if not background_only:\n",
    "        print('and only those which are in')\n",
    "        if nrem_filter == True:\n",
    "            print(' - nrem')\n",
    "            save_var = type_var+'_NREM_sleep'\n",
    "        if rem_filter == True:\n",
    "            print(' - rem')\n",
    "            save_var = type_var+'_REM_sleep'\n",
    "        if nrem_filter == True and rem_filter == True:\n",
    "            save_var = type_var+'_NREM_and_REM_sleep'\n",
    "        \n",
    "    else:\n",
    "        print('and only those which are not in rem/nrem')\n",
    "        save_var = type_var + '_OTHER_nonsleep'\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run index: 3, processing 149_1_1\n",
      "run index: 7, processing 178_1_7\n"
     ]
    }
   ],
   "source": [
    "sleep_ppseq_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\paper_submission\\post_sleep\\\\\"\n",
    "out_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\sleep_wake_link_data\\behaviour_to_replay\\processed_data\\\\\" + save_var + '\\\\'\n",
    "\n",
    "useable_mirs = ['178_1_7','149_1_1']\n",
    "\n",
    "# load in sleep time points\n",
    "sleep_time_point_df = pd.read_csv(sleep_ppseq_path + 'sleep_time_points.csv')\n",
    "# decide when sleep started\n",
    "sleep_start = {}\n",
    "for index,value in enumerate(sleep_time_point_df.approx_sleep_start.values):\n",
    "    mouse = sleep_time_point_df.mir.values[index]\n",
    "    sleep_start[mouse] = value\n",
    "# define mice/sessions in each group    \n",
    "expert_mice = sleep_time_point_df[sleep_time_point_df.group == 'expert'].mir.values\n",
    "hlesion_mice = sleep_time_point_df[sleep_time_point_df.group == 'h_lesion'].mir.values\n",
    "learning_mice = sleep_time_point_df[sleep_time_point_df.group == 'learning'].mir.values\n",
    "\n",
    "\n",
    "var_dict = {'expert':[],'hlesion':[],'learning' :[],'mirs':[],'current_sleep_start':[], 'time_spans':[]}\n",
    "\n",
    "#Load in seq order data \n",
    "sequence_order_df = pd.read_csv(sleep_ppseq_path+\"sequence_order.csv\")\n",
    "# get all the relevant path name and some other data \n",
    "current_mouse_path,var_dict = find_useable_mouse_paths(sleep_ppseq_path,useable_mirs,expert_mice,hlesion_mice,learning_mice,var_dict,sleep_start)\n",
    "# loop across each mouse path:\n",
    "for loop_index, path in enumerate(current_mouse_path):\n",
    "    # create empty chunk vars dict\n",
    "    chunk_vars = empty_chunk_vars()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk1_8300to9300\n",
      "577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_32904\\2716809469.py:65: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## loop across all chunk files\n",
    "for file in os.listdir(path):\n",
    "    if 'chunk' in file:\n",
    "        print(file)\n",
    "        path_ = path + '\\\\' + file + '\\\\'\n",
    "        chunk_time = np.load(path_ + 'chunk_time_interval.npy')\n",
    "        data = pd.read_csv(path_ + 'filtered_replay_clusters_df.csv')\n",
    "        \n",
    "        # filter based on the sequential/rem-nrem conditions set above\n",
    "        filter_mask = make_filter_masks(data,sequential_filter,nrem_filter,rem_filter,sleep_filters_on,background_only)\n",
    "        filtered_chunk_data = data[filter_mask].reset_index()\n",
    "        \n",
    "        # how many reactivations found\n",
    "        reactivations_found = len(filtered_chunk_data)\n",
    "        print(reactivations_found)\n",
    "        \n",
    "        ####################################### chunk rate per minute: (# this one depends on rem/nrem filter... )\n",
    "        mins = determine_chunk_mins(chunk_time,sleep_filters_on,nrem_filter,rem_filter,background_only,path_)\n",
    "        if mins > 0:\n",
    "            chunk_vars['chunk_rpm'] += [reactivations_found/mins]    \n",
    "            \n",
    "        ####################################### replay rate per motif type\n",
    "        all_motif_type_reactivations = []\n",
    "        all_motif_type_reactivations_min = []\n",
    "        all_motif_type_relative_proportion = []\n",
    "        for seq_type in range(1,7):\n",
    "            motif_type_reactivations = [len(np.where(filtered_chunk_data.cluster_seq_type.values == seq_type)[0])][0]\n",
    "            motif_type_reactivations_min = (motif_type_reactivations / mins)\n",
    "            relative_motif_proportion = motif_type_reactivations / len(filtered_chunk_data.cluster_seq_type.values)\n",
    "            all_motif_type_reactivations += [motif_type_reactivations]\n",
    "            all_motif_type_reactivations_min += [motif_type_reactivations_min]\n",
    "            all_motif_type_relative_proportion += [motif_type_reactivations]\n",
    "        chunk_vars['chunk_motif_type_reactivations'] = all_motif_type_reactivations\n",
    "        chunk_vars['chunk_motif_type_reactivations_min'] = all_motif_type_reactivations_min\n",
    "        chunk_vars['chunk_motif_type_relative_proportion'] = all_motif_type_relative_proportion\n",
    "        \n",
    "        \n",
    "        ##################################### av. spikes involved\n",
    "        chunk_vars['mean_spikes_per_event'] = np.mean([len(item) for item in filtered_chunk_data.cluster_spike_times])\n",
    "        # per motif\n",
    "        motif_by_motif_mean_spikes_per_event = []\n",
    "        for motif_number in range(1,7):\n",
    "            motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "            motif_by_motif_mean_spikes_per_event += [np.mean([len(item) for item in motif_data.cluster_spike_times])]\n",
    "        chunk_vars['motif_by_motif_mean_spikes_per_event'] = motif_by_motif_mean_spikes_per_event        \n",
    "                \n",
    "        ########################################## average units involved \n",
    "        chunk_vars['mean_units_per_event'] = np.mean([len(np.unique(ast.literal_eval(item))) for item in filtered_chunk_data.cluster_neurons])\n",
    "        motif_by_motif_mean_units_per_event = []\n",
    "        for motif_number in range(1,7):\n",
    "            motif_data = filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number]\n",
    "            motif_by_motif_mean_units_per_event += [len(np.unique(ast.literal_eval(item))) for item in motif_data.cluster_neurons]\n",
    "        chunk_vars['motif_by_motif_mean_units_per_event'] = motif_by_motif_mean_units_per_event\n",
    "        \n",
    "        ########################################### replay length overall \n",
    "        chunk_vars['chunk_event_lengths'] = filtered_chunk_data.event_length.values\n",
    "        \n",
    "        ########################################### replay length per motif \n",
    "        motif_event_lenghts = []\n",
    "        for i in range(1,7):\n",
    "            motif_event_lenghts += [filtered_chunk_data[filtered_chunk_data.cluster_seq_type == i].event_length.values]\n",
    "        chunk_vars['motif_event_lenghts'] = motif_event_lenghts\n",
    "        \n",
    "        ########################################### coactive rate overall\n",
    "        event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "        # refind the clusters\n",
    "        filtered_chunk_data  = refind_cluster_events(filtered_chunk_data,event_proximity_filter)\n",
    "        # how many single evtns coactivly paired? average coactive rate? proportion of global events coactive? \n",
    "        proportion_single_events_coacitvely_paired,av_coactive_len_per_chunk,proporiton_of_events_coactive = coactive_rate(filtered_chunk_data)\n",
    "        chunk_vars['proportion_single_events_coacitvely_paired'] = proportion_single_events_coacitvely_paired\n",
    "        chunk_vars['av_coactive_len_per_chunk'] = av_coactive_len_per_chunk\n",
    "        chunk_vars['proporiton_of_events_coactive'] = proporiton_of_events_coactive\n",
    "        \n",
    "        # ordering of coactive?\n",
    "        multi_cluster_df,meaned_order,fs_order = create_multicluster_dataframe(filtered_chunk_data)\n",
    "\n",
    "        # pull out sequence order for current mouse\n",
    "        seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].seq_order.values[0])\n",
    "        num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n",
    "        real_order = np.array(seq_order)+1\n",
    "\n",
    "        #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "        if not len(real_order) == num_dominant_seqs:\n",
    "            dominant = list(real_order[0:num_dominant_seqs])\n",
    "            other_ = list(real_order[num_dominant_seqs::])\n",
    "        else:\n",
    "            dominant = list(real_order)\n",
    "            other_ = []\n",
    "            \n",
    "        # orderng amounts for mean ordering\n",
    "        ordered,misordered,other = calculate_ordering_amounts(meaned_order,dominant,other_)\n",
    "\n",
    "        meaned_order_task_related_ordered_prop = ordered/(ordered+misordered)\n",
    "        meaned_order_ordered_proportion_all = ordered/(ordered+misordered+other)\n",
    "        meaned_order_other_proportion = other/(ordered+misordered+other)\n",
    "        chunk_vars['meaned_order_task_related_ordered_prop'] = meaned_order_task_related_ordered_prop\n",
    "        chunk_vars['meaned_order_ordered_proportion_all'] = meaned_order_ordered_proportion_all\n",
    "        chunk_vars['meaned_order_other_proportion'] = meaned_order_other_proportion\n",
    "        \n",
    "        # orderng amounts for first spike ordering\n",
    "        ordered,misordered,other = calculate_ordering_amounts(fs_order,dominant,other_)\n",
    "\n",
    "        fs_order_task_related_ordered_prop = ordered/(ordered+misordered)\n",
    "        fs_order_ordered_proportion_all = ordered/(ordered+misordered+other)\n",
    "        fs_order_other_proportion = other/(ordered+misordered+other)\n",
    "        chunk_vars['fs_order_task_related_ordered_prop'] = fs_order_task_related_ordered_prop\n",
    "        chunk_vars['fs_order_ordered_proportion_all'] = fs_order_ordered_proportion_all\n",
    "        chunk_vars['fs_order_other_proportion'] = fs_order_other_proportion\n",
    "        \n",
    "        ### motif by motif:\n",
    "        # does one motif appear more in coactive?\n",
    "        all_motifs_prop_coactive = all_motifs_proportion_coactive(multi_cluster_df,all_motif_type_reactivations)\n",
    "        chunk_vars['all_motifs_proportion_coactive'] = all_motifs_prop_coactive\n",
    "        \n",
    "        # does one motif appeaer more ordered? \n",
    "        # # this is only calculated for the task related motifs as the non task dont have an order - thought other catagory still exists for times it was task to non task or other way around \n",
    "        # meaned ordering \n",
    "        all_motifs_task_related_ordered_prop,all_motifs_ordered_proportion_all,all_motifs_ordered_proportion_all = motif_by_motif_ordering(meaned_order,real_order,dominant,other_)\n",
    "        chunk_vars['meaned_ordering_all_motifs_task_related_ordered_prop'] = all_motifs_task_related_ordered_prop\n",
    "        chunk_vars['meaned_ordering_all_motifs_ordered_proportion_all'] = all_motifs_ordered_proportion_all\n",
    "        chunk_vars['meaned_ordering_all_motifs_other_proportion'] = all_motifs_ordered_proportion_all\n",
    "        \n",
    "        # first spike ordering \n",
    "        all_motifs_task_related_ordered_prop,all_motifs_ordered_proportion_all,all_motifs_other_proportion = motif_by_motif_ordering(fs_order,real_order,dominant,other_)\n",
    "        chunk_vars['fs_ordering_all_motifs_task_related_ordered_prop'] = all_motifs_task_related_ordered_prop\n",
    "        chunk_vars['fs_ordering_all_motifs_ordered_proportion_all'] = all_motifs_ordered_proportion_all\n",
    "        chunk_vars['fs_ordering_all_motifs_other_proportion'] = all_motifs_other_proportion\n",
    "\n",
    "        ########################################### task related vs other rate\n",
    "        task_seqs = np.array(seq_order)+1\n",
    "        # mask each condition\n",
    "        mask = np.isin(filtered_chunk_data.cluster_seq_type.values, task_seqs)\n",
    "        opposite_mask = ~mask\n",
    "        task_related = filtered_chunk_data[mask]\n",
    "        non_task_related = filtered_chunk_data[opposite_mask]\n",
    "\n",
    "        #  task v nontask overallrate\n",
    "        ratio_task_to_nontask = (len(task_related)/len(task_seqs))/(len(non_task_related)/(6-len(task_seqs)))\n",
    "        chunk_vars['ratio_task_to_nontask'] = ratio_task_to_nontask\n",
    "        \n",
    "        # same but motif by motif\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # coative rate \n",
    "        task_related_number = 0\n",
    "        non_task_related_number = 0\n",
    "        for coactive_ in meaned_order:\n",
    "            for motif_item in coactive_:\n",
    "                if motif_item in task_seqs:\n",
    "                    task_related_number += 1\n",
    "                else:\n",
    "                    non_task_related_number += 1\n",
    "        # make it relative:\n",
    "        task_related_number = task_related_number/len(task_seqs) \n",
    "        non_task_related_number = non_task_related_number/(6-len(task_seqs))\n",
    "        proportion_coacitve_event_that_are_task_related = task_related_number/(task_related_number+non_task_related_number)\n",
    "        chunk_vars['proportion_coacitve_event_that_are_task_related'] = proportion_coacitve_event_that_are_task_related\n",
    "        \n",
    "        # # motif coactive rate for task and non task \n",
    "        # # task\n",
    "        # task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "        # task_proportion_single_events_coacitvely_paired,task_av_coactive_len_per_chunk,task_proporiton_of_events_coactive = coactive_rate(task_related)\n",
    "        # # non task:\n",
    "        # non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "        # nontask_proportion_single_events_coacitvely_paired,nontask_av_coactive_len_per_chunk,nontask_proporiton_of_events_coactive = coactive_rate(non_task_related)\n",
    "\n",
    "        # replay length\n",
    "        #task v non task\n",
    "        # motif by motif  \n",
    "        # spikes involved\n",
    "        \n",
    "        # save out to newly made place\n",
    "\n",
    "            \n",
    "\n",
    "        break\n",
    "\n",
    "##\n",
    "#now do averages for each chunk and save out to a new file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.418918918918919"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 1 (3017412048.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[363], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    task_related[task_related['cluster_seq_type'] == motif_number]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 1\n"
     ]
    }
   ],
   "source": [
    "for motif_number in range(1,7):\n",
    "task_related[task_related['cluster_seq_type'] == motif_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474    [79.8994, 79.9064, 79.9189, 79.7697, 79.8038, ...\n",
       "475    [87.0442, 87.0612, 87.0612, 87.0681, 87.0444, ...\n",
       "476    [104.1823, 104.1824, 104.2019, 104.2224, 104.2...\n",
       "477    [114.9235, 114.9317, 114.9361, 114.9237, 114.9...\n",
       "478    [115.2708, 115.2872, 115.2708, 115.2709, 115.2...\n",
       "                             ...                        \n",
       "572    [962.7655, 962.7667, 962.7715, 962.7764, 962.7...\n",
       "573    [968.8868, 968.8956, 968.8036, 968.8827, 968.8...\n",
       "574    [969.7686, 969.7686, 969.7583, 969.7526, 969.7...\n",
       "575    [982.2066, 982.2067, 982.2069, 982.1887, 982.1...\n",
       "576    [994.5198, 994.52, 994.5202, 994.511, 994.5173...\n",
       "Name: cluster_spike_times, Length: 103, dtype: object"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_related[task_related['cluster_seq_type'] == motif_number].cluster_spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_related\n",
    "non_task_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.572953736654805"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### av. spikes involved\n",
    "chunk_vars['mean_spikes_per_event'] = np.mean(task_related.num_spikes.values)\n",
    "# per motif\n",
    "motif_by_motif_mean_spikes_per_event = []\n",
    "for motif_number in range(1,7):\n",
    "    motif_by_motif_mean_spikes_per_event +=[np.mean(filtered_chunk_data[filtered_chunk_data['cluster_seq_type'] == motif_number].num_spikes)]\n",
    "chunk_vars['motif_by_motif_mean_spikes_per_event'] = motif_by_motif_mean_spikes_per_event        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.372881355932204,\n",
       " 11.078947368421053,\n",
       " 14.635379061371841,\n",
       " 10.25,\n",
       " 14.206896551724139,\n",
       " 12.524271844660195]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_by_motif_mean_spikes_per_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[343], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mall_motifs_proportion_coactive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_cluster_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_motif_type_reactivations\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3220338983050847,\n",
       " 0.4868421052631579,\n",
       " 0.48736462093862815,\n",
       " 0.5,\n",
       " 0.603448275862069,\n",
       " 0.5145631067961165]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_motifs_proportion_coactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ########################################### replay length overall \n",
    "        chunk_vars['chunk_event_lengths'] = filtered_chunk_data.event_length.values\n",
    "        \n",
    "        ########################################### replay length per motif \n",
    "        motif_event_lenghts = []\n",
    "        for i in range(1,7):\n",
    "            motif_event_lenghts += [filtered_chunk_data[filtered_chunk_data.cluster_seq_type == i].event_length.values]\n",
    "        chunk_vars['motif_event_lenghts'] = motif_event_lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_coacitve_event_that_are_task_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32432432432432434"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_proportion_single_events_coacitvely_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 5, 2])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5124555160142349"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many single evtns coactivly paired? average coactive rate? proportion of global events coactive? \n",
    "proportion_single_events_coacitvely_paired,av_coactive_len_per_chunk,proporiton_of_events_coactive = coactive_rate(task_related)\n",
    "nt_proportion_single_events_coacitvely_paired,nt_av_coactive_len_per_chunk,nt_proporiton_of_events_coactive = coactive_rate(non_task_related)\n",
    "# chunk_vars['proportion_single_events_coacitvely_paired'] = proportion_single_events_coacitvely_paired\n",
    "# chunk_vars['av_coactive_len_per_chunk'] = av_coactive_len_per_chunk\n",
    "# chunk_vars['proporiton_of_events_coactive'] = proporiton_of_events_coactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2704626334519573"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_proportion_single_events_coacitvely_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26013513513513514"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_single_events_coacitvely_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26013513513513514"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_single_events_coacitvely_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.0"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.548682968852532"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many reactivations found\n",
    "reactivations_found = len(task_related)\n",
    "print(reactivations_found)\n",
    "\n",
    "\n",
    "reactivations_found/mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_group in motif_cluster_groups:\n",
    "    if len(np.where(multi_cluster_df.new_cluster_group.values == c_group)[0]) == 1:\n",
    "        print('clust')\n",
    "    else:\n",
    "        print('no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([279, 280], dtype=int64)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(multi_cluster_df.new_cluster_group.values == c_group)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_32904\\2690245044.py:13: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == var_dict['mirs'][loop_index]].dominant_task_seqs)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_related_ordered_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_misrtodered_ratio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.225"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_proportion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_machine_for_pair_catagorisation(pair,dominant,other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_machine_for_pair_catagorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 3],\n",
       " [6, 1, 6],\n",
       " [1, 5, 3],\n",
       " [1, 5, 3],\n",
       " [3, 1],\n",
       " [1, 3, 3],\n",
       " [3, 1, 3],\n",
       " [1, 3],\n",
       " [1, 3],\n",
       " [1, 2, 3],\n",
       " [5, 1, 3, 3],\n",
       " [5, 1],\n",
       " [3, 1],\n",
       " [6, 1],\n",
       " [1, 1, 6],\n",
       " [1, 6],\n",
       " [3, 1, 6],\n",
       " [3, 1],\n",
       " [3, 2, 6],\n",
       " [2, 6],\n",
       " [5, 2],\n",
       " [3, 2],\n",
       " [2, 3],\n",
       " [2, 6, 3, 5],\n",
       " [6, 5, 2],\n",
       " [5, 2],\n",
       " [2, 6],\n",
       " [2, 6],\n",
       " [3, 2],\n",
       " [3, 3, 2],\n",
       " [2, 2],\n",
       " [2, 3],\n",
       " [5, 2],\n",
       " [6, 2],\n",
       " [3, 2],\n",
       " [2, 5],\n",
       " [2, 6],\n",
       " [2, 2],\n",
       " [5, 2, 3, 6, 5, 5],\n",
       " [3, 2, 2, 5, 3],\n",
       " [2, 2],\n",
       " [2, 3, 6],\n",
       " [5, 2],\n",
       " [3, 2],\n",
       " [2, 6, 3],\n",
       " [3, 2],\n",
       " [2, 6],\n",
       " [3, 2],\n",
       " [2, 2],\n",
       " [3, 6],\n",
       " [5, 3, 5],\n",
       " [3, 3],\n",
       " [3, 3, 3],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [3, 6, 3],\n",
       " [3, 5],\n",
       " [3, 6],\n",
       " [3, 3],\n",
       " [6, 3],\n",
       " [3, 3, 6, 6],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [3, 5],\n",
       " [6, 3, 5],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " [3, 3],\n",
       " [6, 3, 3],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [3, 3],\n",
       " [3, 5],\n",
       " [6, 3, 5],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [3, 6],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [6, 3],\n",
       " [3, 3],\n",
       " [6, 3],\n",
       " [5, 3],\n",
       " [3, 3],\n",
       " [6, 3],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " [3, 3],\n",
       " [3, 5],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [5, 3, 5],\n",
       " [3, 6],\n",
       " [3, 6],\n",
       " [3, 3, 6],\n",
       " [3, 6, 3],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [6, 3],\n",
       " [3, 3, 6],\n",
       " [3, 3],\n",
       " [3, 6, 5],\n",
       " [6, 3],\n",
       " [3, 3, 3],\n",
       " [3, 3],\n",
       " [4, 4],\n",
       " [5, 6],\n",
       " [5, 6],\n",
       " [6, 5],\n",
       " [6, 5],\n",
       " [5, 6]]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for cluster in meaned_order:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion ordered (mean)\n",
    "# proportion ordered (first spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len( multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "    real_order = list(np.array(seq_order)+1)\n",
    "    # # mean ordering first : \n",
    "    relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [6, 1],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [1, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [1, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " ['None'],\n",
       " [1, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " [1, 3],\n",
       " ['None'],\n",
       " [1, 3],\n",
       " ['None'],\n",
       " [1, 3],\n",
       " ['None'],\n",
       " [1, 2],\n",
       " [2, 3],\n",
       " ['None'],\n",
       " [5, 1],\n",
       " [1, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [5, 1],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " ['None'],\n",
       " [6, 1],\n",
       " ['None'],\n",
       " [1, 1],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 3],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 5],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [2, 3],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [6, 2],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 5],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " [2, 3],\n",
       " [3, 6],\n",
       " [6, 5],\n",
       " [5, 5],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " [2, 2],\n",
       " [2, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [2, 3],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [5, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [6, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [5, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " [6, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [4, 4],\n",
       " ['None'],\n",
       " [5, 6],\n",
       " ['None'],\n",
       " [5, 6],\n",
       " ['None'],\n",
       " [6, 5],\n",
       " ['None'],\n",
       " [6, 5],\n",
       " ['None'],\n",
       " [5, 6],\n",
       " ['None']]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0],\n",
       " [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0],\n",
       " [0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0],\n",
       " [0.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.3333333333333333,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.0],\n",
       " [0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.0],\n",
       " [0.0, 0.25, 0.25, 0.0, 0.25, 0.25, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0.3333333333333333],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "    all_pair_outcomes_todf = []\n",
    "    all_pairs_todf = []\n",
    "    for group in multi_cluster_df.new_cluster_group.unique():\n",
    "        group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "        group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "        all_pairs = []\n",
    "        all_pair_outcomes = []\n",
    "        for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "            all_pairs += [pair_]\n",
    "            all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "        all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "        all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "    multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "    multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "    # # first spike ordering second : \n",
    "    relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,fs_order)\n",
    "    summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "    labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(labels,summed_amounts)\n",
    "    ax.set_title('catagory occurances (seqs ordered by first spike times)')\n",
    "\n",
    "    SaveFig('catagory occurances_2___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "    all_pair_outcomes_todf = []\n",
    "    all_pairs_todf = []\n",
    "    for group in multi_cluster_df.new_cluster_group.unique():\n",
    "        group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "        group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "        all_pairs = []\n",
    "        all_pair_outcomes = []\n",
    "        for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "            all_pairs += [pair_]\n",
    "            all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "        all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "        all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "    multi_cluster_df['pairs_fs_ordering'] = all_pairs_todf\n",
    "    multi_cluster_df['catagories_fs_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "\n",
    "    multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "    multi_cluster_df.to_csv(chunk_path + 'multi_event_clusters_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (281,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m all_pairs_todf \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m multi_cluster_df\u001b[38;5;241m.\u001b[39mnew_cluster_group\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m---> 21\u001b[0m     group_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m[multi_cluster_df[multi_cluster_df\u001b[38;5;241m.\u001b[39mnew_cluster_group \u001b[38;5;241m==\u001b[39m group]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     22\u001b[0m     group_pair_outcomes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pair_outcomes)[multi_cluster_df[multi_cluster_df\u001b[38;5;241m.\u001b[39mnew_cluster_group \u001b[38;5;241m==\u001b[39m group]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     23\u001b[0m     all_pairs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (281,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################## calculate catagory breakdown\n",
    "\n",
    "if len(multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "\n",
    "    real_order = list(np.array(seq_order)+1)\n",
    "\n",
    "    # # mean ordering first : \n",
    "    if len(real_order) > 3: # 3 will always be ordered so exclude\n",
    "        relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "        summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "    #     labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "    #     fig, ax = plt.subplots()\n",
    "    #     ax.bar(labels,summed_amounts)\n",
    "    #     ax.set_title('catagory occurances (seqs ordered by mean spike time)')\n",
    "\n",
    "    #     SaveFig('catagory occurances_1___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "        all_pair_outcomes_todf = []\n",
    "        all_pairs_todf = []\n",
    "        for group in multi_cluster_df.new_cluster_group.unique():\n",
    "            group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "            group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "            all_pairs = []\n",
    "            all_pair_outcomes = []\n",
    "            for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                all_pairs += [pair_]\n",
    "                all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "            all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "            all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "        multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "        multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "        multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "        # chunk_summed_amounts += [list(np.array(summed_amounts)/sum(summed_amounts))]\n",
    "\n",
    "        # chunk_ordered_sum += sum(summed_amounts[0:3])\n",
    "        # chunk_coactive_total += sum(summed_amounts[0:4])\n",
    "    else:\n",
    "        print('only 3 seqs')\n",
    "\n",
    "    \n",
    "    \n",
    "#                             print(chunk_summed_amounts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (281,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (281,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [6, 1],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [1, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [1, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " ['None'],\n",
       " [1, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " [1, 3],\n",
       " ['None'],\n",
       " [1, 3],\n",
       " ['None'],\n",
       " [1, 3],\n",
       " ['None'],\n",
       " [1, 2],\n",
       " [2, 3],\n",
       " ['None'],\n",
       " [5, 1],\n",
       " [1, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [5, 1],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " ['None'],\n",
       " [6, 1],\n",
       " ['None'],\n",
       " [1, 1],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " [1, 6],\n",
       " ['None'],\n",
       " [3, 1],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 3],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 5],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [2, 3],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [6, 2],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 5],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " [2, 3],\n",
       " [3, 6],\n",
       " [6, 5],\n",
       " [5, 5],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " [2, 2],\n",
       " [2, 5],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [2, 3],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [5, 2],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 6],\n",
       " ['None'],\n",
       " [3, 2],\n",
       " ['None'],\n",
       " [2, 2],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [5, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " [6, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [5, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [5, 3],\n",
       " [3, 5],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 6],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 6],\n",
       " [6, 5],\n",
       " ['None'],\n",
       " [6, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [3, 3],\n",
       " ['None'],\n",
       " [4, 4],\n",
       " ['None'],\n",
       " [5, 6],\n",
       " ['None'],\n",
       " [5, 6],\n",
       " ['None'],\n",
       " [6, 5],\n",
       " ['None'],\n",
       " [6, 5],\n",
       " ['None'],\n",
       " [5, 6],\n",
       " ['None']]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cluster_seq_type</th>\n",
       "      <th>num_spikes</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>first_spike_time</th>\n",
       "      <th>event_length</th>\n",
       "      <th>last_spike_time</th>\n",
       "      <th>cluster_spike_times</th>\n",
       "      <th>cluster_neurons</th>\n",
       "      <th>spike_plotting_order</th>\n",
       "      <th>coactive_cluster_group</th>\n",
       "      <th>ordering_classification</th>\n",
       "      <th>rem_events</th>\n",
       "      <th>nrem_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>78.9772</td>\n",
       "      <td>78.9622</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>78.9772</td>\n",
       "      <td>[78.9769, 78.9622, 78.9755, 78.9757, 78.977, 7...</td>\n",
       "      <td>[79.0, 149.0, 163.0, 163.0, 206.0, 208.0, 208.0]</td>\n",
       "      <td>[147. 129.  72.  72. 145. 148. 148.]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>93.5430</td>\n",
       "      <td>93.4474</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>93.5430</td>\n",
       "      <td>[93.4474, 93.4562, 93.4626, 93.4476, 93.4564, ...</td>\n",
       "      <td>[40.0, 40.0, 40.0, 42.0, 42.0, 42.0, 48.0, 48....</td>\n",
       "      <td>[140. 140. 140. 143. 143. 143. 142. 142. 115. ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>114.2586</td>\n",
       "      <td>114.2036</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>114.2586</td>\n",
       "      <td>[114.2374, 114.2375, 114.2511, 114.2512, 114.2...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 206.0, 210.0]</td>\n",
       "      <td>[138. 138. 138. 138. 138. 145. 146.]</td>\n",
       "      <td>15.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>115.0579</td>\n",
       "      <td>115.0205</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>115.0579</td>\n",
       "      <td>[115.0206, 115.0207, 115.0207, 115.0208, 115.0...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 149.0, 149...</td>\n",
       "      <td>[138. 138. 138. 138. 138. 137. 137. 129. 129. ...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>119.7683</td>\n",
       "      <td>119.7589</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>119.7683</td>\n",
       "      <td>[119.7589, 119.768, 119.7682, 119.7683, 119.7679]</td>\n",
       "      <td>[21.0, 40.0, 42.0, 45.0, 48.0]</td>\n",
       "      <td>[116. 140. 143. 139. 142.]</td>\n",
       "      <td>18.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1911</td>\n",
       "      <td>1911</td>\n",
       "      <td>1911</td>\n",
       "      <td>1911</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>962.7765</td>\n",
       "      <td>962.7655</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>962.7765</td>\n",
       "      <td>[962.7655, 962.7667, 962.7715, 962.7764, 962.7...</td>\n",
       "      <td>[15.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 148...</td>\n",
       "      <td>[ 96. 105. 105. 105. 106. 106. 106. 182. 182.]</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>1913</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>968.9051</td>\n",
       "      <td>968.8036</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>968.9051</td>\n",
       "      <td>[968.8868, 968.8956, 968.8036, 968.8827, 968.8...</td>\n",
       "      <td>[15.0, 15.0, 101.0, 170.0, 170.0, 170.0, 170.0...</td>\n",
       "      <td>[ 96.  96. 117.  97.  97.  97.  97. 242. 242.]</td>\n",
       "      <td>978.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1914</td>\n",
       "      <td>1914</td>\n",
       "      <td>1914</td>\n",
       "      <td>1914</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>969.7686</td>\n",
       "      <td>969.7467</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>969.7686</td>\n",
       "      <td>[969.7686, 969.7686, 969.7583, 969.7526, 969.7...</td>\n",
       "      <td>[68.0, 70.0, 200.0, 204.0, 212.0]</td>\n",
       "      <td>[100.  99. 101. 248. 111.]</td>\n",
       "      <td>980.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1916</td>\n",
       "      <td>1916</td>\n",
       "      <td>1916</td>\n",
       "      <td>1916</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>982.2133</td>\n",
       "      <td>982.1402</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>982.2133</td>\n",
       "      <td>[982.2066, 982.2067, 982.2069, 982.1887, 982.1...</td>\n",
       "      <td>[8.0, 10.0, 10.0, 62.0, 68.0, 70.0, 82.0, 82.0...</td>\n",
       "      <td>[212. 213. 213. 175. 100.  99. 108. 108. 108. ...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>994.5238</td>\n",
       "      <td>994.5073</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>994.5238</td>\n",
       "      <td>[994.5198, 994.52, 994.5202, 994.511, 994.5173...</td>\n",
       "      <td>[8.0, 10.0, 10.0, 24.0, 24.0, 58.0, 70.0, 86.0...</td>\n",
       "      <td>[212. 213. 213.  95.  95.  13.  99. 185. 185. ...</td>\n",
       "      <td>133.0</td>\n",
       "      <td>sequential</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  cluster_seq_type  \\\n",
       "0        9             9             9           9                 1   \n",
       "1       10            10            10          10                 1   \n",
       "2       17            17            17          17                 1   \n",
       "3       18            18            18          18                 1   \n",
       "4       21            21            21          21                 1   \n",
       "..     ...           ...           ...         ...               ...   \n",
       "572   1911          1911          1911        1911                 6   \n",
       "573   1913          1913          1913        1913                 6   \n",
       "574   1914          1914          1914        1914                 6   \n",
       "575   1916          1916          1916        1916                 6   \n",
       "576   1918          1918          1918        1918                 6   \n",
       "\n",
       "     num_spikes  num_neurons  first_spike_time  event_length  last_spike_time  \\\n",
       "0             7      78.9772           78.9622        0.0150          78.9772   \n",
       "1            15      93.5430           93.4474        0.0956          93.5430   \n",
       "2             7     114.2586          114.2036        0.0550         114.2586   \n",
       "3            11     115.0579          115.0205        0.0374         115.0579   \n",
       "4             5     119.7683          119.7589        0.0094         119.7683   \n",
       "..          ...          ...               ...           ...              ...   \n",
       "572           9     962.7765          962.7655        0.0110         962.7765   \n",
       "573           9     968.9051          968.8036        0.1015         968.9051   \n",
       "574           5     969.7686          969.7467        0.0219         969.7686   \n",
       "575          20     982.2133          982.1402        0.0731         982.2133   \n",
       "576          18     994.5238          994.5073        0.0165         994.5238   \n",
       "\n",
       "                                   cluster_spike_times  \\\n",
       "0    [78.9769, 78.9622, 78.9755, 78.9757, 78.977, 7...   \n",
       "1    [93.4474, 93.4562, 93.4626, 93.4476, 93.4564, ...   \n",
       "2    [114.2374, 114.2375, 114.2511, 114.2512, 114.2...   \n",
       "3    [115.0206, 115.0207, 115.0207, 115.0208, 115.0...   \n",
       "4    [119.7589, 119.768, 119.7682, 119.7683, 119.7679]   \n",
       "..                                                 ...   \n",
       "572  [962.7655, 962.7667, 962.7715, 962.7764, 962.7...   \n",
       "573  [968.8868, 968.8956, 968.8036, 968.8827, 968.8...   \n",
       "574  [969.7686, 969.7686, 969.7583, 969.7526, 969.7...   \n",
       "575  [982.2066, 982.2067, 982.2069, 982.1887, 982.1...   \n",
       "576  [994.5198, 994.52, 994.5202, 994.511, 994.5173...   \n",
       "\n",
       "                                       cluster_neurons  \\\n",
       "0     [79.0, 149.0, 163.0, 163.0, 206.0, 208.0, 208.0]   \n",
       "1    [40.0, 40.0, 40.0, 42.0, 42.0, 42.0, 48.0, 48....   \n",
       "2              [6.0, 6.0, 6.0, 6.0, 6.0, 206.0, 210.0]   \n",
       "3    [6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 149.0, 149...   \n",
       "4                       [21.0, 40.0, 42.0, 45.0, 48.0]   \n",
       "..                                                 ...   \n",
       "572  [15.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 148...   \n",
       "573  [15.0, 15.0, 101.0, 170.0, 170.0, 170.0, 170.0...   \n",
       "574                  [68.0, 70.0, 200.0, 204.0, 212.0]   \n",
       "575  [8.0, 10.0, 10.0, 62.0, 68.0, 70.0, 82.0, 82.0...   \n",
       "576  [8.0, 10.0, 10.0, 24.0, 24.0, 58.0, 70.0, 86.0...   \n",
       "\n",
       "                                  spike_plotting_order  \\\n",
       "0                 [147. 129.  72.  72. 145. 148. 148.]   \n",
       "1    [140. 140. 140. 143. 143. 143. 142. 142. 115. ...   \n",
       "2                 [138. 138. 138. 138. 138. 145. 146.]   \n",
       "3    [138. 138. 138. 138. 138. 137. 137. 129. 129. ...   \n",
       "4                           [116. 140. 143. 139. 142.]   \n",
       "..                                                 ...   \n",
       "572     [ 96. 105. 105. 105. 106. 106. 106. 182. 182.]   \n",
       "573     [ 96.  96. 117.  97.  97.  97.  97. 242. 242.]   \n",
       "574                         [100.  99. 101. 248. 111.]   \n",
       "575  [212. 213. 213. 175. 100.  99. 108. 108. 108. ...   \n",
       "576  [212. 213. 213.  95.  95.  13.  99. 185. 185. ...   \n",
       "\n",
       "     coactive_cluster_group ordering_classification  rem_events  nrem_events  \n",
       "0                       8.0              sequential           0            1  \n",
       "1                       9.0              sequential           0            1  \n",
       "2                      15.0              sequential           0            1  \n",
       "3                      16.0              sequential           0            1  \n",
       "4                      18.0              sequential           0            1  \n",
       "..                      ...                     ...         ...          ...  \n",
       "572                  1078.0              sequential           0            1  \n",
       "573                   978.0              sequential           0            1  \n",
       "574                   980.0              sequential           0            1  \n",
       "575                   128.0              sequential           0            1  \n",
       "576                   133.0              sequential           0            1  \n",
       "\n",
       "[577 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "                        #2################################\n",
    "\n",
    "#                             current_sleep_start = sleep_start[mouse] - 400\n",
    "                            chunk_number = int(file.split('_')[0][-1])\n",
    "                            start_offset = ([0]+list(np.cumsum(np.diff(time_spans))))[chunk_number-1]\n",
    "\n",
    "\n",
    "                            # take away cumulative chunk offset - this gives time in terms of chunk\n",
    "                            f_spike_times = filtered_chunk_data.first_spike_time.values - start_offset\n",
    "                            # add on ephys time that chunk started - so its in ephys timestamps \n",
    "                            f_spike_times = f_spike_times + chunk_time[0]\n",
    "\n",
    "                            # now make relative to sleep start time\n",
    "                            f_spike_times_relative_to_so = f_spike_times - current_sleep_start \n",
    "                            # do the same but for rem and nrem start\n",
    "\n",
    "                            # filter out anything that happened before sleep onset\n",
    "                            f_spike_times_relative_to_so = f_spike_times_relative_to_so[f_spike_times_relative_to_so > 0]\n",
    "\n",
    "                            ## calculate rate over time:\n",
    "                            time_data = pd.Series(f_spike_times_relative_to_so)\n",
    "                            if len(time_data) > 0:\n",
    "#                                 # Calculate the number of bins required # 5 minute bins\n",
    "#                                 num_bins = int((time_data.max() - time_data.min()) // 40 + 1)\n",
    "#                                 # Create bins and count the occurrences in each bin\n",
    "#                                 chunk_event_rate, chunk_relative_time_bins = np.histogram(time_data, bins=num_bins)\n",
    "#                                 #remove extra final bin and convert to mins\n",
    "#                                 chunk_relative_time_bins = chunk_relative_time_bins[0:-1]/60\n",
    "\n",
    "                                # Calculate the number of bins required # 20s bins\n",
    "                            #     num_bins = int((time_data.max() - time_data.min()) // 40 + 1)\n",
    "                                if time_data.max() - time_data.min() > 19:\n",
    "                                    num_bins = int((time_data.max() - time_data.min())//20)\n",
    "                                    # Create bins and count the occurrences in each bin\n",
    "                                    chunk_event_rate, chunk_relative_time_bins = np.histogram(time_data, bins=num_bins)\n",
    "                                    #remove extra final bin and convert to mins\n",
    "                                    chunk_relative_time_bins = chunk_relative_time_bins[0:-1]/60\n",
    "\n",
    "\n",
    "                                    chunk_binned_rate += [list((chunk_event_rate*3).astype(float))] # *3 because its per 20s so we want it per minute )\n",
    "                                    chunk_bins_relative_so += [list(chunk_relative_time_bins.astype(float))]\n",
    "\n",
    "                        \n",
    "                        #3########################################################\n",
    "\n",
    "                        chunk_event_lens += list(filtered_chunk_data.event_length.values)\n",
    "\n",
    "                        #4 ################################################# coactive stuff -300ms = coactive\n",
    "                        event_proximity_filter =  0.3 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "\n",
    "                        task_seqs = np.load(current_data_path + 'task_order_seqs.npy')+1\n",
    "            \n",
    "                        for motif_type in filtered_chunk_data.cluster_seq_type:\n",
    "                            if motif_type in task_seqs:\n",
    "                                task_related += 1\n",
    "                            else:\n",
    "                                non_task_related += 1\n",
    "        \n",
    "                        total_events += len(filtered_chunk_data.cluster_seq_type)\n",
    "\n",
    "                        # normalise by number of each type: \n",
    "#                         if (6-len(task_seqs)) == 0:\n",
    "#                             chunk_total_nontask_task_related_events += [[non_task_related,(task_related/len(task_seqs))]]\n",
    "#                         else:\n",
    "#                             chunk_total_nontask_task_related_events += [[non_task_related/(6-len(task_seqs)),(task_related/len(task_seqs))]]\n",
    "\n",
    "                        chunk_mid_time_post_onset += [((sum(chunk_time)/2)-current_sleep_start)]\n",
    "\n",
    "                        ### ignore the origonal clusterg rosp and remake them: \n",
    "                        start_times = filtered_chunk_data.first_spike_time.values\n",
    "                        end_times = filtered_chunk_data.last_spike_time.values\n",
    "\n",
    "                        clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "                        cluster_group = np.zeros(len(filtered_chunk_data))\n",
    "                        for index,cluster in enumerate(clustered_events):\n",
    "                            for item in cluster:\n",
    "                                cluster_group[item] = int(index)\n",
    "                        filtered_chunk_data['coactive_cluster_group'] = cluster_group\n",
    "\n",
    "                        # work out how mnay coacitve in chunk: \n",
    "                        current_coactive_freqs_chunk = {}\n",
    "                        for cluster in filtered_chunk_data.coactive_cluster_group.unique():\n",
    "                            num = list(filtered_chunk_data.coactive_cluster_group.values).count(cluster)\n",
    "                            if num in current_coactive_freqs_chunk:\n",
    "                                current_coactive_freqs_chunk[num] += 1\n",
    "                            else:\n",
    "                                current_coactive_freqs_chunk[num] = 1\n",
    "\n",
    "                        avs =[]\n",
    "                        for item in current_coactive_freqs_chunk:\n",
    "                            avs += current_coactive_freqs_chunk[item] * [item]\n",
    "                        av_coactive_len_per_chunk += [np.mean(avs)]\n",
    "                        if mouse in expert_mice:\n",
    "                            chunk_expert += [1]\n",
    "                        elif mouse in hlesion_mice:\n",
    "                            chunk_expert += [2]\n",
    "                        elif mouse in learning_mice:\n",
    "                            chunk_expert += [3]\n",
    "\n",
    "\n",
    "                        # make it relative:\n",
    "                        current_coactive_freqs_chunk = relative_dict(current_coactive_freqs_chunk)\n",
    "\n",
    "                        coactive_freqs_keys = list(current_coactive_freqs_chunk.keys())\n",
    "                        rel_coactive_freqs = list(current_coactive_freqs_chunk.values())\n",
    "                        for index,item in enumerate(rel_coactive_freqs):\n",
    "                            num = int(coactive_freqs_keys[index])\n",
    "                            if num in coactive_freqs_chunk:\n",
    "                                coactive_freqs_chunk[num] += [item]\n",
    "                            else:\n",
    "                                coactive_freqs_chunk[num] = [item]\n",
    "\n",
    "\n",
    "                        task_events = filtered_chunk_data[filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "                        non_task_events = filtered_chunk_data[~filtered_chunk_data.cluster_seq_type.isin(task_seqs)]\n",
    "\n",
    "                        chunk_task_num_spikes+=list(task_events.num_spikes)\n",
    "                        chunk_nontask_num_spikes+=list(non_task_events.num_spikes)\n",
    "                        chunk_task_e_len+=list(task_events.event_length)\n",
    "                        chunk_nontask_e_len+=list(non_task_events.event_length)\n",
    "\n",
    "\n",
    "                        # 5 ##############################################################################\n",
    "\n",
    "                        ############################################## split into multi clusters and process\n",
    "\n",
    "                        multi_cluster_df = pd.DataFrame({'cluster_seq_type':[],\n",
    "                         'num_spikes':[],\n",
    "                         'num_neurons':[],\n",
    "                         'first_spike_time':[],\n",
    "                         'event_length':[],\n",
    "                         'last_spike_time':[],\n",
    "                         'cluster_spike_times':[],\n",
    "                         'cluster_neurons':[],\n",
    "                         'spike_plotting_order':[],\n",
    "                         'coactive_cluster_group':[],\n",
    "                         'new_cluster_group':[],\n",
    "                         'cluster_order_first_spike_defined':[],\n",
    "                         'cluster_order_mean_weighted_spikes_defined':[],\n",
    "                         'pairs_mean_ordering':[],\n",
    "                         'catagories_mean_ordering':[],\n",
    "                         'pairs_fs_ordering':[],\n",
    "                         'catagories_fs_ordering':[],\n",
    "                         'real_sequence_order':[]})\n",
    "                        meaned_order = []\n",
    "                        fs_order = []\n",
    "                        event_times = []\n",
    "                        multi_cluster_df\n",
    "                        count = 0\n",
    "                        for i,group in enumerate(filtered_chunk_data.coactive_cluster_group.unique()):\n",
    "                            group_mask = filtered_chunk_data.coactive_cluster_group == group\n",
    "                            current_cluster = filtered_chunk_data[group_mask]\n",
    "                            if len(current_cluster) > 1:\n",
    "                                means = []\n",
    "                                event_types = []\n",
    "                                fs_orders = []\n",
    "                                for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                                    event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                                    # calculate event order based on spike time weighted mean\n",
    "                                    means += [np.mean(ast.literal_eval(events))]\n",
    "                                    # calculate order based on first spike time:\n",
    "                                    fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "                                # order by mean time:    \n",
    "                                meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "                                # order by first spike:\n",
    "                                fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "                                event_times += [fs_orders]\n",
    "\n",
    "                                current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "                                current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "                                current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "                                if count == 0:\n",
    "                                    multi_cluster_df = current_cluster.copy()\n",
    "                                else:\n",
    "                                    # Concatenate the DataFrames vertically (row-wise)\n",
    "                                    multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                                    # Reset the index if needed\n",
    "                                    multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "                                count += 1\n",
    "\n",
    "                        ############################################## Load in seq order data \n",
    "\n",
    "                        awake_PP_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\\"\n",
    "\n",
    "                        for index_,M_I_R in enumerate(os.listdir(awake_PP_path)):\n",
    "                            if not M_I_R == 'not_suitable':\n",
    "                                mir = '_'.join(M_I_R.split('_')[0:3])\n",
    "                                if mir == mouse:\n",
    "                                    c_path = awake_PP_path + M_I_R + r\"\\analysis_output\\reordered_recolored\\\\\" \n",
    "\n",
    "                        sequence_order_df = pd.read_csv(awake_PP_path+\"sequence_order.csv\")\n",
    "\n",
    "                        import ast\n",
    "                        seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == mouse].seq_order.values[0])\n",
    "                        num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == mouse].dominant_task_seqs)\n",
    "\n",
    "                        ############################################## calculate catagory breakdown\n",
    "\n",
    "                        if len(multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "\n",
    "                            real_order = list(np.array(seq_order)+1)\n",
    "\n",
    "                            # # mean ordering first : \n",
    "                            if len(real_order) > 3: # 3 will always be ordered so exclude\n",
    "                                relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "                                summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "                            #     labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "                            #     fig, ax = plt.subplots()\n",
    "                            #     ax.bar(labels,summed_amounts)\n",
    "                            #     ax.set_title('catagory occurances (seqs ordered by mean spike time)')\n",
    "\n",
    "                            #     SaveFig('catagory occurances_1___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                                all_pair_outcomes_todf = []\n",
    "                                all_pairs_todf = []\n",
    "                                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                                    group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                                    group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                                    all_pairs = []\n",
    "                                    all_pair_outcomes = []\n",
    "                                    for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                                        all_pairs += [pair_]\n",
    "                                        all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "                                    all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "                                    all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "                                multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "                                multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "                                multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "                                chunk_summed_amounts += [list(np.array(summed_amounts)/sum(summed_amounts))]\n",
    "\n",
    "                                chunk_ordered_sum += sum(summed_amounts[0:3])\n",
    "                                chunk_coactive_total += sum(summed_amounts[0:4])\n",
    "                            else:\n",
    "                                print('only 3 seqs')\n",
    "\n",
    "                            \n",
    "                            \n",
    "#                             print(chunk_summed_amounts)\n",
    "                            \n",
    "        \n",
    "                # outside of chunk loop ################################################\n",
    "                \n",
    "                # changed how i do this, now task freq is worke dout by adding up instances across all chunks and lookig at the proportion rather than averageing across chunks \n",
    "                if (6-len(task_seqs)) == 0:\n",
    "                    chunk_total_nontask_task_related_events += [[non_task_related,(task_related/len(task_seqs))]]\n",
    "                else:\n",
    "                    chunk_total_nontask_task_related_events += [[non_task_related/(6-len(task_seqs)),(task_related/len(task_seqs))]]      \n",
    "\n",
    "                ### add to animal vars\n",
    "                #1\n",
    "                reactivations_per_min += [np.mean(chunk_rpm)]\n",
    "                if np.mean(chunk_rpm) < 3:\n",
    "                    print('!!!!!')\n",
    "                #2\n",
    "                event_rate_binned +=[chunk_binned_rate]\n",
    "                er_bins_relative_to_so +=[chunk_bins_relative_so]\n",
    "                #3\n",
    "                event_lens += [chunk_event_lens]\n",
    "\n",
    "\n",
    "                #4 #########    \n",
    "                relative = []\n",
    "                totals = [sum(item) for item in chunk_total_nontask_task_related_events]\n",
    "                for i,item in enumerate(chunk_total_nontask_task_related_events):\n",
    "                    relative += [list(np.array(item)/totals[i])]\n",
    "\n",
    "                all_total_events += [total_events]\n",
    "\n",
    "                num_task_order_seqs = len(np.load(current_data_path+ 'task_order_seqs.npy')+1)\n",
    "\n",
    "                rel_task_nontask += [[np.mean(conactinate_nth_items(relative)[1]),np.mean(conactinate_nth_items(relative)[0])]]\n",
    "\n",
    "                chunks_task_nontask += conactinate_nth_items(relative)[1]\n",
    "\n",
    "                for item in coactive_freqs_chunk:\n",
    "                    if mouse in expert_mice:\n",
    "                        if item in e_coactive_freqs_counts:\n",
    "                            e_coactive_freqs_counts[item] += [np.mean(coactive_freqs_chunk[item])]\n",
    "                        else:\n",
    "                            e_coactive_freqs_counts[item] = [np.mean(coactive_freqs_chunk[item])]\n",
    "                    elif mouse in hlesion_mice:\n",
    "                        if item in hl_coactive_freqs_counts:\n",
    "                            hl_coactive_freqs_counts[item] += [np.mean(coactive_freqs_chunk[item])]\n",
    "                        else:\n",
    "                            hl_coactive_freqs_counts[item] = [np.mean(coactive_freqs_chunk[item])]\n",
    "                    elif mouse in learning_mice:\n",
    "                        if item in l_coactive_freqs_counts:\n",
    "                            l_coactive_freqs_counts[item] += [np.mean(coactive_freqs_chunk[item])]\n",
    "                        else:\n",
    "                            l_coactive_freqs_counts[item] = [np.mean(coactive_freqs_chunk[item])]\n",
    "\n",
    "\n",
    "\n",
    "                task_nontask_num_spikes+= [[np.mean(chunk_task_num_spikes),np.mean(chunk_nontask_num_spikes)]]\n",
    "                task_nontask_e_len+= [[np.mean(chunk_task_e_len),np.mean(chunk_nontask_e_len)]]\n",
    "\n",
    "                #5 #############\n",
    "\n",
    "                if len(chunk_summed_amounts) > 0:\n",
    "                    c_summed_amounts = []\n",
    "                    for item in conactinate_nth_items(chunk_summed_amounts):\n",
    "                        c_summed_amounts +=[np.mean(item)]\n",
    "                    mouse_summed_amounts += [c_summed_amounts]\n",
    "                else:\n",
    "                    mouse_summed_amounts += [[]]\n",
    "                    \n",
    "                    \n",
    "                ordered_sum += [chunk_ordered_sum]\n",
    "                ordered_misordered_total += [chunk_coactive_total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'EJT'in mir:\n",
    "    c_mir = mir.split('T')[-1]\n",
    "else:\n",
    "    c_mir = mir\n",
    "full_sleep_path = None\n",
    "for ppsleep_file in os.listdir(sleep_ppseq_path):\n",
    "    if c_mir in ppsleep_file:\n",
    "        'print sleep file found'\n",
    "        full_sleep_path = os.path.join(sleep_ppseq_path,ppsleep_file + '/analysis_output')\n",
    "if full_sleep_path is None:\n",
    "    raise Exception(f\"no sleep file found for {mir}\")\n",
    "\n",
    "os.listdir(full_sleep_path)\n",
    "\n",
    "chunk_paths = []\n",
    "for file in os.listdir(full_sleep_path):\n",
    "    if 'chunk' in file:\n",
    "        chunk_paths += [os.path.join(full_sleep_path,file)]     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
