{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f35f3d",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd41257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import os\n",
    "from open_ephys.analysis import Session\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob \n",
    "from scipy.signal import butter, sosfilt, sosfilt_zi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# Define the bandpass filter\n",
    "def create_bandpass_filter(lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    sos = butter(order, [low, high], btype='band', output='sos')\n",
    "    return sos\n",
    "    \n",
    "def list_all_datapaths(data_path):\n",
    "    data_paths = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if 'EJT' in file[0:5] or 'revision' in data_path:\n",
    "            for file_ in os.listdir(data_path+file):\n",
    "                if 'record' in file_:\n",
    "                    data_paths += [data_path+file+'\\\\'+file_]\n",
    "            \n",
    "    return data_paths\n",
    "\n",
    "def walk_through_files_countinous_data(recording_path_1):\n",
    "    continuous_file_path = None\n",
    "\n",
    "    for root, dirs, files in os.walk(recording_path_1):\n",
    "        if 'continuous.dat' in files:\n",
    "            continuous_file_path = os.path.join(root, 'continuous.dat')\n",
    "            break\n",
    "\n",
    "    if continuous_file_path:\n",
    "        print(f\"Found 'continuous.dat' at: {continuous_file_path}\")\n",
    "        return continuous_file_path\n",
    "    else:\n",
    "        print(\"'continuous.dat' not found in the specified path.\")\n",
    "        \n",
    "def find_processor_tuples(processor_path):\n",
    "    count = 0\n",
    "    for processor in os.listdir(processor_path):\n",
    "        if count == 0:\n",
    "            main1 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main1_2 = processor.split('.')[-1]\n",
    "        elif count == 1:\n",
    "            main2 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main2_2 = processor.split('.')[-1]\n",
    "        elif count == 2:\n",
    "            main3 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main3_2 = processor.split('.')[-1]\n",
    "        count +=1 \n",
    "\n",
    "    main_processor_tuple=(main1, main1_2)\n",
    "\n",
    "    aux_processor_tuples=((main2,main2_2),(main3,main3_2))\n",
    "    return main_processor_tuple,aux_processor_tuples\n",
    "\n",
    "def process_probe_data_bool(organised_ephys_path,aux_processor_tuples):\n",
    "    process = False\n",
    "    if not 'global-timstamps_event-df.pkl' in os.listdir(organised_ephys_path):\n",
    "        if not 'main_continuous_global_ts_probeA.npy' in os.listdir(organised_ephys_path):\n",
    "            if not 'LFP' in aux_processor_tuples[0][-1]:\n",
    "                if not 'main_continuous_global_ts_probeA_LFP.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "            else:\n",
    "                if not 'main_continuous_global_ts_probeB.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "    return process\n",
    "\n",
    "## new version for new open ephys tools \n",
    "def align_open_ephys_processors(main_processor_tuple, aux_processor_tuples,raw_data_directory, sync_channel=1):\n",
    "\n",
    "    session_data = Session(str(raw_data_directory))\n",
    "    if len(session_data.recordnodes) != 1:\n",
    "        raise ValueError(\"should be exactly one record node.\")\n",
    "    if len(session_data.recordnodes[0].recordings) != 1:\n",
    "        raise ValueError(\"Should be exactly one recording.\")\n",
    "    for rn, recordnode in enumerate(session_data.recordnodes):\n",
    "        for r, recording in enumerate(recordnode.recordings):\n",
    "            # Sync\n",
    "            recording.add_sync_line(\n",
    "                sync_channel,\n",
    "                main_processor_tuple[0],\n",
    "                main_processor_tuple[1],\n",
    "                main=True,\n",
    "            )\n",
    "            for aux_processor in aux_processor_tuples:\n",
    "                recording.add_sync_line(\n",
    "                    sync_channel,\n",
    "                    aux_processor[0],\n",
    "                    aux_processor[1],\n",
    "                    main=False,\n",
    "                )\n",
    "            print('this should be zero:')\n",
    "            print(rn)\n",
    "        \n",
    "    return recording\n",
    "\n",
    "def check_if_probe_is_flipped(A_probes):\n",
    "\n",
    "    # Check if 'CP' appears before 'ccb or ccg' in the list of region acronyms, it shouldnt if the probe is the right way up\n",
    "    if 'CP' in A_probes['Region acronym'].values and 'ccb' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccb'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    elif 'CP' in A_probes['Region acronym'].values and 'ccg' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccg'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    else:\n",
    "        print('error')\n",
    "    return flipped\n",
    "\n",
    "def find_propotion_in_striatum(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        str_prop = callosum_middle_index/len(implant_df)\n",
    "    else:\n",
    "        str_prop = 1- callosum_middle_index/len(implant_df)\n",
    "    return str_prop\n",
    "\n",
    "\n",
    "def process_probe_channels(ProbeA_data,channels,channel_regions,current_mouse,output_path,var_string):\n",
    "    # pull out the data for each channel, bandpass to prevent aliasing then downsample\n",
    "         \n",
    "    # Parameters\n",
    "    lowcut = 20.0  # Lower cutoff frequency in Hz\n",
    "    highcut = 1250.0  # Upper cutoff frequency in Hz\n",
    "    fs = 30000.0  # Original sampling rate in Hz\n",
    "    downsample_factor = 12  # Factor by which to downsample\n",
    "    chunk_size = 2000  # Number of samples per chunk\n",
    "    # Create the bandpass filter\n",
    "    sos = create_bandpass_filter(lowcut, highcut, fs)\n",
    "    # Initialize the filter state\n",
    "    zi = sosfilt_zi(sos)\n",
    "\n",
    "    # Process each channel\n",
    "    for ind_,chosen_channel in enumerate(channels):\n",
    "        data_channel = []\n",
    "        for i in tqdm(range(0, len(ProbeA_data), chunk_size)):\n",
    "            # Extract the current chunk for the chosen channel\n",
    "            chunk = np.array([ProbeA_data[j][chosen_channel] for j in range(i, min(i + chunk_size, len(ProbeA_data)))])\n",
    "\n",
    "            # Apply the bandpass filter to the chunk\n",
    "            if i == 0:\n",
    "                # For the first chunk, use the initial filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi * chunk[0])\n",
    "            else:\n",
    "                # For subsequent chunks, use the updated filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi)\n",
    "\n",
    "            # Append the filtered chunk to the channel data\n",
    "            data_channel.extend(bp_chunk)\n",
    "\n",
    "        # Downsample the filtered data\n",
    "        data_downsampled = data_channel[::downsample_factor]\n",
    "        \n",
    "        # clean up for memory\n",
    "        del data_channel\n",
    "        \n",
    "        mouse_out_path = os.path.join(output_path,var_string) + current_mouse\n",
    "        if not os.path.exists(mouse_out_path):\n",
    "            os.makedirs(mouse_out_path)\n",
    "            \n",
    "        save_path = mouse_out_path + '//channel-' + str(chosen_channel) + '_REGION-' + channel_regions[ind_] + \"_LFP_data.npy\"\n",
    "        np.save(save_path,data_downsampled)\n",
    "        print('data saved for channel ' + str(chosen_channel))\n",
    "\n",
    "\n",
    "\n",
    "def gather_paths_for_old_data(current_mouse,path):\n",
    "    ### find OE processor path and OE_raw_path\n",
    "    OE_processor_path_base = r\"Z:\\projects\\sequence_squad\\data\\raw_neuropixel\\OE_DATA\\\\\"\n",
    "    if current_mouse.split('_')[1] == 2:\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] + '_' + 'implant' + current_mouse.split('_')[1]\n",
    "    else:\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] \n",
    "    for folder in os.listdir(OE_processor_path_base):\n",
    "        if mouse_folder == folder:\n",
    "            print(mouse_folder)\n",
    "            OE_processor_path = OE_processor_path_base + folder + '\\\\'\n",
    "\n",
    "    # next get the date \n",
    "    recording_date = path.split('\\\\')[-1].split('_')[-1]\n",
    "    reformatted_date = ''.join(recording_date.split('-')[0:-1]) + recording_date.split('-')[-1][-2::]\n",
    "    \n",
    "    recording_path_1 = None\n",
    "    for recording_date in os.listdir(OE_processor_path):\n",
    "        if reformatted_date == recording_date:\n",
    "            print(reformatted_date)\n",
    "            recording_path_1 = os.path.join(OE_processor_path,recording_date) + '\\\\'\n",
    "            \n",
    "    # set some OE paths I need \n",
    "    continuous_file_path = walk_through_files_countinous_data(recording_path_1)\n",
    "    processor_path = '\\\\'.join(continuous_file_path.split('\\\\')[0:-2])+ '\\\\'\n",
    "    OE_raw_path = os.path.join(recording_path_1,os.listdir(recording_path_1)[0]) + '\\\\'\n",
    "    return OE_raw_path,processor_path\n",
    "\n",
    "def find_folder_path(parent_folder, target_folder):\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        if target_folder in dirs:\n",
    "            return os.path.join(root, target_folder)\n",
    "        # If the target folder is not found\n",
    "    return (print('not found'))\n",
    "\n",
    "\n",
    "def gather_paths_for_new_data(mir):\n",
    "\n",
    "    ## gather all raw ephys paths\n",
    "    path_ = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\\"\n",
    "    base_recording_paths = []\n",
    "    for q in os.listdir(path_):\n",
    "        if not 'other_sessions' in q:\n",
    "            if not 'sp5_recordings' in q:\n",
    "                folder = os.path.join(path_,q)\n",
    "                for q in os.listdir(folder):\n",
    "                    base_recording_paths+=[os.path.join(folder,q)]\n",
    "                \n",
    "\n",
    "    # gather the corespoding organised paths for each raw dat file \n",
    "    organised_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "    full_organised_paths = []\n",
    "    mirs = []\n",
    "    for i in range(len(base_recording_paths)):\n",
    "        mouse_id = base_recording_paths[i].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths[i].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "        \n",
    "        organised_path_animal = os.path.join(organised_path,mouse_id+'_implant1')\n",
    "\n",
    "        for recording in os.listdir(organised_path_animal):\n",
    "            if date_ in recording:\n",
    "                full_organised_paths += [os.path.join(organised_path_animal,recording)]\n",
    "                mirs += [mouse_id + '_1_' + recording.split('_')[0].split('g')[-1]]\n",
    "                break\n",
    "\n",
    "\n",
    "    for index in range(len(base_recording_paths)):\n",
    "        \n",
    "        mouse_id = base_recording_paths[index].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths[index].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "        \n",
    "        \n",
    "        probeB = False\n",
    "        if mirs[index] == mir:\n",
    "            print(mir)\n",
    "            print(index)\n",
    "            \n",
    "            # set important paths\n",
    "            raw_data_directory = base_recording_paths[index]\n",
    "            print(raw_data_directory)\n",
    "            OE_processor_path = find_folder_path(raw_data_directory, \"continuous\") \n",
    "            Behav_data_path = full_organised_paths[index]+ r'//behav_sync/2_task/Preprocessed//'\n",
    "\n",
    "            Processed_Ephys_data_path_PROBEA = full_organised_paths[index]+ r'/ephys//' + r'probeA/kilosort4_output/sorter_output//'\n",
    "            if 'probeB' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                print('Probe B found')\n",
    "                Processed_Ephys_data_path_PROBEB = full_organised_paths[index]+ r'/ephys//' + r'probeB/kilosort4_output/sorter_output//'\n",
    "                probeB = True\n",
    "            \n",
    "            organised_ephys_path = full_organised_paths[index]+ r'/ephys//'\n",
    "            \n",
    "            for vid_file in os.listdir(full_organised_paths[index] + r'\\video\\videos\\\\'):\n",
    "                if 'BACK' in vid_file:\n",
    "                    if 'avi' in vid_file:\n",
    "                        back_video_path = os.path.join(full_organised_paths[index] + r'\\video\\videos\\\\',vid_file)\n",
    "\n",
    "                    \n",
    "            if 'probeA' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeA'):\n",
    "                    if probeB: \n",
    "                        if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeB'):\n",
    "                            print('All good! Data is kilosorted for PROBE A and PROBE B ')\n",
    "                            \n",
    "                            print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                            print(raw_data_directory)\n",
    "                            print(OE_processor_path)\n",
    "                            print(Behav_data_path)\n",
    "                            print(Processed_Ephys_data_path_PROBEA)\n",
    "                            print(back_video_path)\n",
    "                            break\n",
    "                        else:\n",
    "                            print('PROBE B data not yet kilosorted, skip!')\n",
    "                    else:\n",
    "                        print('All good! Data is kilosorted for PROBE A')\n",
    "                        print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                        print(raw_data_directory)\n",
    "                        print(OE_processor_path)\n",
    "                        print(Behav_data_path)\n",
    "                        print(Processed_Ephys_data_path_PROBEA)\n",
    "                        print(back_video_path)\n",
    "                        break\n",
    "                        \n",
    "                else:\n",
    "                    print('data not yet kilosorted, skip!')\n",
    "            else:\n",
    "                print('data not yet kilosorted, skip!')\n",
    "                \n",
    "    return raw_data_directory, OE_processor_path\n",
    "\n",
    "\n",
    "def find_histology_paths(mouse_id,ProbeB):\n",
    "\n",
    "    # load the brainreg positions for the probe that was used in the recoridng (based on the manual labelling)\n",
    "    brainreg_base_path = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\serial_section\\brainreg_output\\brainreg\\\\\"\n",
    "\n",
    "    for mouse_file in os.listdir(brainreg_base_path):\n",
    "        if mouse_file in mouse_id:\n",
    "            print(mouse_file)\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        elif mouse_file in mouse_id.lower():\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        else:\n",
    "            b_reg_path = None\n",
    "            \n",
    "    shank = shanks[overall_run_index]\n",
    "    if len([f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]) > 0:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]\n",
    "    else:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeA_{shank}' in f and f.endswith('.csv')]\n",
    "        \n",
    "    # Load the CSV files into dataframes\n",
    "    A_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeA_csv_files]\n",
    "\n",
    "    if ProbeB:\n",
    "        if len([f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]) > 0:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]\n",
    "        else:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeB' in f and f.endswith('.csv')]\n",
    "        B_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeB_csv_files]  \n",
    "        \n",
    "        return A_probes, B_probes\n",
    "    else:\n",
    "        return A_probes\n",
    "    \n",
    "    \n",
    "def find_propotion_new_data(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "    boundary_um = implant_df['Distance from first position [um]'][callosum_middle_index]\n",
    "    full_length = max(implant_df['Distance from first position [um]'].values)\n",
    "\n",
    "    # 2.0 probes have 2 electrodes per bank and 15um spacing between banks\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        channel_boundary = int(boundary_um /15) *2\n",
    "    else:\n",
    "        channel_boundary = int((full_length-boundary_um)/15)*2\n",
    "\n",
    "    return channel_boundary, int(full_length/15)*2\n",
    "        \n",
    "def plot_boundary(first_cortex_channel,total_chans):\n",
    "    fig,ax = plt.subplots(1,1,figsize = (1,3))\n",
    "    ax.plot([0,0],[0,total_chans],'-')\n",
    "    ax.plot(first_cortex_channel,'o')\n",
    "    ax.set_title('srtr-cortex boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cae777",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9948fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new place to store the data I am going to use\n",
    "\n",
    "# pull out data path\n",
    "# if its a dir with probe A and B then it has no LFP so i need to dwonsample. \n",
    "\n",
    "# make sure the data is aligned properly. \n",
    "\n",
    "# save out the extracted LFP data to a new location in the revision data folder\n",
    "\n",
    "# it might be worth loading in and saving out the replay events for each session as well here, just so i have everything nice and together\n",
    "\n",
    "# this will make it easier to load in the data for the actual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31f6b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to move the new lars ephys data to make this work better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa1093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "\n",
      "Z:\\projects\\sequence_squad\\organised_data\\animals\\\\EJT136_implant1\\recording3_11-11-2021\n",
      "EJT136\n",
      "111121\n",
      "Found 'continuous.dat' at: Z:\\projects\\sequence_squad\\data\\raw_neuropixel\\OE_DATA\\\\EJT136\\111121\\2021-11-11_14-18-10\\Record Node 102\\experiment1\\recording1\\continuous\\Neuropix-PXI-113.0\\continuous.dat\n",
      "this should be zero:\n",
      "0\n",
      "saving...\n",
      "Z:\\projects\\sequence_squad\\data\\histology\\Neuropixel_tracks\\\\EJT136\\brainreg\\manual_segmentation\\standard_space\\tracks\\implant1.csv\n",
      "cp appears first \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████▎                                                            | 5294/16610 [01:41<02:53, 65.24it/s]"
     ]
    }
   ],
   "source": [
    "# define paths:\n",
    "\n",
    "## old data \n",
    "data_path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\'\n",
    "animals  = ['136_1_3', '136_1_4', '149_1_1', '149_1_2', '149_1_3','162_1_3', '178_1_4', '178_1_5', '178_1_6', '178_1_7', '178_1_8','178_1_9', '178_2_1', '178_2_2', '178_2_4', '268_1_10','269_1_4', '269_1_7', '270_1_5', '270_1_6','270_1_7']\n",
    "\n",
    "## something wrong with '148_2_2',\n",
    "\n",
    "## new data \n",
    "# data_path = r'Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\'\n",
    "# animals = ['ap5R_1_1','ap5R_1_2','ap5R_1_3','seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11','seq008_1_3']\n",
    "# shanks = [1,1,1,3,3,3,3,3,3,3,3,2,1,4,3]\n",
    "\n",
    "\n",
    "output_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\oscillations\\\\\"\n",
    "\n",
    "\n",
    "# loop through all the animals\n",
    "for overall_run_index, animal in enumerate(animals):\n",
    "\n",
    "    # list out all possible datapaths\n",
    "    data_paths = list_all_datapaths(data_path) \n",
    "\n",
    "    for path in data_paths:\n",
    "        if 'EJT' in path:\n",
    "            a = path.split('\\\\')[-2].split('_')[0][3::]\n",
    "        else:\n",
    "            a = path.split('\\\\')[-2].split('_')[0]\n",
    "        b = path.split('\\\\')[-2][-1]\n",
    "        c = path.split('\\\\')[-1].split('_')[0][-1]\n",
    "        current_mouse = '_'.join([a,b,c])\n",
    "        if current_mouse != animal:\n",
    "            continue\n",
    "        print('-------------------------------------')\n",
    "        print()\n",
    "        print(path)\n",
    "        organised_ephys_path = os.path.join(path, r'ephys\\\\')\n",
    "        \n",
    "        # get the recording folder\n",
    "        mouse_folder = None\n",
    "        if 'EJT' in path:\n",
    "            OE_raw_path,processor_path = gather_paths_for_old_data(current_mouse,path)\n",
    "        else:\n",
    "            OE_raw_path, processor_path = gather_paths_for_new_data(current_mouse)\n",
    "            \n",
    "        # find processor tuples\n",
    "        main_processor_tuple,aux_processor_tuples = find_processor_tuples(processor_path)\n",
    "        \n",
    "        #!## LOAD in ephys data: WARNING V SLOW - this could take a few minutes ############!#\n",
    "        \n",
    "        recording = align_open_ephys_processors(main_processor_tuple,aux_processor_tuples,OE_raw_path)\n",
    "        recording.compute_global_timestamps()\n",
    "        \n",
    "        #!##################################################################################!#\n",
    "        \n",
    "        ## Save this out:\n",
    "        print('saving...')\n",
    "        if not os.path.isdir(organised_ephys_path):\n",
    "            os.makedirs(organised_ephys_path)\n",
    "            \n",
    "        # check to see if global object alreayd exists and if it does no need to save\n",
    "        if process_probe_data_bool(organised_ephys_path,aux_processor_tuples) == False:\n",
    "            save_path = organised_ephys_path + f\"global-timstamps_event-df.pkl\"\n",
    "            recording.events.to_pickle(save_path)\n",
    "        \n",
    "        ## Extract timestamp data:\n",
    "        events_df= recording.events\n",
    "\n",
    "        ## extract the main npx continuous data:\n",
    "        # work out the data ind\n",
    "        probeA_data_index = None\n",
    "        probeA_data_index = None\n",
    "        ProbeB = False\n",
    "        if 'EJT' in path:\n",
    "            for index, item in enumerate(os.listdir(processor_path)):\n",
    "                meta_data = recording.continuous[i].metadata\n",
    "                if 'AP' in meta_data['channel_names'][0]:\n",
    "                    probeA_data_index = index\n",
    "        else:\n",
    "            for index, item in enumerate(os.listdir(processor_path)):   \n",
    "                meta_data = recording.continuous[i].metadata\n",
    "                if 'ProbeA' in meta_data['stream_name'][i]:\n",
    "                    probeA_data_index = index\n",
    "                    \n",
    "            #extract probe A\n",
    "            if 'ProbeB' in ''.join(os.listdir(processor_path)[0::]):\n",
    "                ProbeB = True\n",
    "                for index, item in enumerate(os.listdir(processor_path)):   \n",
    "                    meta_data = recording.continuous[i].metadata\n",
    "                    if 'ProbeB' in meta_data['stream_name'][i]:\n",
    "                        probeA_data_index = index\n",
    "                        \n",
    "        ProbeA_data = recording.continuous[probeA_data_index].samples\n",
    "        ProbeA_global_timestamps = recording.continuous[probeA_data_index].global_timestamps\n",
    "        if ProbeB:\n",
    "            ProbeB_data = recording.continuous[probeB_data_index].samples\n",
    "            ProbeB_global_timestamps = recording.continuous[probeB_data_index].global_timestamps\n",
    "            \n",
    "        #choose 8 channels \n",
    "        channels = [50,90,130,170,210,250,290,340]\n",
    "        \n",
    "        # import the histology data\n",
    "        if 'EJT' in path:\n",
    "            brainreg_base_path = r\"Z:\\projects\\sequence_squad\\data\\histology\\Neuropixel_tracks\\\\\"\n",
    "            for mouse_name in os.listdir(brainreg_base_path):\n",
    "                if current_mouse.split('_')[0] in mouse_name:\n",
    "                    hist_path_base = os.path.join(brainreg_base_path,mouse_name)\n",
    "                    hist_path = os.path.join((hist_path_base),\"brainreg\\\\\")\n",
    "                    hist_path = glob.glob(os.path.join(hist_path, '**','tracks'),recursive = True)[0]\n",
    "                    # load the data\n",
    "                    implant_files = []\n",
    "                    for item in os.listdir(hist_path):\n",
    "                        if 'csv' in item:\n",
    "                            implant_files+=[item]\n",
    "\n",
    "                    if len(implant_files) > 1:\n",
    "                        for file in implant_files:\n",
    "                            if current_mouse.split('_')[-2] in file:\n",
    "                                implant_file = file\n",
    "                    else:\n",
    "                        implant_file = implant_files[0]\n",
    "\n",
    "                    probe_track_file = os.path.join(hist_path,implant_file) \n",
    "                    print(probe_track_file)\n",
    "                    implant_df = pd.read_csv(probe_track_file)\n",
    "                    \n",
    "                    # this is only needed for probe A:\n",
    "                    striatum_proportion = find_propotion_in_striatum(implant_df)\n",
    "                    # there should be 400 channels per 4000um (what i implanted), tot_channels = 384, bank_spacing = 20 # 20um, channels_per_bank = 2\n",
    "                    first_cortex_channel = int(striatum_proportion * 400)\n",
    "                    total_chans = 400\n",
    "                    plot_boundary(first_cortex_channel,total_chans)\n",
    "                                        \n",
    "        else:\n",
    "            if ProbeB:\n",
    "                implant_dfs_A,implant_dfs_B = find_histology_paths(current_mouse,ProbeB)\n",
    "            else:\n",
    "                implant_df_A = find_histology_paths(current_mouse,ProbeB)\n",
    "            first_cortex_channel,total_chans = find_propotion_new_data(implant_dfs_A[0])\n",
    "            plot_boundary(first_cortex_channel,total_chans)\n",
    "        \n",
    "        #label the channels   \n",
    "        channel_regions = []\n",
    "        for channel in channels:\n",
    "            if channel >= first_cortex_channel:\n",
    "                channel_regions.append('m_crtex')\n",
    "            elif channel < first_cortex_channel:\n",
    "                channel_regions.append('striatum')\n",
    "                \n",
    "        # process the data for probe A\n",
    "        process_probe_channels(ProbeA_data,channels,channel_regions,current_mouse,output_path,'striatum_lfp\\\\')\n",
    "        \n",
    "        # same for probe B\n",
    "        if ProbeB:\n",
    "            #label the channels for probe B  \n",
    "            probeB_channel_regions = ['hippocampus'] * len(channels)\n",
    "            process_probe_channels(ProbeB_data,channels,probeB_channel_regions,current_mouse,output_path,'hippocampus_lfp\\\\')\n",
    "            \n",
    "        # save out the timestamp data\n",
    "\n",
    "        Fs = 30000\n",
    "        timestamps_index = np.linspace(0,len(ProbeA_global_timestamps),len(ProbeA_global_timestamps)+1).astype(int)\n",
    "        timestamps_index_downsampled = timestamps_index[::12]\n",
    "        timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "        downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "        mouse_out_path = os.path.join(output_path, 'striatum_lfp\\\\') + current_mouse + '\\\\'\n",
    "        downsampled_timstamp_df.tocsv(mouse_out_path + 'probeA_timestamps.csv')\n",
    "        \n",
    "        if ProbeB:\n",
    "            timestamps_index = np.linspace(0,len(ProbeB_global_timestamps),len(ProbeB_global_timestamps)+1).astype(int)\n",
    "            timestamps_index_downsampled = timestamps_index[::12]\n",
    "            timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "            downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "            mouse_out_path = os.path.join(output_path, 'striatum_lfp\\\\') + current_mouse + '\\\\'\n",
    "            downsampled_timstamp_df.tocsv(mouse_out_path + 'probeB_timestamps.csv')\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5503c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b20ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
