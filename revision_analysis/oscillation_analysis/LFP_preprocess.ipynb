{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f35f3d",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bd41257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from open_ephys.analysis import Session\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob \n",
    "from scipy.signal import butter, sosfilt, sosfilt_zi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# Define the bandpass filter\n",
    "def create_bandpass_filter(lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    sos = butter(order, [low, high], btype='band', output='sos')\n",
    "    return sos\n",
    "    \n",
    "def list_all_datapaths(data_path):\n",
    "    data_paths = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if 'EJT' in file[0:5] or 'revision' in data_path:\n",
    "            for file_ in os.listdir(data_path+file):\n",
    "                if 'record' in file_:\n",
    "                    data_paths += [data_path+file+'\\\\'+file_]\n",
    "            \n",
    "    return data_paths\n",
    "\n",
    "def walk_through_files_countinous_data(recording_path_1):\n",
    "    continuous_file_path = None\n",
    "\n",
    "    for root, dirs, files in os.walk(recording_path_1):\n",
    "        if 'continuous.dat' in files:\n",
    "            continuous_file_path = os.path.join(root, 'continuous.dat')\n",
    "            break\n",
    "\n",
    "    if continuous_file_path:\n",
    "        print(f\"Found 'continuous.dat' at: {continuous_file_path}\")\n",
    "        return continuous_file_path\n",
    "    else:\n",
    "        print(\"'continuous.dat' not found in the specified path.\")\n",
    "        \n",
    "def find_processor_tuples(processor_path):\n",
    "    count = 0\n",
    "    for processor in os.listdir(processor_path):\n",
    "        if count == 0:\n",
    "            main1 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main1_2 = processor.split('.')[-1]\n",
    "        elif count == 1:\n",
    "            main2 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main2_2 = processor.split('.')[-1]\n",
    "        elif count == 2:\n",
    "            main3 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main3_2 = processor.split('.')[-1]\n",
    "        count +=1 \n",
    "\n",
    "    main_processor_tuple=(main1, main1_2)\n",
    "\n",
    "    aux_processor_tuples=((main2,main2_2),(main3,main3_2))\n",
    "    return main_processor_tuple,aux_processor_tuples\n",
    "\n",
    "def process_probe_data_bool(organised_ephys_path,aux_processor_tuples):\n",
    "    process = False\n",
    "    if not 'global-timstamps_event-df.pkl' in os.listdir(organised_ephys_path):\n",
    "        if not 'main_continuous_global_ts_probeA.npy' in os.listdir(organised_ephys_path):\n",
    "            if not 'LFP' in aux_processor_tuples[0][-1]:\n",
    "                if not 'main_continuous_global_ts_probeA_LFP.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "            else:\n",
    "                if not 'main_continuous_global_ts_probeB.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "    return process\n",
    "\n",
    "## new version for new open ephys tools \n",
    "def align_open_ephys_processors(main_processor_tuple, aux_processor_tuples,raw_data_directory, sync_channel=1):\n",
    "\n",
    "    session_data = Session(str(raw_data_directory))\n",
    "    if len(session_data.recordnodes) != 1:\n",
    "        raise ValueError(\"should be exactly one record node.\")\n",
    "    if len(session_data.recordnodes[0].recordings) != 1:\n",
    "        raise ValueError(\"Should be exactly one recording.\")\n",
    "    for rn, recordnode in enumerate(session_data.recordnodes):\n",
    "        for r, recording in enumerate(recordnode.recordings):\n",
    "            # Sync\n",
    "            recording.add_sync_line(\n",
    "                sync_channel,\n",
    "                main_processor_tuple[0],\n",
    "                main_processor_tuple[1],\n",
    "                main=True,\n",
    "            )\n",
    "            for aux_processor in aux_processor_tuples:\n",
    "                recording.add_sync_line(\n",
    "                    sync_channel,\n",
    "                    aux_processor[0],\n",
    "                    aux_processor[1],\n",
    "                    main=False,\n",
    "                )\n",
    "            print('this should be zero:')\n",
    "            print(rn)\n",
    "        \n",
    "    return recording\n",
    "\n",
    "def check_if_probe_is_flipped(A_probes):\n",
    "\n",
    "    # Check if 'CP' appears before 'ccb or ccg' in the list of region acronyms, it shouldnt if the probe is the right way up\n",
    "    if 'CP' in A_probes['Region acronym'].values and 'ccb' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccb'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    elif 'CP' in A_probes['Region acronym'].values and 'ccg' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccg'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    else:\n",
    "        print('error')\n",
    "    return flipped\n",
    "\n",
    "def find_propotion_in_striatum(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        str_prop = callosum_middle_index/len(implant_df)\n",
    "    else:\n",
    "        str_prop = 1- callosum_middle_index/len(implant_df)\n",
    "    return str_prop\n",
    "\n",
    "\n",
    "def process_probe_channels(ProbeA_data,channels,channel_regions,current_mouse,output_path,var_string):\n",
    "    # pull out the data for each channel, bandpass to prevent aliasing then downsample\n",
    "         \n",
    "    # Parameters\n",
    "    lowcut = 20.0  # Lower cutoff frequency in Hz\n",
    "    highcut = 1250.0  # Upper cutoff frequency in Hz\n",
    "    fs = 30000.0  # Original sampling rate in Hz\n",
    "    downsample_factor = 12  # Factor by which to downsample\n",
    "    chunk_size = 2000  # Number of samples per chunk\n",
    "    # Create the bandpass filter\n",
    "    sos = create_bandpass_filter(lowcut, highcut, fs)\n",
    "    # Initialize the filter state\n",
    "    zi = sosfilt_zi(sos)\n",
    "\n",
    "    # Process each channel\n",
    "    for ind_,chosen_channel in enumerate(channels):\n",
    "        data_channel = []\n",
    "        for i in tqdm(range(0, len(ProbeA_data), chunk_size)):\n",
    "            # Extract the current chunk for the chosen channel\n",
    "            chunk = np.array([ProbeA_data[j][chosen_channel] for j in range(i, min(i + chunk_size, len(ProbeA_data)))])\n",
    "\n",
    "            # Apply the bandpass filter to the chunk\n",
    "            if i == 0:\n",
    "                # For the first chunk, use the initial filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi * chunk[0])\n",
    "            else:\n",
    "                # For subsequent chunks, use the updated filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi)\n",
    "\n",
    "            # Append the filtered chunk to the channel data\n",
    "            data_channel.extend(bp_chunk)\n",
    "\n",
    "        # Downsample the filtered data\n",
    "        data_downsampled = data_channel[::downsample_factor]\n",
    "        \n",
    "        # clean up for memory\n",
    "        del data_channel\n",
    "        \n",
    "        mouse_out_path = os.path.join(output_path,var_string) + current_mouse\n",
    "        if not os.path.exists(mouse_out_path):\n",
    "            os.makedirs(mouse_out_path)\n",
    "            \n",
    "        save_path = mouse_out_path + '//channel-' + str(chosen_channel) + '_REGION-' + channel_regions[ind_] + \"_LFP_data.npy\"\n",
    "        np.save(save_path,data_downsampled)\n",
    "        print('data saved for channel ' + str(chosen_channel))\n",
    "\n",
    "\n",
    "\n",
    "def gather_paths_for_old_data(current_mouse,path):\n",
    "    ### find OE processor path and OE_raw_path\n",
    "    OE_processor_path_base = r\"Z:\\projects\\sequence_squad\\data\\raw_neuropixel\\OE_DATA\\\\\"\n",
    "    if current_mouse.split('_')[1] == '2':\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] + '_' + 'implant' + current_mouse.split('_')[1]\n",
    "    else:\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] \n",
    "    for folder in os.listdir(OE_processor_path_base):\n",
    "        if mouse_folder == folder:\n",
    "            print(mouse_folder)\n",
    "            OE_processor_path = OE_processor_path_base + folder + '\\\\'\n",
    "\n",
    "    # next get the date \n",
    "    recording_date = path.split('\\\\')[-1].split('_')[-1]\n",
    "    reformatted_date = ''.join(recording_date.split('-')[0:-1]) + recording_date.split('-')[-1][-2::]\n",
    "    \n",
    "    recording_path_1 = None\n",
    "    for recording_date in os.listdir(OE_processor_path):\n",
    "        if reformatted_date == recording_date:\n",
    "            print(reformatted_date)\n",
    "            recording_path_1 = os.path.join(OE_processor_path,recording_date) + '\\\\'\n",
    "            \n",
    "    # set some OE paths I need \n",
    "    continuous_file_path = walk_through_files_countinous_data(recording_path_1)\n",
    "    processor_path = '\\\\'.join(continuous_file_path.split('\\\\')[0:-2])+ '\\\\'\n",
    "    OE_raw_path = os.path.join(recording_path_1,os.listdir(recording_path_1)[0]) + '\\\\'\n",
    "    return OE_raw_path,processor_path\n",
    "\n",
    "def find_folder_path(parent_folder, target_folder):\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        if target_folder in dirs:\n",
    "            return os.path.join(root, target_folder)\n",
    "        # If the target folder is not found\n",
    "    return (print('not found'))\n",
    "\n",
    "\n",
    "def gather_paths_for_new_data(mir):\n",
    "\n",
    "    ## gather all raw ephys paths\n",
    "    path_ = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\\"\n",
    "    base_recording_paths = []\n",
    "    for q in os.listdir(path_):\n",
    "        if not 'other_sessions' in q:\n",
    "            if not 'sp5_recordings' in q:\n",
    "                folder = os.path.join(path_,q)\n",
    "                for q in os.listdir(folder):\n",
    "                    base_recording_paths+=[os.path.join(folder,q)]\n",
    "                \n",
    "\n",
    "    # gather the corespoding organised paths for each raw dat file \n",
    "    organised_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "    full_organised_paths = []\n",
    "    mirs = []\n",
    "    for i in range(len(base_recording_paths)):\n",
    "        mouse_id = base_recording_paths[i].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths[i].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "        \n",
    "        organised_path_animal = os.path.join(organised_path,mouse_id+'_implant1')\n",
    "\n",
    "        for recording in os.listdir(organised_path_animal):\n",
    "            if date_ in recording:\n",
    "                full_organised_paths += [os.path.join(organised_path_animal,recording)]\n",
    "                mirs += [mouse_id + '_1_' + recording.split('_')[0].split('g')[-1]]\n",
    "                break\n",
    "\n",
    "\n",
    "    for index in range(len(base_recording_paths)):\n",
    "        \n",
    "        mouse_id = base_recording_paths[index].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths[index].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "        \n",
    "        \n",
    "        probeB = False\n",
    "        if mirs[index] == mir:\n",
    "            print(mir)\n",
    "            print(index)\n",
    "            \n",
    "            # set important paths\n",
    "            raw_data_directory = base_recording_paths[index]\n",
    "            print(raw_data_directory)\n",
    "            OE_processor_path = find_folder_path(raw_data_directory, \"continuous\") \n",
    "            Behav_data_path = full_organised_paths[index]+ r'//behav_sync/2_task/Preprocessed//'\n",
    "\n",
    "            Processed_Ephys_data_path_PROBEA = full_organised_paths[index]+ r'/ephys//' + r'probeA/kilosort4_output/sorter_output//'\n",
    "            if 'probeB' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                print('Probe B found')\n",
    "                Processed_Ephys_data_path_PROBEB = full_organised_paths[index]+ r'/ephys//' + r'probeB/kilosort4_output/sorter_output//'\n",
    "                probeB = True\n",
    "            \n",
    "            organised_ephys_path = full_organised_paths[index]+ r'/ephys//'\n",
    "            \n",
    "            for vid_file in os.listdir(full_organised_paths[index] + r'\\video\\videos\\\\'):\n",
    "                if 'BACK' in vid_file:\n",
    "                    if 'avi' in vid_file:\n",
    "                        back_video_path = os.path.join(full_organised_paths[index] + r'\\video\\videos\\\\',vid_file)\n",
    "\n",
    "                    \n",
    "            if 'probeA' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeA'):\n",
    "                    if probeB: \n",
    "                        if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeB'):\n",
    "                            print('All good! Data is kilosorted for PROBE A and PROBE B ')\n",
    "                            \n",
    "                            print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                            print(raw_data_directory)\n",
    "                            print(OE_processor_path)\n",
    "                            print(Behav_data_path)\n",
    "                            print(Processed_Ephys_data_path_PROBEA)\n",
    "                            print(back_video_path)\n",
    "                            break\n",
    "                        else:\n",
    "                            print('PROBE B data not yet kilosorted, skip!')\n",
    "                    else:\n",
    "                        print('All good! Data is kilosorted for PROBE A')\n",
    "                        print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                        print(raw_data_directory)\n",
    "                        print(OE_processor_path)\n",
    "                        print(Behav_data_path)\n",
    "                        print(Processed_Ephys_data_path_PROBEA)\n",
    "                        print(back_video_path)\n",
    "                        break\n",
    "                        \n",
    "                else:\n",
    "                    print('data not yet kilosorted, skip!')\n",
    "            else:\n",
    "                print('data not yet kilosorted, skip!')\n",
    "                \n",
    "    return raw_data_directory, OE_processor_path\n",
    "\n",
    "\n",
    "def find_histology_paths(mouse_id,ProbeB,shanks,overall_run_index):\n",
    "\n",
    "    # load the brainreg positions for the probe that was used in the recoridng (based on the manual labelling)\n",
    "    brainreg_base_path = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\serial_section\\brainreg_output\\brainreg\\\\\"\n",
    "\n",
    "    for mouse_file in os.listdir(brainreg_base_path):\n",
    "        if mouse_file in mouse_id:\n",
    "            print(mouse_file)\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        elif mouse_file in mouse_id.lower():\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        else:\n",
    "            b_reg_path = None\n",
    "            \n",
    "    shank = shanks[overall_run_index]\n",
    "    if len([f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]) > 0:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]\n",
    "    else:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeA_{shank}' in f and f.endswith('.csv')]\n",
    "        \n",
    "    # Load the CSV files into dataframes\n",
    "    A_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeA_csv_files]\n",
    "\n",
    "    if ProbeB:\n",
    "        if len([f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]) > 0:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]\n",
    "        else:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeB' in f and f.endswith('.csv')]\n",
    "        B_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeB_csv_files]  \n",
    "        \n",
    "        return A_probes, B_probes\n",
    "    else:\n",
    "        return A_probes\n",
    "    \n",
    "    \n",
    "def find_propotion_new_data(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "    boundary_um = implant_df['Distance from first position [um]'][callosum_middle_index]\n",
    "    full_length = max(implant_df['Distance from first position [um]'].values)\n",
    "\n",
    "    # 2.0 probes have 2 electrodes per bank and 15um spacing between banks\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        channel_boundary = int(boundary_um /15) *2\n",
    "    else:\n",
    "        channel_boundary = int((full_length-boundary_um)/15)*2\n",
    "\n",
    "    return channel_boundary, int(full_length/15)*2\n",
    "        \n",
    "def plot_boundary(first_cortex_channel,total_chans):\n",
    "    fig,ax = plt.subplots(1,1,figsize = (1,3))\n",
    "    ax.plot([0,0],[0,total_chans],'-')\n",
    "    ax.plot(first_cortex_channel,'o')\n",
    "    ax.set_title('srtr-cortex boundary')\n",
    "    \n",
    "def channels_to_process(output_path,channels,current_mouse,str_var):\n",
    "    processed_channel = []\n",
    "    # if it doesnt exist then just process all, if it does exist check which channels exist. \n",
    "    if os.path.exists(os.path.join(output_path, str_var) + current_mouse):\n",
    "        for file in os.listdir(os.path.join(output_path, str_var) + current_mouse):\n",
    "            if not 'probe' in file:\n",
    "                processed_channel += [int(file.split('_')[0].split('-')[-1])]\n",
    "        to_process = []\n",
    "        for channel in channels:\n",
    "            if not channel in processed_channel:\n",
    "                to_process += [channel]\n",
    "        if len(to_process) == 0:\n",
    "            process = False\n",
    "        else:\n",
    "            process = True\n",
    "        return process,to_process\n",
    "    else:\n",
    "        process = True \n",
    "        to_process = channels\n",
    "        return process,to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cae777",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9948fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new place to store the data I am going to use\n",
    "\n",
    "# pull out data path\n",
    "# if its a dir with probe A and B then it has no LFP so i need to dwonsample. \n",
    "\n",
    "# make sure the data is aligned properly. \n",
    "\n",
    "# save out the extracted LFP data to a new location in the revision data folder\n",
    "\n",
    "# it might be worth loading in and saving out the replay events for each session as well here, just so i have everything nice and together\n",
    "\n",
    "# this will make it easier to load in the data for the actual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "31f6b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to move the new lars ephys data to make this work better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa1093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording1_16-11-2024\n",
      "ap5R_1_1\n",
      "16\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\ap5R_2024-11-16_12-22-37\n",
      "All good! Data is kilosorted for PROBE A\n",
      "['global-timstamps_event-df.pkl', 'main_continuous_global_ts_probeA.npy', 'main_continuous_global_ts_probeA_LFP.npy', 'probeA']\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\ap5R_2024-11-16_12-22-37\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\ap5R_2024-11-16_12-22-37\\Record Node 105\\experiment1\\recording1\\continuous\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording1_16-11-2024//behav_sync/2_task/Preprocessed//\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording1_16-11-2024/ephys//probeA/kilosort4_output/sorter_output//\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\ap5R_implant1\\recording1_16-11-2024\\video\\videos\\\\BACK_CAM_AP5_2_R_16-11-2024.avi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m main_processor_tuple,aux_processor_tuples \u001b[38;5;241m=\u001b[39m find_processor_tuples(processor_path)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#!## LOAD in ephys data: WARNING V SLOW - this could take a few minutes ############!#\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m recording \u001b[38;5;241m=\u001b[39m \u001b[43malign_open_ephys_processors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_processor_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43maux_processor_tuples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mOE_raw_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m recording\u001b[38;5;241m.\u001b[39mcompute_global_timestamps()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#!##################################################################################!#\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m## Save this out:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[167], line 77\u001b[0m, in \u001b[0;36malign_open_ephys_processors\u001b[1;34m(main_processor_tuple, aux_processor_tuples, raw_data_directory, sync_channel)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21malign_open_ephys_processors\u001b[39m(main_processor_tuple, aux_processor_tuples,raw_data_directory, sync_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 77\u001b[0m     session_data \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_data_directory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(session_data\u001b[38;5;241m.\u001b[39mrecordnodes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be exactly one record node.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\session.py:65\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, directory, mmap_timestamps)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m=\u001b[39m directory;\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_timestamps \u001b[38;5;241m=\u001b[39m mmap_timestamps;\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_record_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\session.py:83\u001b[0m, in \u001b[0;36mSession._detect_record_nodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecordings \u001b[38;5;241m=\u001b[39m RecordNode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_timestamps)\u001b[38;5;241m.\u001b[39mrecordings\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecordnodes \u001b[38;5;241m=\u001b[39m [\u001b[43mRecordNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmap_timestamps\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m recordnodepaths]\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\recordnode.py:64\u001b[0m, in \u001b[0;36mRecordNode.__init__\u001b[1;34m(self, directory, mmap_timestamps)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m=\u001b[39m directory\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_format()\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_recordings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmmap_timestamps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\recordnode.py:89\u001b[0m, in \u001b[0;36mRecordNode._detect_recordings\u001b[1;34m(self, mmap_timestamps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_detect_recordings\u001b[39m(\u001b[38;5;28mself\u001b[39m, mmap_timestamps):\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    Internal method used to detect Recordings upon initialization\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecordings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_recordings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_timestamps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\formats\\BinaryRecording.py:319\u001b[0m, in \u001b[0;36mBinaryRecording.detect_recordings\u001b[1;34m(directory, mmap_timestamps)\u001b[0m\n\u001b[0;32m    315\u001b[0m     recording_directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39malphanum_key)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m recording_index, recording_directory \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(recording_directories):\n\u001b[1;32m--> 319\u001b[0m         recordings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mBinaryRecording\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mexperiment_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mrecording_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmmap_timestamps\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recordings\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\formats\\BinaryRecording.py:148\u001b[0m, in \u001b[0;36mBinaryRecording.__init__\u001b[1;34m(self, directory, experiment_index, recording_index, mmap_timestamps)\u001b[0m\n\u001b[0;32m    145\u001b[0m Recording\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory, experiment_index, recording_index, mmap_timestamps)  \n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstructure.oebin\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m oebin_file:\n\u001b[1;32m--> 148\u001b[0m      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moebin_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGUI version\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m2\u001b[39m]))\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define paths:\n",
    "\n",
    "## old data \n",
    "# data_path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\'\n",
    "# animals  = ['136_1_3', '136_1_4', '149_1_1', '149_1_2', '149_1_3','178_1_4', '178_1_5', '178_1_6', '178_1_7', '178_1_8','178_1_9', '178_2_1', '178_2_2', '178_2_4', '268_1_10','269_1_4', '269_1_7', '270_1_5', '270_1_6','270_1_7']\n",
    "## done : , \n",
    "## something wrong with '148_2_2','162_1_3',\n",
    "\n",
    "## new data \n",
    "data_path = r'Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\'\n",
    "animals = ['ap5R_1_1','ap5R_1_2','ap5R_1_3','seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11','seq008_1_3']\n",
    "shanks = [1,1,1,3,3,3,3,3,3,3,3,2,1,4,3]\n",
    "\n",
    "\n",
    "output_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\oscillations\\\\\"\n",
    "replace_files = False\n",
    "\n",
    "# loop through all the animals\n",
    "for overall_run_index, animal in enumerate(animals):\n",
    "\n",
    "    # list out all possible datapaths\n",
    "    data_paths = list_all_datapaths(data_path) \n",
    "\n",
    "    for path in data_paths:\n",
    "        if 'EJT' in path:\n",
    "            a = path.split('\\\\')[-2].split('_')[0][3::]\n",
    "        else:\n",
    "            a = path.split('\\\\')[-2].split('_')[0]\n",
    "        b = path.split('\\\\')[-2][-1]\n",
    "        c = path.split('\\\\')[-1].split('_')[0].split('g')[-1]\n",
    "        current_mouse = '_'.join([a,b,c])\n",
    "        if current_mouse != animal:\n",
    "            continue\n",
    "        print('-------------------------------------')\n",
    "        print()\n",
    "        print(path)\n",
    "        organised_ephys_path = os.path.join(path, r'ephys\\\\')\n",
    "        \n",
    "        # get the recording folder\n",
    "        mouse_folder = None\n",
    "        if 'EJT' in path:\n",
    "            OE_raw_path,processor_path = gather_paths_for_old_data(current_mouse,path)\n",
    "        else:\n",
    "            OE_raw_path, processor_path = gather_paths_for_new_data(current_mouse)\n",
    "            \n",
    "        # find processor tuples\n",
    "        main_processor_tuple,aux_processor_tuples = find_processor_tuples(processor_path)\n",
    "        \n",
    "        #!## LOAD in ephys data: WARNING V SLOW - this could take a few minutes ############!#\n",
    "        \n",
    "        recording = align_open_ephys_processors(main_processor_tuple,aux_processor_tuples,OE_raw_path)\n",
    "        recording.compute_global_timestamps()\n",
    "        \n",
    "        #!##################################################################################!#\n",
    "        \n",
    "        ## Save this out:\n",
    "        print('saving...')\n",
    "        if not os.path.isdir(organised_ephys_path):\n",
    "            os.makedirs(organised_ephys_path)\n",
    "            \n",
    "        # check to see if global object alreayd exists and if it does no need to save\n",
    "        if process_probe_data_bool(organised_ephys_path,aux_processor_tuples) == False:\n",
    "            save_path = organised_ephys_path + f\"global-timstamps_event-df.pkl\"\n",
    "            recording.events.to_pickle(save_path)\n",
    "        \n",
    "        ## Extract timestamp data:\n",
    "        events_df= recording.events\n",
    "\n",
    "        ## extract the main npx continuous data:\n",
    "        # work out the data ind\n",
    "        probeA_data_index = None\n",
    "        probeB_data_index = None\n",
    "        ProbeB = False\n",
    "        if 'EJT' in path:\n",
    "            for index, item in enumerate(os.listdir(processor_path)):\n",
    "                meta_data = recording.continuous[index].metadata\n",
    "                if 'AP' in meta_data['channel_names'][0]:\n",
    "                    probeA_data_index = index\n",
    "        else:\n",
    "            stream_names = []\n",
    "            for index, item in enumerate(os.listdir(processor_path)):   \n",
    "                meta_data = recording.continuous[index].metadata\n",
    "                stream_names += [meta_data['stream_name']]\n",
    "\n",
    "            print(stream_names)\n",
    "            stream_names = np.array(stream_names)\n",
    "            if 'ProbeA-AP' in stream_names:\n",
    "                probeA_data_index = np.where(stream_names == 'ProbeA-AP')[0][0]\n",
    "            elif 'ProbeA' in stream_names:\n",
    "                probeA_data_index = np.where(stream_names == 'ProbeA')[0][0]\n",
    "            if 'ProbeB' in stream_names:\n",
    "                ProbeB = True\n",
    "                probeB_data_index = np.where(stream_names == 'ProbeB')[0][0]\n",
    "         \n",
    "        ProbeA_data = recording.continuous[probeA_data_index].samples\n",
    "        if ProbeB:\n",
    "            ProbeB_data = recording.continuous[probeB_data_index].samples\n",
    "            \n",
    "        #choose 8 channels \n",
    "        channels = [50,90,130,170,210,250,290,340]\n",
    "        \n",
    "        # check if the data has alreay been processed\n",
    "        B_process = False\n",
    "        A_process = False\n",
    "        A_chans_to_process = []\n",
    "        B_chans_to_process = []\n",
    "        if not replace_files:\n",
    "            A_process,A_chans_to_process = channels_to_process(output_path,channels,current_mouse,'striatum_lfp\\\\')\n",
    "            if ProbeB: \n",
    "                B_process,B_chans_to_process = channels_to_process(output_path,channels,current_mouse,'hippocampus_lfp\\\\')  \n",
    "        else:\n",
    "            A_chans_to_process = channels \n",
    "            B_chans_to_process = channels\n",
    "            A_process = True\n",
    "            B_process = True   \n",
    "            \n",
    "        if B_process == True or A_process == True:\n",
    "            \n",
    "            # import the histology data\n",
    "            if 'EJT' in path:\n",
    "                brainreg_base_path = r\"Z:\\projects\\sequence_squad\\data\\histology\\Neuropixel_tracks\\\\\"\n",
    "                for mouse_name in os.listdir(brainreg_base_path):\n",
    "                    if current_mouse.split('_')[0] in mouse_name:\n",
    "                        hist_path_base = os.path.join(brainreg_base_path,mouse_name)\n",
    "                        hist_path = os.path.join((hist_path_base),\"brainreg\\\\\")\n",
    "                        hist_path = glob.glob(os.path.join(hist_path, '**','tracks'),recursive = True)[0]\n",
    "                        # load the data\n",
    "                        implant_files = []\n",
    "                        for item in os.listdir(hist_path):\n",
    "                            if 'csv' in item:\n",
    "                                implant_files+=[item]\n",
    "\n",
    "                        if len(implant_files) > 1:\n",
    "                            for file in implant_files:\n",
    "                                if current_mouse.split('_')[-2] in file:\n",
    "                                    implant_file = file\n",
    "                        else:\n",
    "                            implant_file = implant_files[0]\n",
    "\n",
    "                        probe_track_file = os.path.join(hist_path,implant_file) \n",
    "                        print(probe_track_file)\n",
    "                        implant_df = pd.read_csv(probe_track_file)\n",
    "                        \n",
    "                        # this is only needed for probe A:\n",
    "                        striatum_proportion = find_propotion_in_striatum(implant_df)\n",
    "                        # there should be 400 channels per 4000um (what i implanted), tot_channels = 384, bank_spacing = 20 # 20um, channels_per_bank = 2\n",
    "                        first_cortex_channel = int(striatum_proportion * 400)\n",
    "                        total_chans = 400\n",
    "                        plot_boundary(first_cortex_channel,total_chans)\n",
    "                        print(first_cortex_channel)\n",
    "                                            \n",
    "            else:\n",
    "                if ProbeB:\n",
    "                    implant_dfs_A,implant_dfs_B = find_histology_paths(current_mouse,ProbeB,shanks,overall_run_index)\n",
    "                else:\n",
    "                    implant_dfs_A = find_histology_paths(current_mouse,ProbeB,shanks,overall_run_index)\n",
    "                first_cortex_channel,total_chans = find_propotion_new_data(implant_dfs_A[0])\n",
    "                plot_boundary(first_cortex_channel,total_chans)\n",
    "            \n",
    "            if A_process:\n",
    "                #label the channels   \n",
    "                channel_regions = []\n",
    "                for channel in A_chans_to_process:\n",
    "                    if channel >= first_cortex_channel:\n",
    "                        channel_regions.append('m_crtex')\n",
    "                    elif channel < first_cortex_channel:\n",
    "                        channel_regions.append('striatum')\n",
    "                # process the data for probe A\n",
    "                process_probe_channels(ProbeA_data,A_chans_to_process,channel_regions,current_mouse,output_path,'striatum_lfp\\\\')\n",
    "                \n",
    "                # save out the timestamp data\n",
    "                Fs = 30000\n",
    "                samples = len(ProbeA_data)\n",
    "                timestamps_index = np.linspace(0,samples,samples+1).astype(int)\n",
    "                timestamps_index_downsampled = timestamps_index[::12]\n",
    "                timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "                downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "                mouse_out_path = os.path.join(output_path, 'striatum_lfp\\\\') + current_mouse + '\\\\'\n",
    "                downsampled_timstamp_df.to_csv(mouse_out_path + 'probeA_timestamps.csv')\n",
    "                \n",
    "            # same for probe B\n",
    "            if B_process:\n",
    "                #label the channels for probe B  \n",
    "                probeB_channel_regions = ['hippocampus'] * len(B_chans_to_process)\n",
    "                process_probe_channels(ProbeB_data,B_chans_to_process,probeB_channel_regions,current_mouse,output_path,'hippocampus_lfp\\\\')\n",
    "                \n",
    "                # save out timestamp data \n",
    "                samples = len(ProbeB_data)\n",
    "                timestamps_index = np.linspace(0,samples,samples+1).astype(int)\n",
    "                timestamps_index_downsampled = timestamps_index[::12]\n",
    "                timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "                downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "                mouse_out_path = os.path.join(output_path, 'hippocampus_lfp\\\\') + current_mouse + '\\\\'\n",
    "                downsampled_timstamp_df.to_csv(mouse_out_path + 'probeB_timestamps.csv')\n",
    "        else:\n",
    "            print(f'all data already processed for {current_mouse}')\n",
    "                \n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21129302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5c230d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
