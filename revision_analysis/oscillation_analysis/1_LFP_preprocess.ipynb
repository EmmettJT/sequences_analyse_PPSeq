{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f35f3d",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd41257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from open_ephys.analysis import Session\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob \n",
    "from scipy.signal import butter, sosfilt, sosfilt_zi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Define the lowpass filter\n",
    "def create_lowpass_filter(highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    high = highcut / nyquist\n",
    "    sos = butter(order, high, btype='low', output='sos')\n",
    "    return sos\n",
    "    \n",
    "def list_all_datapaths(data_path):\n",
    "    data_paths = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if 'EJT' in file[0:5] or 'revision' in data_path:\n",
    "            for file_ in os.listdir(data_path+file):\n",
    "                if 'record' in file_:\n",
    "                    data_paths += [data_path+file+'\\\\'+file_]\n",
    "            \n",
    "    return data_paths\n",
    "\n",
    "def walk_through_files_countinous_data(recording_path_1):\n",
    "    continuous_file_path = None\n",
    "\n",
    "    for root, dirs, files in os.walk(recording_path_1):\n",
    "        if 'continuous.dat' in files:\n",
    "            continuous_file_path = os.path.join(root, 'continuous.dat')\n",
    "            break\n",
    "\n",
    "    if continuous_file_path:\n",
    "        print(f\"Found 'continuous.dat' at: {continuous_file_path}\")\n",
    "        return continuous_file_path\n",
    "    else:\n",
    "        print(\"'continuous.dat' not found in the specified path.\")\n",
    "        \n",
    "def find_processor_tuples(processor_path):\n",
    "    count = 0\n",
    "    for processor in os.listdir(processor_path):\n",
    "        if count == 0:\n",
    "            main1 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main1_2 = processor.split('.')[-1]\n",
    "        elif count == 1:\n",
    "            main2 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main2_2 = processor.split('.')[-1]\n",
    "        elif count == 2:\n",
    "            main3 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main3_2 = processor.split('.')[-1]\n",
    "        count +=1 \n",
    "\n",
    "    main_processor_tuple=(main1, main1_2)\n",
    "\n",
    "    aux_processor_tuples=((main2,main2_2),(main3,main3_2))\n",
    "    return main_processor_tuple,aux_processor_tuples\n",
    "\n",
    "def process_probe_data_bool(organised_ephys_path,aux_processor_tuples):\n",
    "    process = False\n",
    "    if not 'global-timstamps_event-df.pkl' in os.listdir(organised_ephys_path):\n",
    "        if not 'main_continuous_global_ts_probeA.npy' in os.listdir(organised_ephys_path):\n",
    "            if not 'LFP' in aux_processor_tuples[0][-1]:\n",
    "                if not 'main_continuous_global_ts_probeA_LFP.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "            else:\n",
    "                if not 'main_continuous_global_ts_probeB.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "    return process\n",
    "\n",
    "## new version for new open ephys tools \n",
    "def align_open_ephys_processors(main_processor_tuple, aux_processor_tuples,raw_data_directory, sync_channel=1):\n",
    "\n",
    "    session_data = Session(str(raw_data_directory))\n",
    "    if len(session_data.recordnodes) != 1:\n",
    "        raise ValueError(\"should be exactly one record node.\")\n",
    "    if len(session_data.recordnodes[0].recordings) != 1:\n",
    "        raise ValueError(\"Should be exactly one recording.\")\n",
    "    for rn, recordnode in enumerate(session_data.recordnodes):\n",
    "        for r, recording in enumerate(recordnode.recordings):\n",
    "            # Sync\n",
    "            recording.add_sync_line(\n",
    "                sync_channel,\n",
    "                main_processor_tuple[0],\n",
    "                main_processor_tuple[1],\n",
    "                main=True,\n",
    "            )\n",
    "            for aux_processor in aux_processor_tuples:\n",
    "                recording.add_sync_line(\n",
    "                    sync_channel,\n",
    "                    aux_processor[0],\n",
    "                    aux_processor[1],\n",
    "                    main=False,\n",
    "                )\n",
    "            print('this should be zero:')\n",
    "            print(rn)\n",
    "        \n",
    "    return recording\n",
    "\n",
    "def check_if_probe_is_flipped(A_probes):\n",
    "\n",
    "    # Check if 'CP' appears before 'ccb or ccg' in the list of region acronyms, it shouldnt if the probe is the right way up\n",
    "    if 'CP' in A_probes['Region acronym'].values and 'ccb' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccb'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    elif 'CP' in A_probes['Region acronym'].values and 'ccg' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccg'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    else:\n",
    "        print('error')\n",
    "    return flipped\n",
    "\n",
    "def find_propotion_in_striatum(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        str_prop = callosum_middle_index/len(implant_df)\n",
    "    else:\n",
    "        str_prop = 1- callosum_middle_index/len(implant_df)\n",
    "    return str_prop\n",
    "\n",
    "\n",
    "def process_probe_channels(ProbeA_data,channels,channel_regions,current_mouse,output_path,var_string,highcut_value):\n",
    "    # pull out the data for each channel, lowpass to prevent aliasing (stops the high frequency stuff from folding back onto intself and poluting the low frequency range) then downsample\n",
    "         \n",
    "    # Parameters\n",
    "    highcut = highcut_value  # Upper cutoff frequency in Hz\n",
    "    fs = 30000.0  # Original sampling rate in Hz\n",
    "    downsample_factor = 12  # Factor by which to downsample\n",
    "    chunk_size = 2000  # Number of samples per chunk\n",
    "    # Create the bandpass filter\n",
    "    sos = create_lowpass_filter(highcut, fs, order=4)\n",
    "    \n",
    "    # Initialize the filter state\n",
    "    zi = sosfilt_zi(sos)\n",
    "\n",
    "    # Process each channel\n",
    "    for ind_,chosen_channel in enumerate(channels):\n",
    "        data_channel = []\n",
    "        for i in tqdm(range(0, len(ProbeA_data), chunk_size)):\n",
    "            # Extract the current chunk for the chosen channel\n",
    "            chunk = np.array([ProbeA_data[j][chosen_channel] for j in range(i, min(i + chunk_size, len(ProbeA_data)))])\n",
    "\n",
    "            # Apply the bandpass filter to the chunk\n",
    "            if i == 0:\n",
    "                # For the first chunk, use the initial filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi * chunk[0])\n",
    "            else:\n",
    "                # For subsequent chunks, use the updated filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi)\n",
    "\n",
    "            # Append the filtered chunk to the channel data\n",
    "            data_channel.extend(bp_chunk)\n",
    "\n",
    "        # Downsample the filtered data\n",
    "        data_downsampled = data_channel[::downsample_factor]\n",
    "        \n",
    "        # clean up for memory\n",
    "        del data_channel\n",
    "        \n",
    "        mouse_out_path = os.path.join(output_path,var_string) + current_mouse\n",
    "        if not os.path.exists(mouse_out_path):\n",
    "            os.makedirs(mouse_out_path)\n",
    "            \n",
    "        save_path = mouse_out_path + '//channel-' + str(chosen_channel) + '_REGION-' + channel_regions[ind_] + \"_LFP_data.npy\"\n",
    "        np.save(save_path,data_downsampled)\n",
    "        print('data saved for channel ' + str(chosen_channel))\n",
    "\n",
    "\n",
    "\n",
    "def gather_paths_for_old_data(current_mouse,path):\n",
    "    ### find OE processor path and OE_raw_path\n",
    "    OE_processor_path_base = r\"Z:\\projects\\sequence_squad\\data\\raw_neuropixel\\OE_DATA\\\\\"\n",
    "    if current_mouse.split('_')[1] == '2':\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] + '_' + 'implant' + current_mouse.split('_')[1]\n",
    "    else:\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] \n",
    "    for folder in os.listdir(OE_processor_path_base):\n",
    "        if mouse_folder == folder:\n",
    "            print(mouse_folder)\n",
    "            OE_processor_path = OE_processor_path_base + folder + '\\\\'\n",
    "\n",
    "    # next get the date \n",
    "    recording_date = path.split('\\\\')[-1].split('_')[-1]\n",
    "    reformatted_date = ''.join(recording_date.split('-')[0:-1]) + recording_date.split('-')[-1][-2::]\n",
    "    \n",
    "    recording_path_1 = None\n",
    "    for recording_date in os.listdir(OE_processor_path):\n",
    "        if reformatted_date == recording_date:\n",
    "            print(reformatted_date)\n",
    "            recording_path_1 = os.path.join(OE_processor_path,recording_date) + '\\\\'\n",
    "            \n",
    "    # set some OE paths I need \n",
    "    continuous_file_path = walk_through_files_countinous_data(recording_path_1)\n",
    "    processor_path = '\\\\'.join(continuous_file_path.split('\\\\')[0:-2])+ '\\\\'\n",
    "    OE_raw_path = os.path.join(recording_path_1,os.listdir(recording_path_1)[0]) + '\\\\'\n",
    "    return OE_raw_path,processor_path\n",
    "\n",
    "def find_folder_path(parent_folder, target_folder):\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        if target_folder in dirs:\n",
    "            return os.path.join(root, target_folder)\n",
    "        # If the target folder is not found\n",
    "    return (print('not found'))\n",
    "\n",
    "\n",
    "def gather_paths_for_new_data(mir):\n",
    "\n",
    "    # gather the raw ephys paths for each mouse\n",
    "    path_ = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\\"\n",
    "    base_recording_paths = []\n",
    "    for q in os.listdir(path_):\n",
    "        if not 'other_sessions' in q:\n",
    "            if not 'sp5_recordings' in q:\n",
    "                folder = os.path.join(path_,q)\n",
    "                for q in os.listdir(folder):\n",
    "                    if '2024' in q or '2025' in q:\n",
    "                        base_recording_paths+=[os.path.join(folder,q)]\n",
    "                    else:\n",
    "                        folder_layer = os.path.join(folder,q)\n",
    "                        for r in os.listdir(folder_layer):\n",
    "                            base_recording_paths+=[os.path.join(folder,r)]\n",
    "            \n",
    "\n",
    "    # gather the corespoding organised paths for each raw dat file \n",
    "    organised_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "    full_organised_paths = []\n",
    "    base_recording_paths_keep = []\n",
    "    mirs = []\n",
    "    for i in range(len(base_recording_paths)):\n",
    "        mouse_id = base_recording_paths[i].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths[i].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "\n",
    "        if os.path.exists(os.path.join(organised_path,mouse_id+'_implant1')):\n",
    "            organised_path_animal = os.path.join(organised_path,mouse_id+'_implant1')\n",
    "            base_recording_paths_keep += [base_recording_paths[i]]\n",
    "        else:\n",
    "            # full_organised_paths + ['recording_not_processed_yet']\n",
    "            # mirs += ['recording_not_processed_yet']\n",
    "            continue\n",
    "            \n",
    "        for recording in os.listdir(organised_path_animal):\n",
    "            if date_ in recording:\n",
    "                full_organised_paths += [os.path.join(organised_path_animal,recording)]\n",
    "                mirs += [mouse_id + '_1_' + recording.split('_')[0].split('g')[-1]]\n",
    "                break\n",
    "\n",
    "\n",
    "    for index in range(len(base_recording_paths_keep)):\n",
    "        \n",
    "        mouse_id = base_recording_paths_keep[index].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths_keep[index].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "        \n",
    "        \n",
    "        probeB = False\n",
    "        if mirs[index] == mir:\n",
    "            print(mir)\n",
    "            print(index)\n",
    "            \n",
    "            # set important paths\n",
    "            raw_data_directory = base_recording_paths_keep[index]\n",
    "            print(raw_data_directory)\n",
    "            OE_processor_path = find_folder_path(raw_data_directory, \"continuous\") \n",
    "            Behav_data_path = full_organised_paths[index]+ r'//behav_sync/2_task/Preprocessed//'\n",
    "\n",
    "            Processed_Ephys_data_path_PROBEA = full_organised_paths[index]+ r'/ephys//' + r'probeA/kilosort4_output/sorter_output//'\n",
    "            if 'probeB' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                print('Probe B found')\n",
    "                Processed_Ephys_data_path_PROBEB = full_organised_paths[index]+ r'/ephys//' + r'probeB/kilosort4_output/sorter_output//'\n",
    "                probeB = True\n",
    "            \n",
    "            organised_ephys_path = full_organised_paths[index]+ r'/ephys//'\n",
    "            \n",
    "            for vid_file in os.listdir(full_organised_paths[index] + r'\\video\\videos\\\\'):\n",
    "                if 'BACK' in vid_file:\n",
    "                    if 'avi' in vid_file:\n",
    "                        back_video_path = os.path.join(full_organised_paths[index] + r'\\video\\videos\\\\',vid_file)\n",
    "\n",
    "                    \n",
    "            if 'probeA' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeA'):\n",
    "                    if probeB: \n",
    "                        if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeB'):\n",
    "                            print('All good! Data is kilosorted for PROBE A and PROBE B ')\n",
    "                            \n",
    "                            print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                            print(raw_data_directory)\n",
    "                            print(OE_processor_path)\n",
    "                            print(Behav_data_path)\n",
    "                            print(Processed_Ephys_data_path_PROBEA)\n",
    "                            print(back_video_path)\n",
    "                            break\n",
    "                        else:\n",
    "                            print('PROBE B data not yet kilosorted, skip!')\n",
    "                    else:\n",
    "                        print('All good! Data is kilosorted for PROBE A')\n",
    "                        print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                        print(raw_data_directory)\n",
    "                        print(OE_processor_path)\n",
    "                        print(Behav_data_path)\n",
    "                        print(Processed_Ephys_data_path_PROBEA)\n",
    "                        print(back_video_path)\n",
    "                        break\n",
    "                        \n",
    "                else:\n",
    "                    print('data not yet kilosorted, skip!')\n",
    "            else:\n",
    "                print('data not yet kilosorted, skip!')\n",
    "    return raw_data_directory, OE_processor_path\n",
    "\n",
    "\n",
    "def find_histology_paths(mouse_id,ProbeB,shank):\n",
    "\n",
    "    # load the brainreg positions for the probe that was used in the recoridng (based on the manual labelling)\n",
    "    brainreg_base_path = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\serial_section\\brainreg_output\\brainreg\\\\\"\n",
    "\n",
    "    for mouse_file in os.listdir(brainreg_base_path):\n",
    "        if mouse_file in mouse_id:\n",
    "            print(mouse_file)\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        elif mouse_file in mouse_id.lower():\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        else:\n",
    "            b_reg_path = None\n",
    "            \n",
    "    if len([f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]) > 0:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]\n",
    "    else:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeA_{shank}' in f and f.endswith('.csv')]\n",
    "        \n",
    "    # Load the CSV files into dataframes\n",
    "    A_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeA_csv_files]\n",
    "\n",
    "    if ProbeB:\n",
    "        if len([f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]) > 0:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]\n",
    "        else:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeB' in f and f.endswith('.csv')]\n",
    "        B_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeB_csv_files]  \n",
    "        \n",
    "        return A_probes, B_probes\n",
    "    else:\n",
    "        return A_probes\n",
    "    \n",
    "    \n",
    "def find_propotion_new_data(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "    boundary_um = implant_df['Distance from first position [um]'][callosum_middle_index]\n",
    "    full_length = max(implant_df['Distance from first position [um]'].values)\n",
    "\n",
    "    # 2.0 probes have 2 electrodes per bank and 15um spacing between banks\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        channel_boundary = int(boundary_um /15) *2\n",
    "    else:\n",
    "        channel_boundary = int((full_length-boundary_um)/15)*2\n",
    "\n",
    "    return channel_boundary, int(full_length/15)*2\n",
    "        \n",
    "def plot_boundary(first_cortex_channel,total_chans):\n",
    "    fig,ax = plt.subplots(1,1,figsize = (1,3))\n",
    "    ax.plot([0,0],[0,total_chans],'-')\n",
    "    ax.plot(first_cortex_channel,'o')\n",
    "    ax.set_title('srtr-cortex boundary')\n",
    "    \n",
    "def channels_to_process(output_path,channels,current_mouse,str_var):\n",
    "    processed_channel = []\n",
    "    # if it doesnt exist then just process all, if it does exist check which channels exist. \n",
    "    if os.path.exists(os.path.join(output_path, str_var) + current_mouse):\n",
    "        for file in os.listdir(os.path.join(output_path, str_var) + current_mouse):\n",
    "            if 'channel' in file:\n",
    "                processed_channel += [int(file.split('_')[0].split('-')[-1])]\n",
    "        to_process = []\n",
    "        for channel in channels:\n",
    "            if not channel in processed_channel:\n",
    "                to_process += [channel]\n",
    "        if len(to_process) == 0:\n",
    "            process = False\n",
    "        else:\n",
    "            process = True\n",
    "        return process,to_process\n",
    "    else:\n",
    "        process = True \n",
    "        to_process = channels\n",
    "        return process,to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "277a2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq007_1_1\n",
      "26\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\n",
      "Probe B found\n",
      "All good! Data is kilosorted for PROBE A and PROBE B \n",
      "['global-timstamps_event-df.pkl', 'main_continuous_global_ts_probeA.npy', 'main_continuous_global_ts_probeB.npy', 'probeA', 'probeB']\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\\Record Node 103\\experiment1\\recording1\\continuous\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024//behav_sync/2_task/Preprocessed//\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024/ephys//probeA/kilosort4_output/sorter_output//\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024\\video\\videos\\\\BACK_CAM_seq007_18-11-2024.avi\n",
      "wtf\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\n"
     ]
    }
   ],
   "source": [
    "mir = current_mouse\n",
    "\n",
    "\n",
    "# gather the raw ephys paths for each mouse\n",
    "path_ = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\\"\n",
    "base_recording_paths = []\n",
    "for q in os.listdir(path_):\n",
    "    if not 'other_sessions' in q:\n",
    "        if not 'sp5_recordings' in q:\n",
    "            folder = os.path.join(path_,q)\n",
    "            for q in os.listdir(folder):\n",
    "                if '2024' in q or '2025' in q:\n",
    "                    base_recording_paths+=[os.path.join(folder,q)]\n",
    "                else:\n",
    "                    folder_layer = os.path.join(folder,q)\n",
    "                    for r in os.listdir(folder_layer):\n",
    "                        base_recording_paths+=[os.path.join(folder,r)]\n",
    "        \n",
    "\n",
    "# gather the corespoding organised paths for each raw dat file \n",
    "organised_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "full_organised_paths = []\n",
    "base_recording_paths_keep = []\n",
    "mirs = []\n",
    "for i in range(len(base_recording_paths)):\n",
    "    mouse_id = base_recording_paths[i].split('\\\\')[-1].split('_')[0]\n",
    "    date_ = base_recording_paths[i].split('\\\\')[-1].split('_')[1]\n",
    "    #reverse the date\n",
    "    date_ = '-'.join(date_.split('-')[::-1])\n",
    "\n",
    "    if os.path.exists(os.path.join(organised_path,mouse_id+'_implant1')):\n",
    "        organised_path_animal = os.path.join(organised_path,mouse_id+'_implant1')\n",
    "        base_recording_paths_keep += [base_recording_paths[i]]\n",
    "    else:\n",
    "        # full_organised_paths + ['recording_not_processed_yet']\n",
    "        # mirs += ['recording_not_processed_yet']\n",
    "        continue\n",
    "        \n",
    "    for recording in os.listdir(organised_path_animal):\n",
    "        if date_ in recording:\n",
    "            full_organised_paths += [os.path.join(organised_path_animal,recording)]\n",
    "            mirs += [mouse_id + '_1_' + recording.split('_')[0].split('g')[-1]]\n",
    "            break\n",
    "\n",
    "\n",
    "for index in range(len(base_recording_paths_keep)):\n",
    "    \n",
    "    mouse_id = base_recording_paths_keep[index].split('\\\\')[-1].split('_')[0]\n",
    "    date_ = base_recording_paths_keep[index].split('\\\\')[-1].split('_')[1]\n",
    "    #reverse the date\n",
    "    date_ = '-'.join(date_.split('-')[::-1])\n",
    "    \n",
    "    \n",
    "    probeB = False\n",
    "    if mirs[index] == mir:\n",
    "        print(mir)\n",
    "        print(index)\n",
    "        \n",
    "        # set important paths\n",
    "        raw_data_directory = base_recording_paths_keep[index]\n",
    "        print(raw_data_directory)\n",
    "        OE_processor_path = find_folder_path(raw_data_directory, \"continuous\") \n",
    "        Behav_data_path = full_organised_paths[index]+ r'//behav_sync/2_task/Preprocessed//'\n",
    "\n",
    "        Processed_Ephys_data_path_PROBEA = full_organised_paths[index]+ r'/ephys//' + r'probeA/kilosort4_output/sorter_output//'\n",
    "        if 'probeB' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "            print('Probe B found')\n",
    "            Processed_Ephys_data_path_PROBEB = full_organised_paths[index]+ r'/ephys//' + r'probeB/kilosort4_output/sorter_output//'\n",
    "            probeB = True\n",
    "        \n",
    "        organised_ephys_path = full_organised_paths[index]+ r'/ephys//'\n",
    "        \n",
    "        for vid_file in os.listdir(full_organised_paths[index] + r'\\video\\videos\\\\'):\n",
    "            if 'BACK' in vid_file:\n",
    "                if 'avi' in vid_file:\n",
    "                    back_video_path = os.path.join(full_organised_paths[index] + r'\\video\\videos\\\\',vid_file)\n",
    "\n",
    "                \n",
    "        if 'probeA' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "            if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeA'):\n",
    "                if probeB: \n",
    "                    if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeB'):\n",
    "                        print('All good! Data is kilosorted for PROBE A and PROBE B ')\n",
    "                        \n",
    "                        print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                        print(raw_data_directory)\n",
    "                        print(OE_processor_path)\n",
    "                        print(Behav_data_path)\n",
    "                        print(Processed_Ephys_data_path_PROBEA)\n",
    "                        print(back_video_path)\n",
    "                        break\n",
    "                    else:\n",
    "                        print('PROBE B data not yet kilosorted, skip!')\n",
    "                else:\n",
    "                    print('All good! Data is kilosorted for PROBE A')\n",
    "                    print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                    print(raw_data_directory)\n",
    "                    print(OE_processor_path)\n",
    "                    print(Behav_data_path)\n",
    "                    print(Processed_Ephys_data_path_PROBEA)\n",
    "                    print(back_video_path)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                print('data not yet kilosorted, skip!')\n",
    "        else:\n",
    "            print('data not yet kilosorted, skip!')\n",
    "print('wtf')\n",
    "print(raw_data_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cae777",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9948fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new place to store the data I am going to use\n",
    "\n",
    "# pull out data path\n",
    "# if its a dir with probe A and B then it has no LFP so i need to dwonsample. \n",
    "\n",
    "# make sure the data is aligned properly. \n",
    "\n",
    "# save out the extracted LFP data to a new location in the revision data folder\n",
    "\n",
    "# it might be worth loading in and saving out the replay events for each session as well here, just so i have everything nice and together\n",
    "\n",
    "# this will make it easier to load in the data for the actual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55aeea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for second run:\n",
    "\n",
    "expert_animals = ['178_2_3','178_2_4','269_1_1','269_1_2','269_1_3','270_1_3','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11','seq008_1_3','seq007_1_4']\n",
    "expert_shanks = [1, 1, 1, 1, 1, 1,3,2,1,4,3,3]\n",
    "\n",
    "hlesion_animals = ['262_1_6']\n",
    "hlesion_shanks = [1]\n",
    "\n",
    "learning_animals = ['ap5R_1_1','ap5R_1_2','ap5R_1_3','seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq007_1_1','seq007_1_2','seq007_1_3']\n",
    "learning_shanks = [1,1,1,3,3,3,3,3,3,3,3,3,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e57fb5",
   "metadata": {},
   "source": [
    "# put all data into reference dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "639a5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_animals = ['136_1_3','136_1_4','149_1_1','178_1_4','178_1_5','178_1_6','178_1_7','178_1_8','178_1_9','178_2_1','178_2_2','178_2_3','178_2_4','269_1_1','269_1_2','269_1_3','269_1_4','269_1_7','270_1_3','270_1_5','270_1_6','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11','seq008_1_3','seq007_1_4']\n",
    "# expert_shanks = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,3,2,1,4,3,3]\n",
    "\n",
    "# hlesion_animals = ['256_1_1','255_1_1','255_1_2','255_1_4','262_1_1','262_1_2','262_1_4','262_1_5','262_1_6']\n",
    "# hlesion_shanks = [1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# learning_animals = ['268_1_2','269_1_1','269_1_2','269_1_3','270_1_1','270_1_3','ap5R_1_1','ap5R_1_2','ap5R_1_3','seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq007_1_1','seq007_1_2','seq007_1_3']\n",
    "# learning_shanks = [1,1,1,1,1,1,1,1,1,3,3,3,3,3,3,3,3,3,3]\n",
    "\n",
    "\n",
    "expert_animals = ['seq006_1_9','seq006_1_10','seq006_1_11']\n",
    "expert_shanks = [3,3,3]\n",
    "\n",
    "\n",
    "hlesion_animals = []\n",
    "hlesion_shanks = []\n",
    "\n",
    "learning_animals = ['seq006_1_4','seq007_1_1','seq007_1_2','seq007_1_3']\n",
    "learning_shanks = [3,3,3,3]\n",
    "\n",
    "type_ = ['expert'] * len(expert_animals) + ['learning'] * len(learning_animals) + ['hlesion'] * len(hlesion_animals) \n",
    "\n",
    "# make a dataframe to store the data in\n",
    "data_frame = pd.DataFrame({\n",
    "    'animal': expert_animals + learning_animals + hlesion_animals, \n",
    "    'type': type_,\n",
    "    'shank': expert_shanks + learning_shanks + hlesion_shanks })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54fa1093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap5lr_1_1\n",
      "ap5L_1_1\n",
      "ap5L_1_2\n",
      "ap5R_1_1\n",
      "ap5R_1_2\n",
      "ap5R_1_3\n",
      "ap5R_1_4\n",
      "ap5R_1_5\n",
      "ap5R_1_6\n",
      "ap5R_1_7\n",
      "seq006_1_10\n",
      "seq006_1_11\n",
      "seq006_1_1\n",
      "seq006_1_2\n",
      "seq006_1_3\n",
      "seq006_1_4\n",
      "seq006_1_5\n",
      "seq006_1_6\n",
      "seq006_1_7\n",
      "seq006_1_8\n",
      "seq006_1_9\n",
      "seq007_1_1\n",
      "-------------------------------------\n",
      "\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024\n",
      "seq007_1_1\n",
      "26\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\n",
      "Probe B found\n",
      "All good! Data is kilosorted for PROBE A and PROBE B \n",
      "['global-timstamps_event-df.pkl', 'main_continuous_global_ts_probeA.npy', 'main_continuous_global_ts_probeB.npy', 'probeA', 'probeB']\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\n",
      "Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\learning\\seq007_2024-11-18_08-51-40\\Record Node 103\\experiment1\\recording1\\continuous\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024//behav_sync/2_task/Preprocessed//\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024/ephys//probeA/kilosort4_output/sorter_output//\n",
      "Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\seq007_implant1\\recording1_18-11-2024\\video\\videos\\\\BACK_CAM_seq007_18-11-2024.avi\n",
      "this should be zero:\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#!## LOAD in ephys data: WARNING V SLOW - this could take a few minutes ############!#\u001b[39;00m\n\u001b[0;32m     57\u001b[0m recording \u001b[38;5;241m=\u001b[39m align_open_ephys_processors(main_processor_tuple,aux_processor_tuples,OE_raw_path)\n\u001b[1;32m---> 58\u001b[0m \u001b[43mrecording\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_global_timestamps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m#!##################################################################################!#\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m## Save this out:\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaving...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\recording.py:275\u001b[0m, in \u001b[0;36mRecording.compute_global_timestamps\u001b[1;34m(self, overwrite)\u001b[0m\n\u001b[0;32m    272\u001b[0m main_line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaling\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    273\u001b[0m main_line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m main_start_sample\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m continuous \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuous\u001b[49m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (continuous\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_node_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m main_line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessor_id\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    278\u001b[0m        (continuous\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m main_line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_name\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m    279\u001b[0m        main_line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m continuous\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\recording.py:81\u001b[0m, in \u001b[0;36mRecording.continuous\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontinuous\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_continuous \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_continuous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_continuous\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\formats\\BinaryRecording.py:160\u001b[0m, in \u001b[0;36mBinaryRecording.load_continuous\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContinuous\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmap_timestamps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mprint\u001b[39m(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolder_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m missing file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(e\u001b[38;5;241m.\u001b[39mfilename) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\open_ephys\\analysis\\formats\\BinaryRecording.py:96\u001b[0m, in \u001b[0;36mBinaryRecording.Continuous.__init__\u001b[1;34m(self, info, base_directory, version, mmap_timestamps)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_names\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [ch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbit_volts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [ch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbit_volts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 96\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontinuous.dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     98\u001b[0m                              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_channels\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\numpy\\core\\memmap.py:229\u001b[0m, in \u001b[0;36mmemmap.__new__\u001b[1;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[0;32m    227\u001b[0m     f_ctx \u001b[38;5;241m=\u001b[39m nullcontext(filename)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     f_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m f_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    232\u001b[0m     fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# deal with input arguments\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--ID', type=int, required=True)\n",
    "# args = parser.parse_args()\n",
    "# current_run_id = args.ID\n",
    "# print(f\"Current run ID: {current_run_id}\")\n",
    "\n",
    "current_run_id = 4\n",
    "\n",
    "replace_files = False\n",
    "\n",
    "# extract the current run data from the dataframe\n",
    "row = data_frame.loc[current_run_id]\n",
    "\n",
    "animal = row.animal\n",
    "shank = row.shank\n",
    "experiment_type = row.type\n",
    "\n",
    "if 'seq' not in animal and 'ap5' not in animal:\n",
    "    data_path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\'\n",
    "else:\n",
    "    data_path = r'Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\'\n",
    "\n",
    "output_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\oscillations\\\\\"\n",
    "\n",
    "# list out all possible datapaths\n",
    "data_paths = list_all_datapaths(data_path) \n",
    "\n",
    "for path in data_paths:\n",
    "    if 'EJT' in path:\n",
    "        a = path.split('\\\\')[-2].split('_')[0][3::]\n",
    "    else:\n",
    "        a = path.split('\\\\')[-2].split('_')[0]\n",
    "    b = path.split('\\\\')[-2][-1]\n",
    "    c = path.split('\\\\')[-1].split('_')[0].split('g')[-1]\n",
    "    current_mouse = '_'.join([a,b,c])\n",
    "    print(current_mouse)\n",
    "    if current_mouse != animal:\n",
    "        continue\n",
    "    print('-------------------------------------')\n",
    "    print()\n",
    "    print(path)\n",
    "    organised_ephys_path = os.path.join(path, r'ephys\\\\')\n",
    "\n",
    "    # get the recording folder\n",
    "    mouse_folder = None\n",
    "    if 'EJT' in path:\n",
    "        OE_raw_path,processor_path = gather_paths_for_old_data(current_mouse,path)\n",
    "    else:\n",
    "        OE_raw_path, processor_path = gather_paths_for_new_data(current_mouse)\n",
    "        \n",
    "    # find processor tuples\n",
    "    main_processor_tuple,aux_processor_tuples = find_processor_tuples(processor_path)\n",
    "    \n",
    "    #!## LOAD in ephys data: WARNING V SLOW - this could take a few minutes ############!#\n",
    "    \n",
    "    recording = align_open_ephys_processors(main_processor_tuple,aux_processor_tuples,OE_raw_path)\n",
    "    recording.compute_global_timestamps()\n",
    "    \n",
    "    #!##################################################################################!#\n",
    "    \n",
    "    ## Save this out:\n",
    "    print('saving...')\n",
    "    if not os.path.isdir(organised_ephys_path):\n",
    "        os.makedirs(organised_ephys_path)\n",
    "        \n",
    "    # check to see if global object alreayd exists and if it does no need to save\n",
    "    if process_probe_data_bool(organised_ephys_path,aux_processor_tuples) == False:\n",
    "        save_path = organised_ephys_path + f\"global-timstamps_event-df.pkl\"\n",
    "        recording.events.to_pickle(save_path)\n",
    "    \n",
    "    ## Extract timestamp data:\n",
    "    events_df= recording.events\n",
    "\n",
    "    ## extract the main npx continuous data:\n",
    "    # work out the data ind\n",
    "    probeA_data_index = None\n",
    "    probeB_data_index = None\n",
    "    ProbeB = False\n",
    "    if 'EJT' in path:\n",
    "        for index, item in enumerate(os.listdir(processor_path)):\n",
    "            meta_data = recording.continuous[index].metadata\n",
    "            if 'AP' in meta_data['channel_names'][0]:\n",
    "                probeA_data_index = index\n",
    "    else:\n",
    "        stream_names = []\n",
    "        for index, item in enumerate(os.listdir(processor_path)):   \n",
    "            meta_data = recording.continuous[index].metadata\n",
    "            stream_names += [meta_data['stream_name']]\n",
    "\n",
    "        print(stream_names)\n",
    "        stream_names = np.array(stream_names)\n",
    "        if 'ProbeA-AP' in stream_names:\n",
    "            probeA_data_index = np.where(stream_names == 'ProbeA-AP')[0][0]\n",
    "        elif 'ProbeA' in stream_names:\n",
    "            probeA_data_index = np.where(stream_names == 'ProbeA')[0][0]\n",
    "        if 'ProbeB' in stream_names:\n",
    "            ProbeB = True\n",
    "            probeB_data_index = np.where(stream_names == 'ProbeB')[0][0]\n",
    "        \n",
    "    ProbeA_data = recording.continuous[probeA_data_index].samples\n",
    "    if ProbeB:\n",
    "        ProbeB_data = recording.continuous[probeB_data_index].samples\n",
    "        \n",
    "\n",
    "    #choose 8 channels \n",
    "    channels = [50,90,130,170,210,250,290,340]\n",
    "    \n",
    "    # check if the data has alreay been processed\n",
    "    B_process = False\n",
    "    A_process = False\n",
    "    A_chans_to_process = []\n",
    "    B_chans_to_process = []\n",
    "    if not replace_files:\n",
    "        A_process,A_chans_to_process = channels_to_process(output_path,channels,current_mouse,'striatum_lfp\\\\' + experiment_type + '\\\\')\n",
    "        if ProbeB: \n",
    "            B_process,B_chans_to_process = channels_to_process(output_path,channels,current_mouse,'hippocampus_lfp\\\\' + experiment_type + '\\\\')  \n",
    "    else:\n",
    "        A_chans_to_process = channels \n",
    "        B_chans_to_process = channels\n",
    "        A_process = True\n",
    "        B_process = True   \n",
    "        \n",
    "    if B_process == True or A_process == True:\n",
    "        \n",
    "        # import the histology data\n",
    "        if 'EJT' in path:\n",
    "            brainreg_base_path = r\"Z:\\projects\\sequence_squad\\data\\histology\\Neuropixel_tracks\\\\\"\n",
    "            for mouse_name in os.listdir(brainreg_base_path):\n",
    "                if current_mouse.split('_')[0] in mouse_name:\n",
    "                    hist_path_base = os.path.join(brainreg_base_path,mouse_name)\n",
    "                    hist_path = os.path.join((hist_path_base),\"brainreg\\\\\")\n",
    "                    hist_path = glob.glob(os.path.join(hist_path, '**','tracks'),recursive = True)[0]\n",
    "                    # load the data\n",
    "                    implant_files = []\n",
    "                    for item in os.listdir(hist_path):\n",
    "                        if 'csv' in item:\n",
    "                            implant_files+=[item]\n",
    "\n",
    "                    if len(implant_files) > 1:\n",
    "                        for file in implant_files:\n",
    "                            if current_mouse.split('_')[-2] in file:\n",
    "                                implant_file = file\n",
    "                    else:\n",
    "                        implant_file = implant_files[0]\n",
    "\n",
    "                    probe_track_file = os.path.join(hist_path,implant_file) \n",
    "                    print(probe_track_file)\n",
    "                    implant_df = pd.read_csv(probe_track_file)\n",
    "                    \n",
    "                    # this is only needed for probe A:\n",
    "                    striatum_proportion = find_propotion_in_striatum(implant_df)\n",
    "                    # there should be 400 channels per 4000um (what i implanted), tot_channels = 384, bank_spacing = 20 # 20um, channels_per_bank = 2\n",
    "                    first_cortex_channel = int(striatum_proportion * 400)\n",
    "                    total_chans = 400\n",
    "                    plot_boundary(first_cortex_channel,total_chans)\n",
    "                    print(first_cortex_channel)\n",
    "                                        \n",
    "        else:\n",
    "            if ProbeB:\n",
    "                implant_dfs_A,implant_dfs_B = find_histology_paths(current_mouse,ProbeB,shank)\n",
    "            else:\n",
    "                implant_dfs_A = find_histology_paths(current_mouse,ProbeB,shank)\n",
    "            first_cortex_channel,total_chans = find_propotion_new_data(implant_dfs_A[0])\n",
    "            plot_boundary(first_cortex_channel,total_chans)\n",
    "        \n",
    "        if A_process:\n",
    "            #label the channels   \n",
    "            channel_regions = []\n",
    "            for channel in A_chans_to_process:\n",
    "                if channel >= first_cortex_channel:\n",
    "                    channel_regions.append('m_crtex')\n",
    "                elif channel < first_cortex_channel:\n",
    "                    channel_regions.append('striatum')\n",
    "            # process the data for probe A\n",
    "            process_probe_channels(ProbeA_data,A_chans_to_process,channel_regions,current_mouse,output_path,'striatum_lfp\\\\' + experiment_type + '\\\\', highcut_value = 30)\n",
    "            \n",
    "            # save out the timestamp data\n",
    "            Fs = 30000\n",
    "            samples = len(ProbeA_data)\n",
    "            timestamps_index = np.linspace(0,samples,samples+1).astype(int)\n",
    "            timestamps_index_downsampled = timestamps_index[::12]\n",
    "            timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "            downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "            mouse_out_path = os.path.join(output_path, 'striatum_lfp',experiment_type,current_mouse) + r'\\\\'\n",
    "            downsampled_timstamp_df.to_csv(mouse_out_path + 'probeA_timestamps.csv')\n",
    "            \n",
    "        # same for probe B\n",
    "        if B_process:\n",
    "            #label the channels for probe B  \n",
    "            probeB_channel_regions = ['hippocampus'] * len(B_chans_to_process)\n",
    "            process_probe_channels(ProbeB_data,B_chans_to_process,probeB_channel_regions,current_mouse,output_path,'striatum_lfp\\\\' + experiment_type + '\\\\', highcut_value = 1000)\n",
    "            \n",
    "            # save out timestamp data \n",
    "            samples = len(ProbeB_data)\n",
    "            timestamps_index = np.linspace(0,samples,samples+1).astype(int)\n",
    "            timestamps_index_downsampled = timestamps_index[::12]\n",
    "            timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "            downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "            mouse_out_path = os.path.join(output_path, 'hippocampus_lfp', experiment_type, current_mouse) + r'\\\\'\n",
    "            downsampled_timstamp_df.to_csv(mouse_out_path + 'probeB_timestamps.csv')\n",
    "    else:\n",
    "        print(f'all data already processed for {current_mouse}')\n",
    "            \n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad02174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
