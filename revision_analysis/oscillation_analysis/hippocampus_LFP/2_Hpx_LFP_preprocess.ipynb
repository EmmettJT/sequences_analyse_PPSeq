{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f35f3d",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd41257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from open_ephys.analysis import Session\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob \n",
    "from scipy.signal import butter, sosfilt, sosfilt_zi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Define the lowpass filter\n",
    "def create_lowpass_filter(highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    high = highcut / nyquist\n",
    "    sos = butter(order, high, btype='low', output='sos')\n",
    "    return sos\n",
    "    \n",
    "def list_all_datapaths(data_path):\n",
    "    data_paths = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if 'EJT' in file[0:5] or 'revision' in data_path:\n",
    "            for file_ in os.listdir(data_path+file):\n",
    "                if 'record' in file_:\n",
    "                    data_paths += [data_path+file+'\\\\'+file_]\n",
    "            \n",
    "    return data_paths\n",
    "\n",
    "def walk_through_files_countinous_data(recording_path_1):\n",
    "    continuous_file_path = None\n",
    "\n",
    "    for root, dirs, files in os.walk(recording_path_1):\n",
    "        if 'continuous.dat' in files:\n",
    "            continuous_file_path = os.path.join(root, 'continuous.dat')\n",
    "            break\n",
    "\n",
    "    if continuous_file_path:\n",
    "        print(f\"Found 'continuous.dat' at: {continuous_file_path}\")\n",
    "        return continuous_file_path\n",
    "    else:\n",
    "        print(\"'continuous.dat' not found in the specified path.\")\n",
    "        \n",
    "def find_processor_tuples(processor_path):\n",
    "    count = 0\n",
    "    for processor in os.listdir(processor_path):\n",
    "        if count == 0:\n",
    "            main1 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main1_2 = processor.split('.')[-1]\n",
    "        elif count == 1:\n",
    "            main2 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main2_2 = processor.split('.')[-1]\n",
    "        elif count == 2:\n",
    "            main3 = int(re.findall(r'\\d+', processor)[0])\n",
    "            main3_2 = processor.split('.')[-1]\n",
    "        count +=1 \n",
    "\n",
    "    main_processor_tuple=(main1, main1_2)\n",
    "    if count == 3:\n",
    "        aux_processor_tuples=((main2,main2_2),(main3,main3_2))\n",
    "    else:\n",
    "        aux_processor_tuples=[(main2,main2_2)]\n",
    "        \n",
    "    return main_processor_tuple,aux_processor_tuples\n",
    "\n",
    "def process_probe_data_bool(organised_ephys_path,aux_processor_tuples):\n",
    "    process = False\n",
    "    if not 'global-timstamps_event-df.pkl' in os.listdir(organised_ephys_path):\n",
    "        if not 'main_continuous_global_ts_probeA.npy' in os.listdir(organised_ephys_path):\n",
    "            if not 'LFP' in aux_processor_tuples[0][-1]:\n",
    "                if not 'main_continuous_global_ts_probeA_LFP.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "            else:\n",
    "                if not 'main_continuous_global_ts_probeB.npy' in os.listdir(organised_ephys_path):\n",
    "                    process = True\n",
    "    return process\n",
    "\n",
    "## new version for new open ephys tools \n",
    "def align_open_ephys_processors(main_processor_tuple, aux_processor_tuples,raw_data_directory, sync_channel=1):\n",
    "\n",
    "    session_data = Session(str(raw_data_directory))\n",
    "    if len(session_data.recordnodes) != 1:\n",
    "        raise ValueError(\"should be exactly one record node.\")\n",
    "    if len(session_data.recordnodes[0].recordings) != 1:\n",
    "        raise ValueError(\"Should be exactly one recording.\")\n",
    "    for rn, recordnode in enumerate(session_data.recordnodes):\n",
    "        for r, recording in enumerate(recordnode.recordings):\n",
    "            # Sync\n",
    "            recording.add_sync_line(\n",
    "                sync_channel,\n",
    "                main_processor_tuple[0],\n",
    "                main_processor_tuple[1],\n",
    "                main=True,\n",
    "            )\n",
    "            for aux_processor in aux_processor_tuples:\n",
    "                recording.add_sync_line(\n",
    "                    sync_channel,\n",
    "                    aux_processor[0],\n",
    "                    aux_processor[1],\n",
    "                    main=False,\n",
    "                )\n",
    "            print('this should be zero:')\n",
    "            print(rn)\n",
    "        \n",
    "    return recording\n",
    "\n",
    "def check_if_probe_is_flipped(A_probes):\n",
    "\n",
    "    # Check if 'CP' appears before 'ccb or ccg' in the list of region acronyms, it shouldnt if the probe is the right way up\n",
    "    if 'CP' in A_probes['Region acronym'].values and 'ccb' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccb'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    elif 'CP' in A_probes['Region acronym'].values and 'ccg' in A_probes['Region acronym'].values:\n",
    "        if list(A_probes['Region acronym'].values).index('CP') < list(A_probes['Region acronym'].values).index('ccg'):\n",
    "            print('cp appears first ')\n",
    "            flipped = True\n",
    "        else:\n",
    "            print('cp appears second - good')\n",
    "            flipped = False\n",
    "    else:\n",
    "        print('error')\n",
    "    return flipped\n",
    "\n",
    "def find_propotion_in_striatum(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        str_prop = callosum_middle_index/len(implant_df)\n",
    "    else:\n",
    "        str_prop = 1- callosum_middle_index/len(implant_df)\n",
    "    return str_prop\n",
    "\n",
    "\n",
    "def process_probe_channels(ProbeA_data,channels,channel_regions,current_mouse,output_path,var_string,highcut_value):\n",
    "    # pull out the data for each channel, lowpass to prevent aliasing (stops the high frequency stuff from folding back onto intself and poluting the low frequency range) then downsample\n",
    "         \n",
    "    # Parameters\n",
    "    highcut = highcut_value  # Upper cutoff frequency in Hz\n",
    "    fs = 30000.0  # Original sampling rate in Hz\n",
    "    downsample_factor = 12  # Factor by which to downsample\n",
    "    chunk_size = 2000  # Number of samples per chunk\n",
    "    # Create the bandpass filter\n",
    "    sos = create_lowpass_filter(highcut, fs, order=4)\n",
    "    \n",
    "    # Initialize the filter state\n",
    "    zi = sosfilt_zi(sos)\n",
    "\n",
    "    # Process each channel\n",
    "    for ind_,chosen_channel in enumerate(channels):\n",
    "        data_channel = []\n",
    "        for i in tqdm(range(0, len(ProbeA_data), chunk_size)):\n",
    "            # Extract the current chunk for the chosen channel\n",
    "            chunk = np.array([ProbeA_data[j][chosen_channel] for j in range(i, min(i + chunk_size, len(ProbeA_data)))])\n",
    "\n",
    "            # Apply the bandpass filter to the chunk\n",
    "            if i == 0:\n",
    "                # For the first chunk, use the initial filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi * chunk[0])\n",
    "            else:\n",
    "                # For subsequent chunks, use the updated filter state\n",
    "                bp_chunk, zi = sosfilt(sos, chunk, zi=zi)\n",
    "\n",
    "            # Append the filtered chunk to the channel data\n",
    "            data_channel.extend(bp_chunk)\n",
    "\n",
    "        # Downsample the filtered data\n",
    "        data_downsampled = data_channel[::downsample_factor]\n",
    "        \n",
    "        # clean up for memory\n",
    "        del data_channel\n",
    "        \n",
    "        mouse_out_path = os.path.join(output_path,var_string) + current_mouse\n",
    "        if not os.path.exists(mouse_out_path):\n",
    "            os.makedirs(mouse_out_path)\n",
    "            \n",
    "        save_path = mouse_out_path + '//channel-' + str(chosen_channel) + '_REGION-' + channel_regions[ind_] + \"_LFP_data.npy\"\n",
    "        np.save(save_path,data_downsampled)\n",
    "        print('data saved for channel ' + str(chosen_channel))\n",
    "\n",
    "\n",
    "\n",
    "def gather_paths_for_old_data(current_mouse,path):\n",
    "    ### find OE processor path and OE_raw_path\n",
    "    OE_processor_path_base = r\"Z:\\projects\\sequence_squad\\data\\raw_neuropixel\\OE_DATA\\\\\"\n",
    "    if current_mouse.split('_')[1] == '2':\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] + '_' + 'implant' + current_mouse.split('_')[1]\n",
    "    else:\n",
    "        mouse_folder = 'EJT' + current_mouse.split('_')[0] \n",
    "    for folder in os.listdir(OE_processor_path_base):\n",
    "        if mouse_folder == folder:\n",
    "            print(mouse_folder)\n",
    "            OE_processor_path = OE_processor_path_base + folder + '\\\\'\n",
    "\n",
    "    # next get the date \n",
    "    recording_date = path.split('\\\\')[-1].split('_')[-1]\n",
    "    reformatted_date = ''.join(recording_date.split('-')[0:-1]) + recording_date.split('-')[-1][-2::]\n",
    "    \n",
    "    recording_path_1 = None\n",
    "    for recording_date in os.listdir(OE_processor_path):\n",
    "        if reformatted_date == recording_date:\n",
    "            print(reformatted_date)\n",
    "            recording_path_1 = os.path.join(OE_processor_path,recording_date) + '\\\\'\n",
    "            \n",
    "    # set some OE paths I need \n",
    "    continuous_file_path = walk_through_files_countinous_data(recording_path_1)\n",
    "    processor_path = '\\\\'.join(continuous_file_path.split('\\\\')[0:-2])+ '\\\\'\n",
    "    OE_raw_path = os.path.join(recording_path_1,os.listdir(recording_path_1)[0]) + '\\\\'\n",
    "    return OE_raw_path,processor_path\n",
    "\n",
    "def find_folder_path(parent_folder, target_folder):\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        if target_folder in dirs:\n",
    "            return os.path.join(root, target_folder)\n",
    "        # If the target folder is not found\n",
    "    return (print('not found'))\n",
    "\n",
    "\n",
    "def gather_paths_for_new_data(mir):\n",
    "\n",
    "    # gather the raw ephys paths for each mouse\n",
    "    path_ = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\ephys\\\\\"\n",
    "    base_recording_paths = []\n",
    "    for q in os.listdir(path_):\n",
    "        if not 'other_sessions' in q:\n",
    "            if not 'sp5_recordings' in q:\n",
    "                folder = os.path.join(path_,q)\n",
    "                for q in os.listdir(folder):\n",
    "                    if '2024' in q or '2025' in q:\n",
    "                        base_recording_paths+=[os.path.join(folder,q)]\n",
    "                    else:\n",
    "                        folder_layer = os.path.join(folder,q)\n",
    "                        for r in os.listdir(folder_layer):\n",
    "                            base_recording_paths+=[os.path.join(folder_layer,r)]\n",
    "            \n",
    "\n",
    "    # gather the corespoding organised paths for each raw dat file \n",
    "    organised_path = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "    full_organised_paths = []\n",
    "    base_recording_paths_keep = []\n",
    "    mirs = []\n",
    "    for i in range(len(base_recording_paths)):\n",
    "        \n",
    "        # add exception for trash folder in seq052\n",
    "        if 'trash' in base_recording_paths[i] or 'seq039' in base_recording_paths[i]:\n",
    "            continue\n",
    "        \n",
    "        mouse_id = base_recording_paths[i].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths[i].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "\n",
    "        if os.path.exists(os.path.join(organised_path,mouse_id+'_implant1')):\n",
    "            organised_path_animal = os.path.join(organised_path,mouse_id+'_implant1')\n",
    "            base_recording_paths_keep += [base_recording_paths[i]]\n",
    "        else:\n",
    "            # full_organised_paths + ['recording_not_processed_yet']\n",
    "            # mirs += ['recording_not_processed_yet']\n",
    "            continue\n",
    "            \n",
    "        for recording in os.listdir(organised_path_animal):\n",
    "            if date_ in recording:\n",
    "                full_organised_paths += [os.path.join(organised_path_animal,recording)]\n",
    "                mirs += [mouse_id + '_1_' + recording.split('_')[0].split('g')[-1]]\n",
    "                break\n",
    "\n",
    "\n",
    "    for index in range(len(base_recording_paths_keep)):\n",
    "        \n",
    "        mouse_id = base_recording_paths_keep[index].split('\\\\')[-1].split('_')[0]\n",
    "        date_ = base_recording_paths_keep[index].split('\\\\')[-1].split('_')[1]\n",
    "        #reverse the date\n",
    "        date_ = '-'.join(date_.split('-')[::-1])\n",
    "        \n",
    "        \n",
    "        probeB = False\n",
    "        if mirs[index] == mir:\n",
    "            print(mir)\n",
    "            print(index)\n",
    "            \n",
    "            # set important paths\n",
    "            raw_data_directory = base_recording_paths_keep[index]\n",
    "            print(raw_data_directory)\n",
    "            OE_processor_path = find_folder_path(raw_data_directory, \"continuous\") \n",
    "            Behav_data_path = full_organised_paths[index]+ r'//behav_sync/2_task/Preprocessed//'\n",
    "\n",
    "            Processed_Ephys_data_path_PROBEA = full_organised_paths[index]+ r'/ephys//' + r'probeA/kilosort4_output/sorter_output//'\n",
    "            if 'probeB' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                print('Probe B found')\n",
    "                Processed_Ephys_data_path_PROBEB = full_organised_paths[index]+ r'/ephys//' + r'probeB/kilosort4_output/sorter_output//'\n",
    "                probeB = True\n",
    "            \n",
    "            organised_ephys_path = full_organised_paths[index]+ r'/ephys//'\n",
    "            \n",
    "            for vid_file in os.listdir(full_organised_paths[index] + r'\\video\\videos\\\\'):\n",
    "                if 'BACK' in vid_file:\n",
    "                    if 'avi' in vid_file:\n",
    "                        back_video_path = os.path.join(full_organised_paths[index] + r'\\video\\videos\\\\',vid_file)\n",
    "\n",
    "                    \n",
    "            if 'probeA' in os.listdir(full_organised_paths[index]+ r'/ephys//'):\n",
    "                if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeA'):\n",
    "                    if probeB: \n",
    "                        if 'unit_info.txt' in os.listdir(full_organised_paths[index]+ r'/ephys//' + 'probeB'):\n",
    "                            print('All good! Data is kilosorted for PROBE A and PROBE B ')\n",
    "                            \n",
    "                            print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                            print(raw_data_directory)\n",
    "                            print(OE_processor_path)\n",
    "                            print(Behav_data_path)\n",
    "                            print(Processed_Ephys_data_path_PROBEA)\n",
    "                            print(back_video_path)\n",
    "                            break\n",
    "                        else:\n",
    "                            print('PROBE B data not yet kilosorted, skip!')\n",
    "                    else:\n",
    "                        print('All good! Data is kilosorted for PROBE A')\n",
    "                        print(os.listdir(full_organised_paths[index]+ r'/ephys//'))\n",
    "                        print(raw_data_directory)\n",
    "                        print(OE_processor_path)\n",
    "                        print(Behav_data_path)\n",
    "                        print(Processed_Ephys_data_path_PROBEA)\n",
    "                        print(back_video_path)\n",
    "                        break\n",
    "                        \n",
    "                else:\n",
    "                    print('data not yet kilosorted, skip!')\n",
    "            else:\n",
    "                print('data not yet kilosorted, skip!')\n",
    "    return raw_data_directory, OE_processor_path\n",
    "\n",
    "\n",
    "def find_histology_paths(mouse_id,ProbeB,shank):\n",
    "\n",
    "    # load the brainreg positions for the probe that was used in the recoridng (based on the manual labelling)\n",
    "    brainreg_base_path = r\"Z:\\projects\\sequence_squad\\revision_data\\lars_recordings\\serial_section\\brainreg_output\\brainreg\\\\\"\n",
    "\n",
    "    for mouse_file in os.listdir(brainreg_base_path):\n",
    "        if mouse_file in mouse_id:\n",
    "            print(mouse_file)\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        elif mouse_file in mouse_id.lower():\n",
    "            b_reg_path = os.path.join(brainreg_base_path,mouse_file)+ r'\\segmentation\\atlas_space\\tracks\\\\'\n",
    "            print(b_reg_path)\n",
    "            break\n",
    "        else:\n",
    "            b_reg_path = None\n",
    "            \n",
    "    if len([f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]) > 0:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'probeA_{shank}' in f and f.endswith('.csv')]\n",
    "    else:\n",
    "        probeA_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeA_{shank}' in f and f.endswith('.csv')]\n",
    "        \n",
    "    # Load the CSV files into dataframes\n",
    "    A_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeA_csv_files]\n",
    "\n",
    "    if ProbeB:\n",
    "        if len([f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]) > 0:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'probeB' in f and f.endswith('.csv')]\n",
    "        else:\n",
    "            probeB_csv_files = [f for f in os.listdir(b_reg_path) if f'ProbeB' in f and f.endswith('.csv')]\n",
    "        B_probes = [pd.read_csv(os.path.join(b_reg_path, file)) for file in probeB_csv_files]  \n",
    "        \n",
    "        return A_probes, B_probes\n",
    "    else:\n",
    "        return A_probes\n",
    "    \n",
    "    \n",
    "def find_propotion_new_data(implant_df):\n",
    "    try:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccb')))\n",
    "    except:\n",
    "        callosum_middle_index = int(np.median(np.where(implant_df['Region acronym'].values == 'ccg')))\n",
    "    boundary_um = implant_df['Distance from first position [um]'][callosum_middle_index]\n",
    "    full_length = max(implant_df['Distance from first position [um]'].values)\n",
    "    \n",
    "\n",
    "    # 2.0 probes have 2 electrodes per bank and 15um spacing between banks\n",
    "    if check_if_probe_is_flipped(implant_df):\n",
    "        channel_boundary = int(boundary_um /15) *2\n",
    "    else:\n",
    "        channel_boundary = int((full_length-boundary_um)/15)*2\n",
    "\n",
    "    return channel_boundary, int(full_length/15)*2\n",
    "        \n",
    "def plot_boundary(first_cortex_channel,total_chans):\n",
    "    fig,ax = plt.subplots(1,1,figsize = (1,3))\n",
    "    ax.plot([0,0],[0,total_chans],'-')\n",
    "    ax.plot(first_cortex_channel,'o')\n",
    "    ax.set_title('srtr-cortex boundary')\n",
    "    \n",
    "def channels_to_process(output_path,channels,current_mouse,str_var):\n",
    "    processed_channel = []\n",
    "    # if it doesnt exist then just process all, if it does exist check which channels exist. \n",
    "    if os.path.exists(os.path.join(output_path, str_var) + current_mouse):\n",
    "        for file in os.listdir(os.path.join(output_path, str_var) + current_mouse):\n",
    "            if 'channel' in file:\n",
    "                processed_channel += [int(file.split('_')[0].split('-')[-1])]\n",
    "        to_process = []\n",
    "        for channel in channels:\n",
    "            if not channel in processed_channel:\n",
    "                to_process += [channel]\n",
    "        if len(to_process) == 0:\n",
    "            process = False\n",
    "        else:\n",
    "            process = True\n",
    "        return process,to_process\n",
    "    else:\n",
    "        process = True \n",
    "        to_process = channels\n",
    "        return process,to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cae777",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e57fb5",
   "metadata": {},
   "source": [
    "put all data into reference dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(animal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa1093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "animal_list = ['seq006_1_1','seq006_1_2','seq006_1_3','seq006_1_4','seq006_1_5','seq006_1_6','seq006_1_7','seq006_1_8','seq006_1_9','seq006_1_10','seq006_1_11',\n",
    "'seq007_1_1','seq007_1_2','seq007_1_3','seq007_1_4',\n",
    "'seq008_1_1','seq008_1_2','seq008_1_3','seq008_1_4',\n",
    "'seq120_1_1','seq120_1_2','seq120_1_3','seq120_1_4','seq120_1_5','seq120_1_6','seq120_1_7','seq120_1_8']\n",
    "\n",
    "\n",
    "\n",
    "deal with input arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--ID', type=int, required=True)\n",
    "args = parser.parse_args()\n",
    "current_run_id = args.ID\n",
    "print(f\"Current run ID: {current_run_id}\")\n",
    "\n",
    "\n",
    "\n",
    "replace_files = False\n",
    "\n",
    "# # extract the current run data from the dataframe\n",
    "# row = data_frame.loc[current_run_id]\n",
    "\n",
    "# animal = row.animal\n",
    "# shank = row.shank\n",
    "# experiment_type = row.type\n",
    "animal = animal_list[current_run_id]\n",
    "\n",
    "if 'seq' not in animal and 'ap5' not in animal:\n",
    "    data_path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\'\n",
    "else:\n",
    "    data_path = r'Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\'\n",
    "\n",
    "output_path = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\oscillations\\\\\"\n",
    "\n",
    "# list out all possible datapaths\n",
    "data_paths = list_all_datapaths(data_path) \n",
    "\n",
    "for path in data_paths:\n",
    "    if 'EJT' in path:\n",
    "        a = path.split('\\\\')[-2].split('_')[0][3::]\n",
    "    else:\n",
    "        a = path.split('\\\\')[-2].split('_')[0]\n",
    "    b = path.split('\\\\')[-2][-1]\n",
    "    c = path.split('\\\\')[-1].split('_')[0].split('g')[-1]\n",
    "    current_mouse = '_'.join([a,b,c])\n",
    "    print(current_mouse)\n",
    "    if current_mouse != animal:\n",
    "        continue\n",
    "    print('-------------------------------------')\n",
    "    print()\n",
    "    print(path)\n",
    "    organised_ephys_path = os.path.join(path, r'ephys\\\\')\n",
    "\n",
    "    # get the recording folder\n",
    "    mouse_folder = None\n",
    "    if 'EJT' in path:\n",
    "        OE_raw_path,processor_path = gather_paths_for_old_data(current_mouse,path)\n",
    "    else:\n",
    "        OE_raw_path, processor_path = gather_paths_for_new_data(current_mouse)\n",
    "        \n",
    "    # find processor tuples\n",
    "    main_processor_tuple,aux_processor_tuples = find_processor_tuples(processor_path)\n",
    "\n",
    "    #!## LOAD in ephys data: WARNING V SLOW - this could take a few minutes ############!#\n",
    "    recording = align_open_ephys_processors(main_processor_tuple,aux_processor_tuples,OE_raw_path)\n",
    "    recording.compute_global_timestamps()\n",
    "    \n",
    "    #!##################################################################################!#\n",
    "    \n",
    "    ## Save this out:\n",
    "    print('saving...')\n",
    "    if not os.path.isdir(organised_ephys_path):\n",
    "        os.makedirs(organised_ephys_path)\n",
    "        \n",
    "    # check to see if global object alreayd exists and if it does no need to save\n",
    "    if process_probe_data_bool(organised_ephys_path,aux_processor_tuples) == False:\n",
    "        save_path = organised_ephys_path + f\"global-timstamps_event-df.pkl\"\n",
    "        recording.events.to_pickle(save_path)\n",
    "    \n",
    "    ## Extract timestamp data:\n",
    "    events_df= recording.events\n",
    "    \n",
    "    ## extract the main npx continuous data:\n",
    "    # work out the data ind\n",
    "    probeB_data_index = None\n",
    "\n",
    "    stream_names = []\n",
    "    for index, item in enumerate(os.listdir(processor_path)):   \n",
    "        meta_data = recording.continuous[index].metadata\n",
    "        stream_names += [meta_data['stream_name']]\n",
    "\n",
    "    print(stream_names)\n",
    "    stream_names = np.array(stream_names)\n",
    "\n",
    "    probeB_data_index = np.where(stream_names == 'ProbeB')[0][0]\n",
    "        \n",
    "    ProbeB_data = recording.continuous[probeB_data_index].samples\n",
    "        \n",
    "    lections = pd.read_csv(\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\oscillations\\hippocampus_lfp\\\\hpc_electrode_selections.csv\")\n",
    "\n",
    "    #choose 8 channels \n",
    "    import ast\n",
    "    electrode_selections = pd.read_csv(r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\oscillations\\hippocampus_lfp\\\\hpc_electrode_selections.csv\")\n",
    "    channels = ast.literal_eval(electrode_selections[electrode_selections.mouse_id == animal.split('_')[0]].electrodes_to_use.values[0])\n",
    "    \n",
    "    # check if the data has alreay been processed\n",
    "    B_process = False\n",
    "    B_chans_to_process = []\n",
    "    \n",
    "    if not replace_files:\n",
    "        B_process,B_chans_to_process = channels_to_process(output_path,channels,current_mouse,'hippocampus_lfp\\\\' + experiment_type + '\\\\')  \n",
    "    else:\n",
    "        B_chans_to_process = channels\n",
    "        B_process = True   \n",
    "        \n",
    "    if B_process == True\n",
    "    \n",
    "        Fs = 30000\n",
    "        #label the channels for probe B  \n",
    "        probeB_channel_regions = ['hippocampus'] * len(B_chans_to_process)\n",
    "        process_probe_channels(ProbeB_data,B_chans_to_process,probeB_channel_regions,current_mouse,output_path,'hippocampus_lfp\\extracted_LFP\\\\', highcut_value = 500)\n",
    "\n",
    "        # save out timestamp data \n",
    "        samples = len(ProbeB_data)\n",
    "        timestamps_index = np.linspace(0,samples,samples+1).astype(int)\n",
    "        timestamps_index_downsampled = timestamps_index[::12]\n",
    "        timestamps_downsampled = timestamps_index_downsampled/Fs\n",
    "        downsampled_timstamp_df = pd.DataFrame({'sample_number':timestamps_index_downsampled,'ephys_timestamp':timestamps_downsampled})\n",
    "        mouse_out_path = os.path.join(output_path, 'hippocampus_lfp', 'extracted_LFP', current_mouse) + r'\\\\'\n",
    "        downsampled_timstamp_df.to_csv(mouse_out_path + 'probeB_timestamps.csv')\n",
    "    else:\n",
    "        print(f'all data already processed for {current_mouse}')\n",
    "            \n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2371409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\projects\\\\sequence_squad\\\\revision_data\\\\emmett_revisions\\\\oscillations\\\\\\\\'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77bfacd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77b0ed6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d568808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[141, 142, 187, 188, 330, 331, 378, 378]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac6d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a47de5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 'ProbeA')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_processor_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ca004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b13ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad02174",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_recording_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbase_recording_paths\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_recording_paths' is not defined"
     ]
    }
   ],
   "source": [
    "base_recording_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765f652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
