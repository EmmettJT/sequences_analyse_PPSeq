{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93df4bd",
   "metadata": {},
   "source": [
    "# Import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7346c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import seaborn as sns;\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_H5_bodypart(tracking_path,video_type, tracking_point):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'task' in file:\n",
    "                back_file = pd.read_hdf(tracking_path + file)     \n",
    "                \n",
    "    # drag data out of the df\n",
    "    scorer = back_file.columns.tolist()[0][0]\n",
    "    body_part = back_file[scorer][tracking_point]\n",
    "    \n",
    "    parts=[]\n",
    "    for item in list(back_file[scorer]):\n",
    "        parts+=[item[0]]\n",
    "    print(np.unique(parts))\n",
    "    \n",
    "    # clean and interpolate frames with less than 98% confidence\n",
    "    clean_and_interpolate(body_part,0.98)\n",
    "    \n",
    "    return(body_part)\n",
    "  \n",
    "def load_H5_ports(tracking_path,video_type):\n",
    "\n",
    "    # Load in all '.h5' files for a given folder:\n",
    "    TFiles_unsort = list_files(tracking_path, 'h5')\n",
    "\n",
    "    for file in TFiles_unsort:\n",
    "        print(file)\n",
    "        if video_type in file:\n",
    "            if 'port' in file or 'PORT' in file:\n",
    "                back_ports_file = pd.read_hdf(tracking_path + file)\n",
    "\n",
    "    ## same for the ports:\n",
    "    scorer = back_ports_file.columns.tolist()[0][0]\n",
    "        \n",
    "    if video_type == 'back':\n",
    "        port1 =back_ports_file[scorer]['port2']\n",
    "        port2 =back_ports_file[scorer]['port1']\n",
    "        port3 =back_ports_file[scorer]['port6']\n",
    "        port4 =back_ports_file[scorer]['port3']\n",
    "        port5 =back_ports_file[scorer]['port7']\n",
    "    else:\n",
    "        port1 =back_ports_file[scorer]['Port2']\n",
    "        port2 =back_ports_file[scorer]['Port1']\n",
    "        port3 =back_ports_file[scorer]['Port6']\n",
    "        port4 =back_ports_file[scorer]['Port3']\n",
    "        port5 =back_ports_file[scorer]['Port7']\n",
    "\n",
    "    clean_and_interpolate(port1,0.98)\n",
    "    clean_and_interpolate(port2,0.98)\n",
    "    clean_and_interpolate(port3,0.98)\n",
    "    clean_and_interpolate(port4,0.98)\n",
    "    clean_and_interpolate(port5,0.98)\n",
    "    \n",
    "    return(port1,port2,port3,port4,port5)\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "def clean_and_interpolate(head_centre,threshold):\n",
    "    bad_confidence_inds = np.where(head_centre.likelihood.values<threshold)[0]\n",
    "    newx = head_centre.x.values\n",
    "    newx[bad_confidence_inds] = 0\n",
    "    newy = head_centre.y.values\n",
    "    newy[bad_confidence_inds] = 0\n",
    "\n",
    "    start_value_cleanup(newx)\n",
    "    interped_x = interp_0_coords(newx)\n",
    "\n",
    "    start_value_cleanup(newy)\n",
    "    interped_y = interp_0_coords(newy)\n",
    "    \n",
    "    head_centre['interped_x'] = interped_x\n",
    "    head_centre['interped_y'] = interped_y\n",
    "    \n",
    "def start_value_cleanup(coords):\n",
    "    # This is for when the starting value of the coords == 0; interpolation will not work on these coords until the first 0 \n",
    "    #is changed. The 0 value is changed to the first non-zero value in the coords lists\n",
    "    for index, value in enumerate(coords):\n",
    "        working = 0\n",
    "        if value > 0:\n",
    "            start_value = value\n",
    "            start_index = index\n",
    "            working = 1\n",
    "            break\n",
    "    if working == 1:\n",
    "        for x in range(start_index):\n",
    "            coords[x] = start_value\n",
    "            \n",
    "def interp_0_coords(coords_list):\n",
    "    #coords_list is one if the outputs of the get_x_y_data = a list of co-ordinate points\n",
    "    for index, value in enumerate(coords_list):\n",
    "        if value == 0:\n",
    "            if coords_list[index-1] > 0:\n",
    "                value_before = coords_list[index-1]\n",
    "                interp_start_index = index-1\n",
    "                #print('interp_start_index: ', interp_start_index)\n",
    "                #print('interp_start_value: ', value_before)\n",
    "                #print('')\n",
    "\n",
    "        if index < len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                if coords_list[index+1] > 0:\n",
    "                    interp_end_index = index+1\n",
    "                    value_after = coords_list[index+1]\n",
    "                    #print('interp_end_index: ', interp_end_index)\n",
    "                    #print('interp_end_value: ', value_after)\n",
    "                    #print('')\n",
    "\n",
    "                    #now code to interpolate over the values\n",
    "                    try:\n",
    "                        interp_diff_index = interp_end_index - interp_start_index\n",
    "                    except UnboundLocalError:\n",
    "#                         print('the first value in list is 0, use the function start_value_cleanup to fix')\n",
    "                        break\n",
    "                    #print('interp_diff_index is:', interp_diff_index)\n",
    "\n",
    "                    new_values = np.linspace(value_before, value_after, interp_diff_index)\n",
    "                    #print(new_values)\n",
    "\n",
    "                    interp_index = interp_start_index+1\n",
    "                    for x in range(interp_diff_index):\n",
    "                        #print('interp_index is:', interp_index)\n",
    "                        #print('new_value should be:', new_values[x])\n",
    "                        coords_list[interp_index] = new_values[x]\n",
    "                        interp_index +=1\n",
    "        if index == len(coords_list)-1:\n",
    "            if value ==0:\n",
    "                for x in range(30):\n",
    "                    coords_list[index-x] = coords_list[index-30]\n",
    "                    #print('')\n",
    "#     print('function exiting')\n",
    "    return(coords_list)\n",
    "\n",
    "\n",
    "# Function to find corresponding number in another column\n",
    "def find_corresponding(nums,df_dict):\n",
    "    return [df_dict[num] for num in nums]\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def load_in_paths(pp_file, PP_PATH, DAT_PATH,run_index,ignore_list):\n",
    "    mir = '_'.join(pp_file.split('_')[0:3])\n",
    "    print(str(run_index+1) + '/' + str(len(os.listdir(PP_PATH))-1) + '-------------------------------------------------------------------------')\n",
    "    print(pp_file)\n",
    "    mouse_session_recording = pp_file.split('_')[0] + '_' + pp_file.split('_')[1] + '_' + pp_file.split('_')[2] \n",
    "    skip = False\n",
    "    for item in ignore_list:\n",
    "        if item == mouse_session_recording:\n",
    "            skip = True\n",
    "\n",
    "    save_path = PP_PATH + pp_file + '\\\\_final_analysis_output\\\\'\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    ## set dat_path:\n",
    "    for file_ in os.listdir(DAT_PATH):\n",
    "        if mouse_session_recording.split('_')[0] in file_:\n",
    "            if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "                dat_path = os.path.join(DAT_PATH,file_)\n",
    "    for recording in os.listdir(os.path.join(DAT_PATH,dat_path)):\n",
    "        if recording.split('_')[0][9::] == mouse_session_recording.split('_')[-1]:\n",
    "            dat_path = os.path.join(dat_path,recording)\n",
    "\n",
    "    # set tracking path\n",
    "    for file_ in os.listdir(dat_path + r\"\\video\\tracking\\\\\"):\n",
    "        if 'EJT' in mir:\n",
    "            if 'task' in file_:\n",
    "                if not 'clock' in file_:\n",
    "                    tracking_path = os.path.join(dat_path + r\"\\video\\tracking\\\\\",file_) + '\\\\'\n",
    "        else:\n",
    "            tracking_path = dat_path + r\"\\video\\tracking\\\\\"\n",
    "            \n",
    "\n",
    "    return mir,mouse_session_recording,save_path,tracking_path,dat_path\n",
    "\n",
    "                        \n",
    "# def load_PPSEQ_data(PP_PATH,pp_file,dat_path):\n",
    "\n",
    "#     # -------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "#     ## LOAD \n",
    "#     print(\"LOADING PPSEQ DATA\")\n",
    "#     print('\\n')\n",
    "#     #The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\n",
    "#     assignment_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\assigment_hist_frame.csv\")\n",
    "\n",
    "#     # latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\n",
    "#     latent_event_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\latent_event_hist.csv\")\n",
    "\n",
    "#     # seq_type_log_proportions: log p of each type of sequence at each iteration\n",
    "#     seq_type_log_proportions_df = pd.read_csv(PP_PATH + pp_file + r\"\\seq_type_log_proportions.csv\")\n",
    "\n",
    "#     # neuron_responses.csv: iterations x neurons by 3(number of sequences). Each neuron has three parameters per sequence to describe how it is influenced by each sequence type. \n",
    "#     # Each iteration these are resampled, therefore there are number of neurons by iterations by 3 by number of sequences of these numbers.\n",
    "#     neuron_response_df = pd.read_csv(PP_PATH + pp_file + r\"\\neuron_response.csv\")\n",
    "\n",
    "\n",
    "#     masking = False\n",
    "#     for dat_files in os.listdir(PP_PATH + pp_file):\n",
    "#         if 'unmasked_spikes' in dat_files:\n",
    "#             masking = True\n",
    "#             print('masking was used')\n",
    "\n",
    "#     if masking == True:\n",
    "#         #log_p_hist.csv: the history of the log_p of the model\n",
    "#         log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\test_log_p_hist.csv\")\n",
    "\n",
    "#         unmasked_spikes_df = pd.read_csv(PP_PATH + pp_file + r\"\\unmasked_spikes.csv\")\n",
    "#     else:\n",
    "#         log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\log_p_hist.csv\")\n",
    "\n",
    "#         spikes_file = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + mouse_session_recording + '.txt'\n",
    "#         neuron_ids, spike_times= [], []\n",
    "#         with open(spikes_file) as f:\n",
    "#             for (i, line) in enumerate(f.readlines()):\n",
    "#                 [neuron_id, spike_time] = line.split(' ', 1)\n",
    "#                 spike_time = eval(spike_time.split('\\n')[0])\n",
    "#                 neuron_id = eval(neuron_id.split('\\t')[0])\n",
    "#                 spike_times.append(spike_time)\n",
    "#                 neuron_ids.append(neuron_id)\n",
    "#         unmasked_spikes_df = pd.DataFrame({'neuron':neuron_ids,'timestamp':spike_times}) \n",
    "\n",
    "#     bkgd_log_proportions_array = pd.read_csv(PP_PATH + pp_file + r\"\\bkgd_log_proportions_array.csv\")\n",
    "\n",
    "\n",
    "#     # Opening JSON file\n",
    "#     f = open(PP_PATH + pp_file + r'\\config_file.json')\n",
    "#     # returns JSON object as a dictionary\n",
    "#     config = eval(json.load(f))\n",
    "#     print(f'      done')\n",
    "\n",
    "#     ## LOAD behaviour data\n",
    "#     print('\\n')\n",
    "#     print(\"LOADING BEHAV DATA\")\n",
    "\n",
    "#     ## load in the timespan used for pppseq:\n",
    "#     input_params_path = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + ('params_' + mouse_session_recording +'.json')\n",
    "#     # Opening JSON file\n",
    "#     f = open(input_params_path)\n",
    "#     # returns JSON object as \n",
    "#     # a dictionary\n",
    "#     input_config = json.load(f)\n",
    "#     behav_time_interval_start = input_config['time_span']\n",
    "#     print(f\"      A corresponding time span has been found. Time span set to {behav_time_interval_start}\")\n",
    "\n",
    "#     ### load in data:\n",
    "#     for sub_file in os.listdir(dat_path + '\\\\behav_sync\\\\'):\n",
    "#         if 'task' in sub_file:\n",
    "#             behav_sync_path = dat_path + '\\\\behav_sync\\\\' + sub_file +'\\\\'\n",
    "#     behav_sync = pd.read_csv(behav_sync_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "#     transitions = pd.read_csv(behav_sync_path + 'Transition_data_sync.csv')\n",
    "\n",
    "#     return assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start\n",
    "\n",
    "def load_PPSEQ_data(PP_PATH,pp_file,dat_path,mouse_session_recording):\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "    ## LOAD \n",
    "    print(\"LOADING PPSEQ DATA\")\n",
    "    print('\\n')\n",
    "    #The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\n",
    "    assignment_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\assigment_hist_frame.csv\")\n",
    "\n",
    "    # latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\n",
    "    latent_event_history_df = pd.read_csv(PP_PATH + pp_file + r\"\\latent_event_hist.csv\")\n",
    "\n",
    "    # seq_type_log_proportions: log p of each type of sequence at each iteration\n",
    "    seq_type_log_proportions_df = pd.read_csv(PP_PATH + pp_file + r\"\\seq_type_log_proportions.csv\")\n",
    "\n",
    "    # neuron_responses.csv: iterations x neurons by 3(number of sequences). Each neuron has three parameters per sequence to describe how it is influenced by each sequence type. \n",
    "    # Each iteration these are resampled, therefore there are number of neurons by iterations by 3 by number of sequences of these numbers.\n",
    "    neuron_response_df = pd.read_csv(PP_PATH + pp_file + r\"\\neuron_response.csv\")\n",
    "\n",
    "    masking = False\n",
    "    for dat_files in os.listdir(PP_PATH + pp_file):\n",
    "        if 'unmasked_spikes' in dat_files:\n",
    "            masking = True\n",
    "            print('masking was used')\n",
    "\n",
    "    if masking == True:\n",
    "        #log_p_hist.csv: the history of the log_p of the model\n",
    "        log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\test_log_p_hist.csv\")\n",
    "\n",
    "        unmasked_spikes_df = pd.read_csv(PP_PATH + pp_file + r\"\\unmasked_spikes.csv\")\n",
    "    else:\n",
    "        log_p_hist_df = pd.read_csv(PP_PATH + pp_file + r\"\\log_p_hist.csv\")\n",
    "\n",
    "        spikes_file = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + mouse_session_recording + '.txt'\n",
    "        neuron_ids, spike_times= [], []\n",
    "        with open(spikes_file) as f:\n",
    "            for (i, line) in enumerate(f.readlines()):\n",
    "                neuron_id, spike_time = line.split('\\t')\n",
    "                spike_time = float(spike_time.strip())\n",
    "                neuron_id = float(neuron_id)\n",
    "                spike_times.append(spike_time)\n",
    "                neuron_ids.append(neuron_id)\n",
    "        unmasked_spikes_df = pd.DataFrame({'neuron':neuron_ids,'timestamp':spike_times}) \n",
    "        bkgd_log_proportions_array = pd.read_csv(PP_PATH + pp_file + r\"\\bkgd_log_proportions_array.csv\")\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open(PP_PATH + pp_file + r'\\config_file.json')\n",
    "    # returns JSON object as a dictionary\n",
    "    config = eval(json.load(f))\n",
    "    print(f'      done')\n",
    "\n",
    "    ## LOAD behaviour data\n",
    "    print('\\n')\n",
    "    print(\"LOADING BEHAV DATA\")\n",
    "\n",
    "    ## load in the timespan used for pppseq:\n",
    "    input_params_path = os.path.join(PP_PATH + pp_file,'trainingData\\\\') + ('params_' + mouse_session_recording +'.json')\n",
    "    # Opening JSON file\n",
    "    f = open(input_params_path)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    input_config = json.load(f)\n",
    "    behav_time_interval_start = input_config['time_span']\n",
    "    print(f\"      A corresponding time span has been found. Time span set to {behav_time_interval_start}\")\n",
    "\n",
    "    ### load in data:\n",
    "    for sub_file in os.listdir(dat_path + '\\\\behav_sync\\\\'):\n",
    "        if 'task' in sub_file:\n",
    "            behav_sync_path = dat_path + '\\\\behav_sync\\\\' + sub_file +'\\\\'\n",
    "    behav_sync = pd.read_csv(behav_sync_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "    transitions = pd.read_csv(behav_sync_path + 'Transition_data_sync.csv')\n",
    "\n",
    "    return assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start\n",
    "\n",
    "\n",
    "\n",
    "def plot_save_log_l_curve(log_p_hist_df,save_path):\n",
    "    # find 95% of growth value and when it crossed this\n",
    "    max_ = max(log_p_hist_df.x1)\n",
    "    min_ = min(log_p_hist_df.x1)\n",
    "    growth = max_ - min_\n",
    "    _prcntile =  max_ - (0.02 * growth)\n",
    "\n",
    "    ## model log likley hood curve\n",
    "    plt.plot(log_p_hist_df.x1)\n",
    "    plt.axhline(y=_prcntile, color='r', linestyle='--')\n",
    "\n",
    "    SaveFig('log_l_curve.png',save_path)\n",
    "    \n",
    "def plot_data_raster(behav_time_interval_start, spikes_df, neuron_index, colors, save_path):\n",
    "    # calculate interval timings and end points\n",
    "    interval_lengths = []\n",
    "    for interval in behav_time_interval_start:\n",
    "        interval_lengths += [np.diff(interval)[0]]\n",
    "    total_time = sum(interval_lengths)\n",
    "    interval_end_points = np.cumsum(interval_lengths)\n",
    "\n",
    "    # Plot sequences - basic\n",
    "    timeframe = [0, total_time]\n",
    "    mask = (spikes_df.timestamp > timeframe[0]) * (spikes_df.timestamp < timeframe[-1])\n",
    "\n",
    "    # Define neuron order\n",
    "    neuron_permute_loc = np.zeros(len(neuron_index))\n",
    "    for i in range(len(neuron_index)):\n",
    "        neuron_permute_loc[i] = int(list(neuron_index).index(i))\n",
    "    neuron_order = neuron_permute_loc[(spikes_df.neuron - 1).astype(int)]\n",
    "\n",
    "    # Plotting\n",
    "    fig, [ax, ax2] = plt.subplots(2, 1, figsize=(20, 20))\n",
    "\n",
    "    # Plot background in grey\n",
    "    background_keep_mask = (spikes_df[mask].sequence_type_adjusted < 0) | (spikes_df[mask].sequence_type_adjusted >= 7.0)\n",
    "    ax.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],\n",
    "               marker='o', s=40, linewidth=0, color='lightgrey', alpha=0.3)\n",
    "    c_ = np.array(colors)[spikes_df[mask][background_keep_mask].sequence_type_adjusted.values.astype(int)]\n",
    "    ax2.scatter(spikes_df[mask][background_keep_mask].timestamp, neuron_order[mask][background_keep_mask],\n",
    "                marker='o', s=40, linewidth=0, color=c_, alpha=0.3)\n",
    "    ax2.set_title('extra sequences and background only')\n",
    "\n",
    "    # Plot spikes without background\n",
    "    background_remove_mask = (spikes_df[mask].sequence_type_adjusted >= 0) * \\\n",
    "                             (spikes_df[mask].sequence_type_adjusted != 7.0) * \\\n",
    "                             (spikes_df[mask].sequence_type_adjusted != 8.0)\n",
    "    c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "    ax.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],\n",
    "               marker='o', s=40, linewidth=0, color=c_, alpha=1)\n",
    "    ax.set_title('held sequences in color and extra sequences + background in grey')\n",
    "\n",
    "    for end_p in interval_end_points:\n",
    "        ax.axvline(x=end_p, color='k')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    return interval_end_points,neuron_order\n",
    "\n",
    "\n",
    "def conactinate_nth_items(startlist):\n",
    "    concatinated_column_vectors = []\n",
    "    for c in range(len(max(startlist, key=len))):\n",
    "        column = []\n",
    "        for t in range(len(startlist)):\n",
    "            if c <= len(startlist[t])-1:\n",
    "                column = column + [startlist[t][c]]\n",
    "        concatinated_column_vectors.append(column)\n",
    "    return concatinated_column_vectors\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def split_list(numbers):\n",
    "    chunks = []\n",
    "    indices = []\n",
    "    current_chunk = []\n",
    "    current_indices = []\n",
    "    for i, num in enumerate(numbers):\n",
    "        if num == 0:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                indices.append(current_indices)\n",
    "                current_chunk = []\n",
    "                current_indices = []\n",
    "        else:\n",
    "            current_chunk.append(num)\n",
    "            current_indices.append(i)\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        indices.append(current_indices)\n",
    "    return chunks, indices\n",
    "\n",
    "def cluster_events(start_times, end_times, threshold):\n",
    "    clusters = []\n",
    "    for i in range(len(start_times)):\n",
    "        event_added = False\n",
    "        for cluster in clusters:\n",
    "            for index in cluster:\n",
    "                if (start_times[i] <= end_times[index] + threshold and end_times[i] >= start_times[index] - threshold):\n",
    "                    cluster.append(i)\n",
    "                    event_added = True\n",
    "                    break\n",
    "            if event_added:\n",
    "                break\n",
    "        if not event_added:\n",
    "            clusters.append([i])\n",
    "    return clusters\n",
    "\n",
    "def catagorize_seqs(real_order,num_dominant_seqs,order):\n",
    "    \n",
    "    #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "    if not len(real_order) == num_dominant_seqs:\n",
    "        dominant = real_order[0:num_dominant_seqs]\n",
    "        other_ = real_order[num_dominant_seqs::]\n",
    "    else:\n",
    "        dominant = real_order\n",
    "        other_ = []\n",
    "\n",
    "    amounts = []\n",
    "    relative_amounts = []\n",
    "    pair_outcomes = []\n",
    "    pairs = []\n",
    "    for sequence in order:\n",
    "        ordered = 0\n",
    "        reverse = 0\n",
    "        repeat = 0\n",
    "        misordered = 0\n",
    "        non_to_task = 0\n",
    "        task_to_non = 0\n",
    "        other = 0\n",
    "        for index,element in enumerate(sequence[0:-1]):\n",
    "            pair = [element,sequence[index+1]]\n",
    "            outcome = (logic_machine_for_pair_catagorisation(pair,dominant,other_))\n",
    "            if outcome == 'ordered':\n",
    "                ordered +=1\n",
    "            elif outcome == 'reverse':\n",
    "                reverse +=1\n",
    "            elif outcome == 'repeat':\n",
    "                repeat +=1\n",
    "            elif outcome == 'misordered':\n",
    "                misordered +=1\n",
    "            elif outcome == 'task to other':\n",
    "                task_to_non +=1\n",
    "            elif outcome == 'other to task':\n",
    "                non_to_task +=1\n",
    "            elif outcome == 'other':\n",
    "                other +=1\n",
    "            pairs += [pair]\n",
    "            pair_outcomes += [[outcome]]\n",
    "        pairs += [[None]]\n",
    "        pair_outcomes += [[None]]\n",
    "\n",
    "        relative_amounts += [list(np.array([ordered,reverse,repeat,misordered,non_to_task,task_to_non,other])/(len(sequence)-1))]\n",
    "        amounts += [[ordered,reverse,repeat,misordered,non_to_task,task_to_non,other]]\n",
    "        \n",
    "    return relative_amounts, amounts,pair_outcomes,pairs\n",
    "\n",
    "def logic_machine_for_pair_catagorisation(pair,dominant,other):\n",
    "    # if first one in dominant check for ordering:\n",
    "    if pair[0] in dominant and pair[-1] in dominant:\n",
    "        if pair_in_sequence(pair,dominant):\n",
    "            return('ordered')\n",
    "        elif pair_in_sequence(pair,dominant[::-1]):\n",
    "            return('reverse')\n",
    "        elif pair[-1] == pair[0]:\n",
    "            return('repeat')\n",
    "        elif pair[-1] in dominant:\n",
    "            return('misordered') \n",
    "    # if its not these  options then check if it could be in the extra task seqs\n",
    "    elif pair[0] in  (dominant + other) and pair[-1] in  (dominant + other):\n",
    "        for item in other:\n",
    "            if pair[0] in  (dominant + [item]):\n",
    "                if pair_in_sequence(pair,(dominant + [item])):\n",
    "                    return('ordered')\n",
    "                elif pair_in_sequence(pair,(dominant + [item])[::-1]):\n",
    "                    return('reverse')\n",
    "                elif pair[-1] == pair[0]:\n",
    "                    return('repeat')\n",
    "                elif pair[-1] in (dominant + [item]):\n",
    "                    return('misordered')  \n",
    "        # if not this then check if both are in the extra seqs (and are not a repeat):\n",
    "        if pair[0] in other and pair[-1] in other:\n",
    "            if not pair[-1] == pair[0]: \n",
    "                return('ordered')\n",
    "    else:\n",
    "        # if item 1 is in but item 2 isnt then task to other \n",
    "        if pair[0] in  (dominant + other):\n",
    "            if not pair[-1] in  (dominant + other):\n",
    "                return('task to other')\n",
    "        # if item 2 is in but item 1 isnt then other to task \n",
    "        elif not pair[0] in  (dominant + other):\n",
    "            if pair[-1] in  (dominant + other):\n",
    "                return('other to task')\n",
    "            else:\n",
    "                return('other')\n",
    "    return print('ERROR!')\n",
    "\n",
    "def pair_in_sequence(pair, sequence):\n",
    "    for i in range(len(sequence) - 1):\n",
    "        if sequence[i] == pair[0] and sequence[i + 1] == pair[1]:\n",
    "            return True\n",
    "        # because its ciruclar:\n",
    "        elif sequence[-1] == pair[0] and sequence[0] == pair[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_inds_for_seq_groups(lst):\n",
    "    groups = []\n",
    "    new = True\n",
    "    for ind,item in enumerate(lst):\n",
    "        if new:\n",
    "            if item > 0:\n",
    "                start = ind\n",
    "                new = False\n",
    "        else:\n",
    "            if item == 0:\n",
    "                end = ind-1\n",
    "                groups.append((start, end))\n",
    "                new = True\n",
    "    return groups\n",
    "\n",
    "\n",
    "def plot_event_transitions(event_transitions,axes,title):\n",
    "    transitions = list(event_transitions.keys())\n",
    "    counts = list(event_transitions.values())\n",
    "\n",
    "    # Create a bar chart\n",
    "    axes.bar(range(len(transitions)), counts,alpha = 0.5)\n",
    "\n",
    "    # Set x-axis labels\n",
    "    labels = [str(transition) for transition in transitions]\n",
    "    axes.set_xticks(range(len(transitions)), labels, rotation=90)\n",
    "\n",
    "    # Set y-axis label\n",
    "    axes.set_ylabel('Normalized Occurrences %')\n",
    "\n",
    "    # Add title\n",
    "    axes.set_title('Event Transitions' + title)\n",
    "\n",
    "def count_event_transitions(event_list):\n",
    "    \n",
    "    ### change code so that it sets each dictionary with these pairs a sa starting point! \n",
    "    from itertools import product\n",
    "    numbers = range(1, 7)  # Numbers 1 to 6a\n",
    "    all_possible_pairs = list(product(numbers, repeat=2))\n",
    "\n",
    "    transitions = {key: 0 for key in all_possible_pairs}  # Create an empty dictionary with the defined keys\n",
    "    \n",
    "    total_transitions = len(event_list) - 1\n",
    "    \n",
    "    for i in range(total_transitions):\n",
    "        current_event = event_list[i]\n",
    "        next_event = event_list[i + 1]\n",
    "        transition = (current_event, next_event)\n",
    "        if transition in transitions:\n",
    "            transitions[transition] += 1\n",
    "        else:\n",
    "            transitions[transition] = 1\n",
    "    \n",
    "    normalized_transitions = {}\n",
    "    for transition, count in transitions.items():\n",
    "        normalized_count = count / total_transitions * 100\n",
    "        normalized_transitions[transition] = normalized_count\n",
    "    \n",
    "    return normalized_transitions\n",
    "\n",
    "\n",
    "def normalize_counts_to_percentages(events_dict):\n",
    "    total_count = sum(events_dict.values())\n",
    "    normalized_dict = {}\n",
    "\n",
    "    for event, count in events_dict.items():\n",
    "        percentage = (count / total_count) * 100\n",
    "        normalized_dict[event] = percentage\n",
    "\n",
    "    return normalized_dict\n",
    "\n",
    "def SaveFig_noclose(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637acc0",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1066: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:1066: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\Emmett Thompson\\AppData\\Local\\Temp\\ipykernel_50024\\3825370069.py:1066: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  behav_sync = pd.read_csv(dat_path + r'\\behav_sync\\\\' + file + '\\Behav_Ephys_Camera_Sync.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/12-------------------------------------------------------------------------\n",
      "ap5R_1_1_run_2605025_1136\n",
      "LOADING PPSEQ DATA\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m mir,mouse_session_recording,save_path,tracking_path,dat_path \u001b[38;5;241m=\u001b[39m load_in_paths(pp_file, PP_PATH, DAT_PATH)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m## load in PPseq output data\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start \u001b[38;5;241m=\u001b[39m \u001b[43mload_PPSEQ_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdat_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# plot out log l curve \u001b[39;00m\n\u001b[0;32m     37\u001b[0m plot_save_log_l_curve(log_p_hist_df,save_path)\n",
      "Cell \u001b[1;32mIn[3], line 280\u001b[0m, in \u001b[0;36mload_PPSEQ_data\u001b[1;34m(PP_PATH, pp_file, dat_path, mouse_session_recording)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m#The assignment history frame (assigment_hist_frame.csv): Spikes by iterations, how each spike is assigned to a sequence ID (in latent_event_hist) or to background (-1)\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m assignment_history_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPP_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpp_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43massigment_hist_frame.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# latent_event_hist.csv: history of latent events. All latent events across all iterations have a row\u001b[39;00m\n\u001b[0;32m    283\u001b[0m latent_event_history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(PP_PATH \u001b[38;5;241m+\u001b[39m pp_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlatent_event_hist.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## set paths\n",
    "\n",
    "\n",
    "ignore_list= []\n",
    "# PP_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\paper_submission\\post_sleep\\\\\"\n",
    "# PP_PATH =  \"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\output_data\\old_data_filter_method\\striatum\\Medium_pre_sleep\\8_seq\\\\\"\n",
    "\n",
    "# PP_PATH = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\Reveiw_Pre_sleep\\\\\"\n",
    "\n",
    "# PP_PATH = r\"Z:\\projects\\sequence_squad\\revision_data\\emmett_revisions\\circular_shuffle\\synthetic_data\\non_shuffled\\ppseq_output\\\\\"\n",
    "# DAT_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\animals\\\\\"\n",
    "\n",
    "PP_PATH = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\ppseq_data\\output_data\\striatum\\Post_sleep\\\\\"\n",
    "DAT_PATH = r\"Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\\"\n",
    "\n",
    "\n",
    "# PP_PATH = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\output_data\\striatum\\New_post_sleep_shuffle\\\\\"\n",
    "# PP_PATH = \"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\output_data\\old_data_filter_method\\striatum\\Medium_post_sleep\\8_seq\\\\\"\n",
    "\n",
    "######################################################################################################################################################################################################################\n",
    "# load in data, extract, filter PPseq spikes etc.\n",
    "# main filtering here:\n",
    "# backgroudn confidence - spike had to be classified as a seq type 75% of the time across the last 50 iterations to be kept.   \n",
    "######################################################################################################################################################################################################################\n",
    "\n",
    "for run_index,pp_file in enumerate(os.listdir(PP_PATH)):\n",
    "    \n",
    "#     pp_file= '262_1_4_run_2106023_2357'\n",
    "    \n",
    "    if run_index >-1 and 'run' in pp_file:\n",
    "        # load in paths for that specific mouse and recording\n",
    "        mir,mouse_session_recording,save_path,tracking_path,dat_path = load_in_paths(pp_file, PP_PATH, DAT_PATH,run_index,ignore_list)\n",
    "        \n",
    "        ## load in PPseq output data\n",
    "        assignment_history_df,latent_event_history_df,seq_type_log_proportions_df,neuron_response_df,log_p_hist_df,unmasked_spikes_df,bkgd_log_proportions_array,behav_sync,transitions,behav_time_interval_start = load_PPSEQ_data(PP_PATH,pp_file,dat_path,mir)\n",
    "        # plot out log l curve \n",
    "        plot_save_log_l_curve(log_p_hist_df,save_path)\n",
    "        \n",
    "    # ---filter_across_itterations---------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        # Initialize an empty df to store the result\n",
    "        seq_types_df = pd.DataFrame()\n",
    "        # Iterate through the range\n",
    "        # for iteration_ in tqdm(range(400, 500)):\n",
    "        for iteration_ in tqdm(range(250, 300)):\n",
    "            # Extract the relevant column from the assignment history dataframe\n",
    "            assignment_history_df_split = assignment_history_df[str(list(assignment_history_df)[iteration_])]\n",
    "            # Get the index of the -1 split markers in the latent event history dataframe\n",
    "            end_markers = latent_event_history_df.loc[latent_event_history_df['seq_type'] == -1.0].index\n",
    "            # Extract the relevant portion of the latent event history dataframe\n",
    "            latent_event_history_df_split =  latent_event_history_df[end_markers[iteration_-1]:end_markers[iteration_]]\n",
    "            # Create a dictionary from the dataframe for faster lookups\n",
    "            df_dict = latent_event_history_df_split.set_index('assignment_id')['seq_type'].to_dict()\n",
    "            # Match the sequence ID to the sequence type\n",
    "            seq_type = find_corresponding(assignment_history_df_split,df_dict)\n",
    "            # Append the result to the df\n",
    "            seq_types_df[str(iteration_+1)] = seq_type\n",
    "        proportion = []\n",
    "        seq_type = []\n",
    "        for index in tqdm(range(len(seq_types_df))):\n",
    "            row = seq_types_df.loc[index]\n",
    "            seq_type += [statistics.mode(row)] \n",
    "            proportion += [np.count_nonzero(row == statistics.mode(row)) / len(row)]\n",
    "        # add seq type to dataframe\n",
    "        unmasked_spikes_df['sequence_type'] = seq_type\n",
    "        # add seq type to dataframe\n",
    "        unmasked_spikes_df['seq_confidence'] = proportion\n",
    "        \n",
    "        # ## filter for background confidence :-------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        thresh = max(proportion) *.75 ### \n",
    "        plt.plot(np.sort(proportion)[::-1])\n",
    "        plt.axhline(y = thresh, color = 'r', linestyle = '-')\n",
    "        unmasked_spikes_df['sequence_type_adjusted'] = seq_type\n",
    "        unmasked_spikes_df.sequence_type_adjusted[np.where(unmasked_spikes_df.seq_confidence < thresh)[0]] = -1\n",
    "        SaveFig('filtering_curve.png',save_path)\n",
    "        \n",
    "        ## load in colors and order from awake data -------------------------------------------------------------------------------------------------------------------------------------------------------------------    \n",
    "        found = False\n",
    "        awake_PP_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\expert\\awake\\\\\"\n",
    "        for file_ in os.listdir(awake_PP_path):\n",
    "            if mouse_session_recording in file_:\n",
    "                awake_file = file_\n",
    "                found = True\n",
    "        if not found:\n",
    "            awake_PP_path = r\"Z:\\projects\\sequence_squad\\ppseq_finalised_publication_data\\learning\\awake\\\\\"\n",
    "            for file_ in os.listdir(awake_PP_path):\n",
    "                if mouse_session_recording in file_:\n",
    "                    awake_file = file_\n",
    "                    found = True\n",
    "\n",
    "        ordered_preferred_type = pd.read_pickle(awake_PP_path + awake_file + r\"\\analysis_output\\reordered_recolored\\\\\" + 'ordered_preferred_type')\n",
    "        neuron_index = pd.read_pickle(awake_PP_path + awake_file + r\"\\analysis_output\\reordered_recolored\\\\\" + 'neuron_index')\n",
    "        colors = pd.read_pickle(awake_PP_path + awake_file + r\"\\analysis_output\\reordered_recolored\\\\\" + 'colors')\n",
    "        spikes_df = unmasked_spikes_df\n",
    "        colors += ['pink','lightblue', 'k'] \n",
    "        \n",
    "        ############### plot simple rasters ------------------------------\n",
    "        interval_end_points,neuron_order = plot_data_raster(behav_time_interval_start, spikes_df, neuron_index, colors, save_path + 'all_data_raster.png')\n",
    "\n",
    "\n",
    "        ######################################################################################################################################################################################################################\n",
    "        # filter replays \n",
    "        # spikes for each type are binned into 40ms time bins.  ## used to be 20ms - for phd thesis\n",
    "        # if spikes occur continuously across bins then this is classified as a single contiguous replay event \n",
    "\n",
    "        # if at least 5 spikes and 3 neurons were involved its called a replay.\n",
    "\n",
    "        ######################################################################################################################################################################################################################    \n",
    "        # TEST - was 5\n",
    "        min_spikes_filter = 7\n",
    "        #3\n",
    "        min_neurons_involved_filter = 5\n",
    "        bin_size = 0.02\n",
    "\n",
    "        chunk_paths = []\n",
    "        # run for each time chunk: \n",
    "        for index_,interval_start in enumerate([0] + list(interval_end_points)[0:-1]):\n",
    "\n",
    "            ## define path for this chunk\n",
    "            chunk_path = save_path + 'chunk' + str(index_+1)+'_' + str(behav_time_interval_start[index_][0]) + 'to' + str(behav_time_interval_start[index_][1]) + '\\\\'\n",
    "            chunk_paths += [chunk_path]\n",
    "            if not os.path.isdir(chunk_path):\n",
    "                os.mkdir(chunk_path)\n",
    "\n",
    "            ## save out time interval for current chunk:\n",
    "            \n",
    "            np.save(chunk_path+ 'chunk_time_interval.npy',np.array(behav_time_interval_start[index_]))\n",
    "\n",
    "            timeframe = [interval_start,interval_end_points[index_]-1]\n",
    "            total_time = np.diff(timeframe)[0]+1\n",
    "            #mask\n",
    "            # spikemask for plotting\n",
    "            mask = (spikes_df.timestamp>timeframe[0])*(spikes_df.timestamp<timeframe[-1])\n",
    "\n",
    "            fig, [ax1,ax2] = plt.subplots(2, 1,figsize=(35, 15))\n",
    "\n",
    "            # plot spikes without background\n",
    "            background_remove_mask = (spikes_df[mask].sequence_type_adjusted > -1) * (spikes_df[mask].sequence_type_adjusted < 7)\n",
    "            c_ = np.array(colors)[spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "            ax1.scatter(spikes_df[mask][background_remove_mask].timestamp, neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "\n",
    "            # define chunk data:\n",
    "            chunk_mask = (spikes_df.timestamp > interval_start) * (spikes_df.timestamp < interval_end_points[index_])\n",
    "            chunk_df = spikes_df[chunk_mask].copy()\n",
    "            chunk_df = chunk_df.reset_index(drop=True)\n",
    "\n",
    "#             if not run_index == 16:\n",
    "            #save out chunk data \n",
    "            chunk_df.to_csv(chunk_path + 'unfiltered_spikes_data.csv', index=False)\n",
    "\n",
    "            ### bin the spiking for each seq type\n",
    "            seqs = np.unique(chunk_df.sequence_type_adjusted)\n",
    "            seq_spikes = []\n",
    "            seq_neurons = []\n",
    "            for seq_type_ in seqs:  \n",
    "                seq_spikes += [chunk_df.timestamp[np.where(chunk_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "                seq_neurons += [chunk_df.neuron[np.where(chunk_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "            binned_seq_r_events = []\n",
    "            for spikes_ in seq_spikes:\n",
    "                # Use the numpy.histogram function to bin the data\n",
    "                hist, bins = np.histogram(spikes_, bins=np.arange(interval_start, interval_end_points[index_], bin_size))\n",
    "                binned_seq_r_events += [list(hist)]\n",
    "            strt_ = int(timeframe[0]/bin_size)\n",
    "            end_ = int(timeframe[1]/bin_size)   \n",
    "\n",
    "            # loop over smoothed data for eeahc sew and create cluster chunks from this: \n",
    "            r_start_ = []\n",
    "            r_end_ = []\n",
    "            r_seq_type = []\n",
    "            for _index_,sequence_type in enumerate(seqs):\n",
    "                sequence_type = int(sequence_type)\n",
    "                # if seq type not background or other new types:\n",
    "                if sequence_type > 0 and sequence_type <= 6:\n",
    "                    print(sequence_type)\n",
    "\n",
    "                    # smooth over binned spikes:\n",
    "                    smoothed_binned_spikes = convolve_movmean(binned_seq_r_events[_index_],2)\n",
    "\n",
    "                    # plot smoothed counts for this sequence \n",
    "                    time_bins = np.arange(timeframe[0],timeframe[0]+np.diff(timeframe)+1,bin_size)\n",
    "                    ax2.plot(time_bins[0:-1],smoothed_binned_spikes, c = colors[sequence_type])\n",
    "                    ax2.sharex(ax1)\n",
    "\n",
    "                    ## split smoothed replay into single events\n",
    "                    replay_chunks,indices = split_list(list(smoothed_binned_spikes))\n",
    "\n",
    "                    for index,chunk in enumerate(replay_chunks):\n",
    "                        # start and end of chunk event:\n",
    "                        r_seq_type += [sequence_type]\n",
    "                        r_start_ += [time_bins[indices[index][0]]]\n",
    "                        r_end_ += [time_bins[indices[index][-1]]]\n",
    "\n",
    "            ## perform filtering on cluster chunks\n",
    "            #filter1 number of spikes\n",
    "            #filter2 number of neurons \n",
    "            filtered_cluster_sequence_type = []\n",
    "            filtered_cluster_timestamps = []\n",
    "            filtered_num_spikes = []\n",
    "            filtered_num_neurons = []\n",
    "            filtered_first_spike_time = []\n",
    "            filtered_last_spike_time = []\n",
    "            filtered_cluster_neurons = []\n",
    "            event_length = []\n",
    "            cluster_neuron_order = []\n",
    "            for i in range(len(r_seq_type)):\n",
    "                r_event_df = chunk_df[(chunk_df.timestamp >= r_start_[i]) * (chunk_df.timestamp <= r_end_[i])].copy()\n",
    "                neuron_orders = neuron_order[chunk_mask][(chunk_df.timestamp >= r_start_[i]) * (chunk_df.timestamp <= r_end_[i])]\n",
    "                # only neurons of the seq type\n",
    "                neuron_orders = neuron_orders[r_event_df.sequence_type_adjusted == r_seq_type[i]]\n",
    "                r_event_df = r_event_df[r_event_df.sequence_type_adjusted == r_seq_type[i]]\n",
    "                if len(r_event_df) > 0:\n",
    "                    num_spikes = len(r_event_df.sequence_type_adjusted)\n",
    "                    num_neurons = len(r_event_df.neuron.unique())\n",
    "                    first_spike = min(r_event_df.timestamp)\n",
    "                    last_spike = max(r_event_df.timestamp)\n",
    "\n",
    "                    if num_spikes >= min_spikes_filter:\n",
    "                        if num_neurons >= min_neurons_involved_filter:\n",
    "                            ax1.axvspan(first_spike,last_spike, color=colors[r_seq_type[i]], alpha=0.5)\n",
    "                            ## save replay clusters out!\n",
    "                            filtered_cluster_sequence_type += [r_seq_type[i]]\n",
    "                            filtered_cluster_timestamps += [list(r_event_df.timestamp.values)]\n",
    "                            filtered_cluster_neurons += [list(r_event_df.neuron.values)]\n",
    "                            filtered_num_spikes += [num_spikes]\n",
    "                            filtered_num_neurons += [len(r_event_df.neuron.unique())]\n",
    "                            filtered_first_spike_time += [first_spike]\n",
    "                            filtered_last_spike_time += [last_spike]\n",
    "                            event_length += [last_spike-first_spike]\n",
    "                            cluster_neuron_order += [neuron_orders]\n",
    "\n",
    "            filtered_r_clusters_df = pd.DataFrame({'cluster_seq_type':filtered_cluster_sequence_type, 'num_spikes':filtered_num_spikes,  'num_neurons': filtered_num_neurons,'first_spike_time':filtered_first_spike_time,'event_length':event_length,'last_spike_time':filtered_last_spike_time, 'cluster_spike_times':filtered_cluster_timestamps,'cluster_neurons':filtered_cluster_neurons,'spike_plotting_order': cluster_neuron_order})\n",
    "            filtered_r_clusters_df.to_csv(chunk_path + 'filtered_replay_clusters_df.csv', index=False)    \n",
    "\n",
    "            SaveFig_noclose(mouse_session_recording + 'non_zoomed_data_filtering_chunk_'+ str(index_+1) + '.png',os.path.join(PP_PATH,'quick_view_output/'))\n",
    "            \n",
    "            ax1.set_xlim(timeframe[0]+140,timeframe[0]+150)\n",
    "            ax2.set_xlim(timeframe[0]+140,timeframe[0]+150)\n",
    "\n",
    "            SaveFig_noclose(mouse_session_recording + 'zoomed_data_filtering_chunk_'+ str(index_+1) + '.png',os.path.join(PP_PATH,'quick_view_output/'))\n",
    "            SaveFig('zoomed_data_filtering_chunk_'+ str(index_+1) + '.png',chunk_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ######################################################################################################################################################################################################################\n",
    "            # cluster these events into coactive chunks\n",
    "            # if events are within 200ms of each other then they are clustered together\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "            #cluster events\n",
    "            start_times = filtered_r_clusters_df.first_spike_time.values\n",
    "            end_times = filtered_r_clusters_df.last_spike_time.values\n",
    "            event_proximity_filter =  0.2 #s (how close events have to be to each other to be clustered together as coacitve \n",
    "\n",
    "            clustered_events = cluster_events(start_times, end_times,event_proximity_filter)\n",
    "\n",
    "\n",
    "            spike_times = [item for sublist in filtered_r_clusters_df.cluster_spike_times.values for item in sublist]    \n",
    "            neuron_orders = [item for sublist in filtered_r_clusters_df.spike_plotting_order.values for item in sublist]    \n",
    "\n",
    "            cst = []\n",
    "            for index,item in enumerate(filtered_r_clusters_df.cluster_spike_times):\n",
    "                cst += len(item)*[filtered_r_clusters_df.cluster_seq_type.values[index]]\n",
    "            c_ = np.array(colors)[np.array(cst)]\n",
    "            fig, [ax1,ax2] = plt.subplots(2, 1,figsize=(20, 10))\n",
    "\n",
    "            ax1.scatter(spike_times, neuron_orders,marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "            ax2.scatter(spike_times, neuron_orders,marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "\n",
    "            for i in range(len(start_times)):\n",
    "                ax1.axvspan(start_times[i],end_times[i],color = colors[filtered_r_clusters_df.cluster_seq_type.values[i]],alpha = 0.2)\n",
    "\n",
    "            for cluster in clustered_events:\n",
    "                starts = []\n",
    "                ends = []\n",
    "                for item in cluster:\n",
    "                    starts += [filtered_r_clusters_df.first_spike_time[item]]\n",
    "                    ends += [filtered_r_clusters_df.last_spike_time[item]]\n",
    "                ax2.axvspan(min(starts),max(ends),color = 'grey' ,alpha = 0.2)\n",
    "\n",
    "            ax1.set_title('single R events')\n",
    "            ax2.set_title('co-ative R event groups')\n",
    "            ax1.set_xlim(timeframe[0]+60,timeframe[0]+90)\n",
    "            ax2.set_xlim(timeframe[0]+60,timeframe[0]+90)\n",
    "\n",
    "            SaveFig('zoomed_r_event_co-occurance__chunk_'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            cluster_group = np.zeros(len(filtered_r_clusters_df))\n",
    "            for index,cluster in enumerate(clustered_events):\n",
    "                for item in cluster:\n",
    "                    cluster_group[item] = int(index)\n",
    "\n",
    "            # add this to df\n",
    "            filtered_r_clusters_df['coactive_cluster_group'] = cluster_group\n",
    "            # save this out \n",
    "            filtered_r_clusters_df.to_csv(chunk_path + 'filtered_replay_clusters_df.csv', index=False) \n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot single event length\n",
    "            # all single events (not just the ones that arent in coactive clusters)\n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "            fig, ax = plt.subplots(1, 1,figsize=(2, 7))\n",
    "            ax.plot(np.zeros(len(filtered_r_clusters_df.event_length)),filtered_r_clusters_df.event_length,'o',alpha = 0.4)\n",
    "\n",
    "            plt_df = pd.DataFrame({'x':['single event length']*len(filtered_r_clusters_df.event_length) ,'time': filtered_r_clusters_df.event_length})\n",
    "            ax=sns.boxplot( y = 'time', x = 'x', data = plt_df, color = 'blue', width = .2, zorder = 10,\\\n",
    "                        showcaps = True, boxprops = {'facecolor':'none', \"zorder\":10},\\\n",
    "                        showfliers=False, whiskerprops = {'linewidth':2, \"zorder\":10},\\\n",
    "                           saturation = 1, orient = 'v',ax = ax)\n",
    "\n",
    "\n",
    "            ax.plot(0,np.mean(filtered_r_clusters_df.event_length),'_',color = 'firebrick',markersize = 10)\n",
    "\n",
    "            SaveFig('single_R_event_lenght__chunk_'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot event rate over time in chunk\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "            # List of event times\n",
    "            event_times = filtered_r_clusters_df.first_spike_time  # Add your event times here\n",
    "\n",
    "\n",
    "            bin_size = np.round((np.diff(timeframe)[[0]][0])/60)\n",
    "\n",
    "            # Calculate the number of events per minute\n",
    "            events_per_time, bins = np.histogram(event_times, bins=np.arange(timeframe[0],timeframe[0]+np.diff(timeframe)+1,bin_size))\n",
    "\n",
    "            # Calculate the mean events per minute\n",
    "            mean_events_per_time = np.mean(events_per_time)\n",
    "\n",
    "            # Create the figure and axes\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # Plot the number of events per minute\n",
    "            ax.plot(bins[:-1], events_per_time, drawstyle='steps-post')\n",
    "\n",
    "            # Plot the vertical line representing the mean events per minute\n",
    "            ax.axhline(y=mean_events_per_time, color='r', linestyle='--', label='Mean')\n",
    "\n",
    "            # Set the x-axis label\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            # Set the y-axis label\n",
    "            ax.set_ylabel('Number of Events')\n",
    "            # Set the title\n",
    "            ax.set_title('Number of Events per min (approx)')\n",
    "\n",
    "            # Add a legend\n",
    "            ax.legend()\n",
    "\n",
    "            SaveFig('sinlge_R_events_over_time__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot frequency of coactive events by group \n",
    "            # save out these frequencies \n",
    "\n",
    "            ######################################################################################################################################################################################################################    \n",
    "\n",
    "\n",
    "            group_sizes = []\n",
    "            for coactive_group in filtered_r_clusters_df.coactive_cluster_group.unique():\n",
    "                coactive_group_mask = filtered_r_clusters_df.coactive_cluster_group == coactive_group\n",
    "                group_sizes += [len(filtered_r_clusters_df[coactive_group_mask])]\n",
    "\n",
    "            # Counting occurrences of each number\n",
    "            counts = {}\n",
    "            for num in group_sizes:\n",
    "                if num in counts:\n",
    "                    counts[num] += 1\n",
    "                else:\n",
    "                    counts[num] = 1\n",
    "\n",
    "            # Extracting numbers and their frequencies as separate lists\n",
    "            x = list(counts.keys())\n",
    "            y = list(counts.values())\n",
    "\n",
    "            # Creating the bar chart\n",
    "            plt.bar(x, y)\n",
    "\n",
    "            # Adding labels and title\n",
    "            plt.xlabel('Numbers')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('total frequency of coactive event sizes ')\n",
    "\n",
    "            SaveFig('coactive_frequencies__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            ### save out data:\n",
    "#             if not run_index == 16:\n",
    "            chunk_df.to_csv(chunk_path + 'unfiltered_spikes_data.csv', index=False)\n",
    "\n",
    "            # Specify the file path where you want to save the dictionary\n",
    "            file_path = chunk_path + 'coactive_frequencies.json'\n",
    "            # Open the file in write mode and use json.dump to save the dictionary\n",
    "            with open(file_path, \"w\") as file:\n",
    "                json.dump(counts, file)\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot motif frequenices over time for that chunk (1), total freqs by sequence (2) and task v non task (3)\n",
    "\n",
    "\n",
    "            # 1 #####################################################################################################################################################################################################################    \n",
    "\n",
    "            # for each group size, find these groups and find their times\n",
    "            all_group_event_times = {}\n",
    "            for group_size in np.unique(group_sizes):\n",
    "                group_event_times = []\n",
    "                np.where(np.array(group_sizes) == group_size)\n",
    "                groups_of_size_inds = np.where(np.array(group_sizes) == group_size)[0]\n",
    "                groups_of_size = list(filtered_r_clusters_df.coactive_cluster_group[groups_of_size_inds])\n",
    "                for coactive_group in groups_of_size:\n",
    "                    coactive_group_mask = filtered_r_clusters_df.coactive_cluster_group == coactive_group\n",
    "                    group_event_times += [min(filtered_r_clusters_df[coactive_group_mask].first_spike_time)]\n",
    "                all_group_event_times[group_size] = group_event_times\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            for item in all_group_event_times.keys():\n",
    "                ax.plot(all_group_event_times[item],np.ones(len(all_group_event_times[item]))*(item+1),'x')    \n",
    "            # for i,group_times in enumerate(all_group_event_times):\n",
    "            #     ax.plot(group_times,np.ones(len(group_times))*(i+1),'x')\n",
    "            # Set the x-axis label\n",
    "            ax.set_xlabel('coative event times (s)')\n",
    "            # Set the y-axis label\n",
    "            ax.set_ylabel('number of R events coactive together')\n",
    "\n",
    "            SaveFig('coactive_event_times__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            # Specify the file path where you want to save the dictionary\n",
    "            file_path = chunk_path + 'coactive_event_times.pickle'\n",
    "            with open(file_path, 'wb') as handle:\n",
    "                pickle.dump(all_group_event_times, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            # 2 ####################################################################################################################################################################################################\n",
    "\n",
    "            awake_PP_path = r\"Z:\\projects\\sequence_squad\\organised_data\\ppseq_data\\finalised_output\\striatum\\awake\\\\\"\n",
    "\n",
    "            for index_,M_I_R in enumerate(os.listdir(awake_PP_path)):\n",
    "                if not M_I_R == 'not_suitable':\n",
    "                    mouse = '_'.join(M_I_R.split('_')[0:3])\n",
    "                    if mouse == mir:\n",
    "                        print(mouse)\n",
    "                        c_path = awake_PP_path + M_I_R + r\"\\analysis_output\\reordered_recolored\\\\\" \n",
    "\n",
    "            sequence_order_df = pd.read_csv(awake_PP_path+\"sequence_order.csv\")\n",
    "\n",
    "            import ast\n",
    "            seq_order= ast.literal_eval(sequence_order_df[sequence_order_df.mir == mir].seq_order.values[0])\n",
    "            num_dominant_seqs = int(sequence_order_df[sequence_order_df.mir == mir].dominant_task_seqs)\n",
    "            extended_list = seq_order.copy()  # Create a copy of the original list\n",
    "            for num in range(6):\n",
    "                if num not in extended_list:\n",
    "                    extended_list += [num]\n",
    "            extended_list = np.array(extended_list)\n",
    "\n",
    "            # Counting occurrences of each number\n",
    "            counts = {}\n",
    "            for item in range(1,7):\n",
    "                counts[item] = list(filtered_r_clusters_df.cluster_seq_type).count(item)\n",
    "\n",
    "            # Extracting numbers and their frequencies as separate lists\n",
    "            x = list(counts.keys())\n",
    "            y = list(counts.values())\n",
    "\n",
    "            # reorder into seq order:\n",
    "            x_ordered = np.array(x)[extended_list]\n",
    "            y_ordered = np.array(y)[extended_list]\n",
    "\n",
    "            # Creating the bar chart\n",
    "            plt.bar([0,1,2,3,4,5],y_ordered, color = np.array(colors)[extended_list+1])\n",
    "\n",
    "\n",
    "            # Customizing x-axis labels\n",
    "            custom_labels = np.array(x_ordered).astype(str)\n",
    "            for index,item in enumerate(seq_order):\n",
    "                for i,label in enumerate(custom_labels):\n",
    "                    if str(item+1) == label: \n",
    "                        custom_labels[i] = custom_labels[i] + ' *'\n",
    "                        break\n",
    "            plt.xticks([0,1,2,3,4,5], custom_labels)\n",
    "\n",
    "            # Adding labels and title\n",
    "            plt.xlabel('motif     *task related')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(' frequency of motfs ')\n",
    "\n",
    "            SaveFig('motif_frequencies__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            # 3 ####################################################################################################################################################################################################\n",
    "            non_seqs = []\n",
    "            for i in range(6):\n",
    "                if not i in seq_order:\n",
    "                    non_seqs += [i]\n",
    "\n",
    "            fig, ax= plt.subplots(1, 1,figsize=(5, 5))\n",
    "            task_related_events = sum(np.array(y)[seq_order])\n",
    "            non_task_related_events = sum(np.array(y)[non_seqs])\n",
    "            ax.set_title('total_events')\n",
    "            ax.bar(['task related','non task related'],[task_related_events,non_task_related_events])\n",
    "\n",
    "            SaveFig('task-related_vs_non-task-related__chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            # save out this data: \n",
    "            np.save(chunk_path+'total_task_related_event.npy',task_related_events)\n",
    "            np.save(chunk_path+'total_nontask_related_event.npy',non_task_related_events)\n",
    "\n",
    "            np.save(chunk_path+'task_order_seqs.npy',seq_order)\n",
    "            np.save(chunk_path+'nontask_seqs.npy',non_seqs)\n",
    "\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # plot coaive_motif_transitions heatmap \n",
    "\n",
    "\n",
    "            #####################################################################################################################################################################################################################   \n",
    "\n",
    "            seq_orders = []\n",
    "            for coactive_group in filtered_r_clusters_df.coactive_cluster_group.unique():\n",
    "                coactive_group_mask = filtered_r_clusters_df.coactive_cluster_group == coactive_group\n",
    "                if len(filtered_r_clusters_df[coactive_group_mask]) > 1:\n",
    "                    # reorder base don first spike time order:\n",
    "                    re_ordering = np.argsort(filtered_r_clusters_df[coactive_group_mask].first_spike_time.values)\n",
    "                    seq_orders += [list(filtered_r_clusters_df[coactive_group_mask].cluster_seq_type.values[re_ordering])]\n",
    "\n",
    "            fragments =seq_orders\n",
    "\n",
    "            transitions = {}\n",
    "\n",
    "            # Iterate over each fragment\n",
    "            for fragment in fragments:\n",
    "                # Iterate over each pair of adjacent elements in the fragment\n",
    "                for i in range(len(fragment) - 1):\n",
    "                    current = fragment[i]\n",
    "                    next_element = fragment[i + 1]\n",
    "\n",
    "                    # Check if the current element is already a key in the transitions dictionary\n",
    "                    if current not in transitions:\n",
    "                        transitions[current] = {}\n",
    "\n",
    "                    # Check if the next element is already a key in the nested dictionary for the current element\n",
    "                    if next_element not in transitions[current]:\n",
    "                        transitions[current][next_element] = 1\n",
    "                    else:\n",
    "                        transitions[current][next_element] += 1\n",
    "\n",
    "            # Define the desired order of elements\n",
    "            element_order = list(extended_list+1)\n",
    "\n",
    "            element_order_reversed = list(reversed(element_order))\n",
    "\n",
    "            # Create an empty matrix with dimensions equal to the number of elements\n",
    "            matrix = np.zeros((len(element_order), len(element_order)))\n",
    "\n",
    "            # Fill the matrix with the transition counts\n",
    "\n",
    "            for i, current in enumerate(element_order):\n",
    "                for j, next_element in enumerate(element_order):\n",
    "                    if current in transitions and next_element in transitions[current]:\n",
    "                        matrix[i][j] = transitions[current][next_element]\n",
    "\n",
    "            # Normalize the matrix by column\n",
    "            matrix = matrix[::-1]\n",
    "            col_sums = matrix.sum(axis=0)\n",
    "            normalized_matrix = matrix / col_sums\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            im = ax.imshow(normalized_matrix, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "            # Add colored squares to represent the true sequence\n",
    "            logical_order = ast.literal_eval(sequence_order_df[sequence_order_df.mir == mir].seq_order.values[0])\n",
    "            for i, current in enumerate(element_order[0:len(logical_order)]):\n",
    "                for j, next_element in enumerate(element_order_reversed[0:len(logical_order)]):\n",
    "                    if current == next_element:\n",
    "                        j = j-1\n",
    "                        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='white')\n",
    "                        ax.add_patch(rect)\n",
    "\n",
    "            logical_order = ast.literal_eval(sequence_order_df[sequence_order_df.mir == mir].seq_order.values[0])\n",
    "            for i, current in enumerate(element_order_reversed[0:len(logical_order)]):\n",
    "                for j, next_element in enumerate(element_order[0:len(logical_order)]):\n",
    "                    if current == next_element:          \n",
    "                        i = i+1\n",
    "                        rect = plt.Rectangle((j-0.5, i-0.5), 1, 1, fill=False, edgecolor='grey')\n",
    "                        ax.add_patch(rect)\n",
    "\n",
    "            # Plot the heatmap\n",
    "\n",
    "            plt.colorbar(im,label='Normalized Transition Count')\n",
    "            plt.xticks(range(len(element_order)), element_order)\n",
    "            plt.yticks(range(len(element_order)), element_order_reversed)\n",
    "            plt.xlabel('first motif')\n",
    "            plt.ylabel('second motif')\n",
    "            plt.title('Transition Heatmap - white squares = forward order, grey = reverse')\n",
    "\n",
    "            # color labels by motif/seq colour\n",
    "            for i, tick_label in enumerate(ax.axes.get_yticklabels()):\n",
    "                tick_label.set_color(colors[element_order[::-1][i]])\n",
    "                tick_label.set_fontsize(\"15\")\n",
    "            for i, tick_label in enumerate(ax.axes.get_xticklabels()):\n",
    "                tick_label.set_color(colors[element_order[i]])\n",
    "                tick_label.set_fontsize(\"15\")\n",
    "\n",
    "            # plt.show()\n",
    "            SaveFig('coaive_motif_transitions_normalised___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # PPseq warp values used\n",
    "\n",
    "            # to extract warps... for each sequence type, across last 100 iterations, I find warp values that are reported for that that type between relevant window. Add them as relative values for each seq\n",
    "            # its possible for more than one warp value to contriubute to what i define as a sequence. so for each seq I mark the relative contributions of each warp to each sequence. I then sum these for all sequences. \n",
    "            #####################################################################################################################################################################################################################  \n",
    "\n",
    "\n",
    "            # Get the index of the -1 split markers in the latent event history dataframe\n",
    "            end_markers = latent_event_history_df.loc[latent_event_history_df['seq_type'] == -1.0].index\n",
    "            # Extract the relevant portion of the latent event history dataframe\n",
    "            latent_event_history_df_split_final_100 =  latent_event_history_df[end_markers[-100]:end_markers[-1]]\n",
    "\n",
    "\n",
    "            all_warps_present = latent_event_history_df_split.seq_warp.unique()\n",
    "            relative_warps_per_seq = []\n",
    "            for index, row in filtered_r_clusters_df.iterrows():\n",
    "                event_seq_type = row.cluster_seq_type\n",
    "                first_spiketime = row.first_spike_time\n",
    "                last_spiketime = row.last_spike_time\n",
    "                latent_df_seq = latent_event_history_df_split_final_100[latent_event_history_df_split_final_100.seq_type == event_seq_type]\n",
    "\n",
    "                seq_warps = list(latent_df_seq[(latent_df_seq.timestamp >= first_spiketime-0.05) * (latent_df_seq.timestamp <= last_spiketime+0.05)].seq_warp.values)\n",
    "                counts = []\n",
    "                for item in sorted(all_warps_present):\n",
    "                    counts += [seq_warps.count(item)]\n",
    "                relative_warps_per_seq += [list(np.array(counts)/sum(counts))]\n",
    "\n",
    "\n",
    "            relative__contributions_summed = []\n",
    "            for index,item in enumerate(conactinate_nth_items(relative_warps_per_seq)):\n",
    "                relative__contributions_summed += [np.nansum(item)]\n",
    "\n",
    "            labels = np.round(np.array(sorted(all_warps_present)),1).astype(str)\n",
    "\n",
    "            nrow = 1 \n",
    "            ncol = 1\n",
    "            fig, axs = plt.subplots(nrow, ncol,figsize=(15, 8))\n",
    "\n",
    "            for ind, ax in enumerate(fig.axes):\n",
    "\n",
    "                ax.plot(relative__contributions_summed)\n",
    "\n",
    "                ax.set_xticks(range(0,len(labels)))\n",
    "                ax.set_xticklabels(labels)\n",
    "\n",
    "            plt.xlabel('warp value')\n",
    "            plt.ylabel('summed realtive occrance ')\n",
    "            plt.title('Transition Heatmap - white squares = forward order, grey = reverse')\n",
    "\n",
    "\n",
    "            np.save(chunk_path+ 'summed_relative_warp_contributions.npy',relative__contributions_summed)\n",
    "\n",
    "            SaveFig('warp_values_averaged_acorss_iterations___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "            ######################################################################################################################################################################################################################\n",
    "            # the rest of this is stuff about catagorisation (ordered,reverse,repeat,misordered) and when the reward motif occurrs. I think I probbaly dont need it but am leaving it in for posterity \n",
    "\n",
    "            #####################################################################################################################################################################################################################          \n",
    "            ############################################################ catagorisation\n",
    "\n",
    "            ### step 1, how muhc of each classification do we have. \n",
    "\n",
    "\n",
    "            ### plot proportions for chunk\n",
    "            # eventually plot proportions over time \n",
    "            # save out this data + the time it happened - this is super important for both a timeing plot and LFP analysis. \n",
    "\n",
    "            # create empty df \n",
    "            # multi_cluster_df = pd.DataFrame({'cluster_seq_type':[],\n",
    "            #  'num_spikes':[],\n",
    "            #  'num_neurons':[],\n",
    "            #  'first_spike_time':[],\n",
    "            #  'event_length':[],\n",
    "            #  'last_spike_time':[],\n",
    "            #  'cluster_spike_times':[],\n",
    "            #  'cluster_neurons':[],\n",
    "            #  'spike_plotting_order':[],\n",
    "            #  'coactive_cluster_group':[],\n",
    "            #  'new_cluster_group':[],\n",
    "            #  'cluster_order_first_spike_defined':[],\n",
    "            #  'cluster_order_mean_weighted_spikes_defined':[],\n",
    "            #  'pairs_mean_ordering':[],\n",
    "            #  'catagories_mean_ordering':[],\n",
    "            #  'pairs_fs_ordering':[],\n",
    "            #  'catagories_fs_ordering':[],\n",
    "            #  'real_sequence_order':[]})\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## this code loops across group of replay events (which has been clustered together based on proximity)\n",
    "            ## it then orders the events based on first spike time and mean spike time.\n",
    "            # it creates multi_cluster_df to store this information\n",
    "        \n",
    "            meaned_order = []\n",
    "            fs_order = []\n",
    "            event_times = []\n",
    "            count = 0\n",
    "            for i,group in enumerate(filtered_r_clusters_df.coactive_cluster_group.unique()):\n",
    "                group_mask = filtered_r_clusters_df.coactive_cluster_group == group\n",
    "                current_cluster = filtered_r_clusters_df[group_mask]\n",
    "                if len(current_cluster) > 1:\n",
    "                    means = []\n",
    "                    event_types = []\n",
    "                    fs_orders = []\n",
    "                    for index,events in enumerate(current_cluster.cluster_spike_times):\n",
    "                        event_types += [current_cluster.cluster_seq_type.values[index]]\n",
    "                        # calculate event order based on spike time weighted mean\n",
    "                        means += [np.mean(events)]\n",
    "                        # calculate order based on first spike time:\n",
    "                        fs_orders += [current_cluster.first_spike_time.values[index]]\n",
    "\n",
    "                    # order by mean time:    \n",
    "                    meaned_order += [list(np.array(event_types)[np.argsort(means)])]\n",
    "                    # order by first spike:\n",
    "                    fs_order += [list(np.array(event_types)[np.argsort(fs_orders)])]\n",
    "\n",
    "                    event_times += [fs_orders]\n",
    "\n",
    "                    current_cluster['new_cluster_group'] =  [count]*len(current_cluster)\n",
    "                    current_cluster['cluster_order_first_spike_defined'] =  list(np.argsort(np.argsort(fs_orders)))\n",
    "                    current_cluster['cluster_order_mean_weighted_spikes_defined'] =  list(np.argsort(np.argsort(means)))\n",
    "\n",
    "                    if count == 0:\n",
    "                        multi_cluster_df = current_cluster.copy()\n",
    "                    else:\n",
    "                        # Concatenate the DataFrames vertically (row-wise)\n",
    "                        multi_cluster_df = pd.concat([multi_cluster_df, current_cluster], axis=0)\n",
    "                        # Reset the index if needed\n",
    "                        multi_cluster_df = multi_cluster_df.reset_index(drop=True)\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            # next I catagorise the the groupe and ordered 'clusters' (replays that are proximal to each other)\n",
    "            if len( multi_cluster_df.coactive_cluster_group.unique()) > 1:\n",
    "                real_order = list(np.array(seq_order)+1)\n",
    "                # # mean ordering first (each replay ordered by its average spike time) : \n",
    "                relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,meaned_order)\n",
    "                #convert pairs to object\n",
    "                pairs = np.array(pairs, dtype=object)\n",
    "                summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "                labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.bar(labels,summed_amounts)\n",
    "                ax.set_title('catagory occurances (seqs ordered by mean spike time)')\n",
    "\n",
    "                SaveFig('catagory occurances_1___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                all_pair_outcomes_todf = []\n",
    "                all_pairs_todf = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    all_pairs = []\n",
    "                    all_pair_outcomes = []\n",
    "                    for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                        all_pairs += [pair_]\n",
    "                        all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "                    all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "                    all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "                multi_cluster_df['pairs_mean_ordering'] = all_pairs_todf\n",
    "                multi_cluster_df['catagories_mean_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "                # # first spike ordering second : \n",
    "                relative_amounts,amounts,pair_outcomes,pairs = catagorize_seqs(real_order,num_dominant_seqs,fs_order)                \n",
    "                #convert pairs to object\n",
    "                pairs = np.array(pairs, dtype=object)\n",
    "                summed_amounts = [sum(items) for items in conactinate_nth_items(amounts)]\n",
    "                labels = ['ordered','reverse','repeat','misordered','other_to_task','task_to_other','other']\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.bar(labels,summed_amounts)\n",
    "                ax.set_title('catagory occurances (seqs ordered by first spike times)')\n",
    "\n",
    "                SaveFig('catagory occurances_2___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                all_pair_outcomes_todf = []\n",
    "                all_pairs_todf = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    group_pairs = np.array(pairs)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    group_pair_outcomes = np.array(pair_outcomes)[multi_cluster_df[multi_cluster_df.new_cluster_group == group].index.values]\n",
    "                    all_pairs = []\n",
    "                    all_pair_outcomes = []\n",
    "                    for index,pair_ in enumerate(group_pairs[0:-1]):\n",
    "                        all_pairs += [pair_]\n",
    "                        all_pair_outcomes += [group_pair_outcomes[index]]\n",
    "\n",
    "                    all_pair_outcomes_todf  += [all_pair_outcomes] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "                    all_pairs_todf += [all_pairs] * len(multi_cluster_df[multi_cluster_df.new_cluster_group == group])\n",
    "\n",
    "                multi_cluster_df['pairs_fs_ordering'] = all_pairs_todf\n",
    "                multi_cluster_df['catagories_fs_ordering'] = all_pair_outcomes_todf\n",
    "\n",
    "\n",
    "                multi_cluster_df['real_sequence_order'] = [real_order]*len(multi_cluster_df)\n",
    "\n",
    "                multi_cluster_df.to_csv(chunk_path + 'multi_event_clusters_df.csv', index=False)\n",
    "\n",
    "\n",
    "                ########### CATAGORISATION STUFF ###########################################################################################\n",
    "\n",
    "\n",
    "                ### plot of a comparision of pair ratios during behaviour and during task. firts just plot them next to each other, then somekind of comparisoon? \n",
    "                # save this data out.\n",
    "\n",
    "                #### load in awake task data dn determine awake sequence ordering:\n",
    "\n",
    "                with open(awake_PP_path + awake_file + r'\\analysis_output\\\\' + 'spikes_seq_type_adjusted.pickle', 'rb') as handle:\n",
    "                    unmasked_spikes_df = pickle.load(handle)\n",
    "\n",
    "                with open(awake_PP_path + awake_file + r'\\analysis_output\\reordered_recolored\\\\' + 'neuron_order', 'rb') as handle:\n",
    "                    awake_neuron_order = pickle.load(handle)\n",
    "\n",
    "\n",
    "                seqs = np.unique(unmasked_spikes_df.sequence_type_adjusted)\n",
    "                seq_spikes = []\n",
    "                for seq_type_ in seqs:  \n",
    "                    seq_spikes += [unmasked_spikes_df.timestamp[np.where(unmasked_spikes_df.sequence_type_adjusted ==seq_type_)[0]].values]\n",
    "\n",
    "                # Define the bin size (in this case, 0.2s)\n",
    "                bin_size = 0.2\n",
    "\n",
    "                seq_spike_occurance = []\n",
    "                for spikes_ in seq_spikes:\n",
    "                    # Use the numpy.histogram function to bin the data\n",
    "                    hist, bins = np.histogram(spikes_, bins=np.arange(0, np.diff(behav_time_interval_start)[0], bin_size))\n",
    "                    seq_spike_occurance += [list(hist)]\n",
    "\n",
    "\n",
    "                #### first define when seqs occur and label neurons that contribute to them \n",
    "\n",
    "\n",
    "                seq_size_threshold= 5\n",
    "\n",
    "\n",
    "                all_seq_neurons = []  \n",
    "                df_seq_inds = []\n",
    "                total_seqs_by_type =[]\n",
    "                seq_numbers_passed = []\n",
    "                current_order = []\n",
    "                all_mid_point_times = []\n",
    "                for i in range(1,7):\n",
    "                    print(i)\n",
    "\n",
    "                    seq_spike_count = seq_spike_occurance[i]\n",
    "                    # find seq start and end, defined by whetehr there were spikes or not \n",
    "                    groups = return_inds_for_seq_groups(seq_spike_count)\n",
    "\n",
    "                    ### plot to check that I am accounting ofr sequences properly\n",
    "\n",
    "                    #mask\n",
    "                    # spikemask\n",
    "                    timeframe = [0,800]\n",
    "                    mask = (unmasked_spikes_df.timestamp>timeframe[0])*(unmasked_spikes_df.timestamp<timeframe[-1])\n",
    "                    background_remove_mask = unmasked_spikes_df[mask].sequence_type_adjusted >= 0\n",
    "                    c_ = np.array(colors)[unmasked_spikes_df[mask][background_remove_mask].sequence_type_adjusted.values.astype(int)]\n",
    "\n",
    "                    fig,[ax1,ax2] = plt.subplots(2, 1,figsize=(10, 5))\n",
    "                    ax1.scatter(unmasked_spikes_df[mask][background_remove_mask].timestamp, awake_neuron_order[mask][background_remove_mask],marker = 'o', s=40, linewidth=0,color = c_ ,alpha=1)\n",
    "                    ax2.plot(seq_spike_count, color = colors[i])\n",
    "                    for item in groups:\n",
    "                        ax2.plot(item,[-5,-5], color = 'red')\n",
    "\n",
    "\n",
    "                    ax1.set_xlim([0,100])\n",
    "                    ax2.set_xlim([0,(100/bin_size)])\n",
    "\n",
    "                    seq_neurons = []\n",
    "                    df_index = []\n",
    "                    mid_point_time = []\n",
    "\n",
    "                    counter = 0\n",
    "                    for group in groups:\n",
    "\n",
    "                        # spikemask\n",
    "                        timeframe = [(group[0] * bin_size)-0.5,(group[-1] * bin_size)+0.5]\n",
    "                        mid_point_time += [timeframe[0] + (np.diff(timeframe)[0]/2)]\n",
    "                        mask = (unmasked_spikes_df.timestamp>timeframe[0])*(unmasked_spikes_df.timestamp<timeframe[-1])\n",
    "                        seq_mask = unmasked_spikes_df[mask].sequence_type_adjusted == i\n",
    "\n",
    "                        if len(unmasked_spikes_df[mask][seq_mask]) > seq_size_threshold:\n",
    "                            counter +=1\n",
    "                            seq_neurons.append(list(unmasked_spikes_df[mask][seq_mask].neuron))\n",
    "                            df_index += [list(unmasked_spikes_df[mask][seq_mask].index)]\n",
    "\n",
    "                    total_seqs_by_type += [counter]\n",
    "                    all_seq_neurons.append(seq_neurons)\n",
    "                    df_seq_inds.append(df_index)\n",
    "                    all_mid_point_times += [mid_point_time]\n",
    "\n",
    "                    current_order +=[i]\n",
    "\n",
    "                flat_seqs = []\n",
    "                for index,item in enumerate(all_mid_point_times):\n",
    "                    flat_seqs += [current_order[index]]*len(item)\n",
    "                flat_seq_times = [item for sublist in all_mid_point_times for item in sublist]\n",
    "                awake_seq_order = list(np.array(flat_seqs)[np.argsort(flat_seq_times)])\n",
    "\n",
    "\n",
    "\n",
    "                #### plot event transitions freqs for awake and sleep: \n",
    "\n",
    "\n",
    "                awake_event_transitions = count_event_transitions(awake_seq_order)\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'awake_event_transitions.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(awake_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                # Print the frequency of each event transition\n",
    "                # for transition, count in awake_event_transitions.items():\n",
    "                #     print(f\"Transition {transition}: {count:.2f}% occurrences\")\n",
    "\n",
    "\n",
    "                fig,[ax,ax2] = plt.subplots(1, 2,figsize=(20, 5))\n",
    "                plot_event_transitions(awake_event_transitions,ax,'')\n",
    "                plot_event_transitions(awake_event_transitions,ax2,'')\n",
    "\n",
    "                ### do the same for sleep:\n",
    "\n",
    "                mean_ordering_pairs = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    current_group = multi_cluster_df[multi_cluster_df.new_cluster_group == group]\n",
    "                    mean_ordering_pairs += current_group.pairs_mean_ordering.values[0]\n",
    "\n",
    "                sleep_event_transitions = {key: 0 for key in list(awake_event_transitions.keys())}\n",
    "                for pair in mean_ordering_pairs:\n",
    "                    tup_pair = pair[0],pair[-1]\n",
    "                    if tup_pair in sleep_event_transitions:\n",
    "                        sleep_event_transitions[tup_pair] += 1\n",
    "                    else:\n",
    "                        sleep_event_transitions[tup_pair] = 1\n",
    "\n",
    "                #normalise to percentage\n",
    "                mean_ordered_sleep_event_transitions = normalize_counts_to_percentages(sleep_event_transitions)\n",
    "                plot_event_transitions(mean_ordered_sleep_event_transitions,ax,'- mean spike times ordered')\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'sleep_event_transitions_mean_spike_time_ordered.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(mean_ordered_sleep_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "                fs_ordering_pairs = []\n",
    "                for group in multi_cluster_df.new_cluster_group.unique():\n",
    "                    current_group = multi_cluster_df[multi_cluster_df.new_cluster_group == group]\n",
    "                    fs_ordering_pairs += current_group.pairs_fs_ordering.values[0]\n",
    "\n",
    "                sleep_event_transitions = {key: 0 for key in list(awake_event_transitions.keys())}\n",
    "                for pair in fs_ordering_pairs:\n",
    "                    tup_pair = pair[0],pair[-1]\n",
    "                    if tup_pair in sleep_event_transitions:\n",
    "                        sleep_event_transitions[tup_pair] += 1\n",
    "                    else:\n",
    "                        sleep_event_transitions[tup_pair] = 1\n",
    "\n",
    "                #normalise to percentage\n",
    "                fs_ordered_sleep_event_transitions = normalize_counts_to_percentages(sleep_event_transitions)\n",
    "                plot_event_transitions(fs_ordered_sleep_event_transitions,ax2, '- first spike ordered')\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'sleep_event_transitions_first_spike_time_ordered.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(fs_ordered_sleep_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                ax.text(1,23,'sleep transitions',color = 'orange',size = 10)\n",
    "                ax.text(1,22,'Awake transitions', color = 'blue',size = 10)\n",
    "                ax.text(0,-5,'sequence order =' + ','.join(list((np.array(seq_order)+1).astype(str))) + '   |   num dominant = ' +  str(num_dominant_seqs), color = 'blue',size = 10)\n",
    "\n",
    "                SaveFig('event_transitions_awake_sleep___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "            #             for key in fs_ordered_sleep_event_transitions:\n",
    "            #                 if not key in awake_event_transitions:\n",
    "            #                     awake_event_transitions[key] = 0\n",
    "            #             for key in mean_ordered_sleep_event_transitions:\n",
    "            #                 if not key in awake_event_transitions:\n",
    "            #                     awake_event_transitions[key] = 0\n",
    "\n",
    "                #  plot correlation all\n",
    "                fig,[ax,ax2]= plt.subplots(1, 2,figsize=(10, 5))\n",
    "                sns.regplot(y=list(fs_ordered_sleep_event_transitions.values()), x=list(awake_event_transitions.values()), ax = ax)\n",
    "                sns.regplot(y=list(mean_ordered_sleep_event_transitions.values()), x=list(awake_event_transitions.values()), ax = ax2)\n",
    "                ax.set_title('transition event freqs: awake vs sleep (first spike ordered)', size =7)\n",
    "                ax2.set_title('transition event freqs: awake vs sleep (mean spike time ordered)', size =7)\n",
    "                ax.set_xlabel(' awake transition event freq (%)')\n",
    "                ax.set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                SaveFig('transition_events_awake_sleep_frequencies___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                ### plot correlation for each catagory ####################################################################################\n",
    "\n",
    "                #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "                if not len(real_order) == num_dominant_seqs:\n",
    "                    dominant = real_order[0:num_dominant_seqs]\n",
    "                    other_ = real_order[num_dominant_seqs::]\n",
    "                else:\n",
    "                    dominant = real_order\n",
    "                    other_ = []\n",
    "\n",
    "                outcome= []\n",
    "                pairs = []\n",
    "                for pair in list(awake_event_transitions.keys()):\n",
    "                    pairs += [list(pair)]\n",
    "                    outcome += [logic_machine_for_pair_catagorisation(list(pair),dominant,other_)]\n",
    "\n",
    "                awake_event_freqs_df = pd.DataFrame({'transition': pairs,'catagory':outcome,'awake_event_transitions':list(awake_event_transitions.values()),'mean_spike_ordered_sleep_frequencies':list(mean_ordered_sleep_event_transitions.values()),'fs_ordered_sleep_frequencies':list(fs_ordered_sleep_event_transitions.values())})\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                # Assuming you have a dataframe named 'df' with columns 'categories', 'datax', and 'datay'\n",
    "\n",
    "                # Group the dataframe by 'categories'\n",
    "                grouped_df = awake_event_freqs_df.groupby('catagory')\n",
    "\n",
    "                # Get the number of unique categories\n",
    "                num_categories = len(grouped_df)\n",
    "\n",
    "                # Create subplots\n",
    "                fig, axes = plt.subplots(num_categories, 1, figsize=(5, 5*num_categories))\n",
    "\n",
    "                # Iterate over each category and subplot\n",
    "                for i, (category, group) in enumerate(grouped_df):\n",
    "                    awake_event_freqs = group.awake_event_transitions.values\n",
    "                    mo_sleep_freqs = group.mean_spike_ordered_sleep_frequencies.values\n",
    "                    fs_sleep_freqs = group.fs_ordered_sleep_frequencies.values\n",
    "\n",
    "                    sns.regplot(y=mo_sleep_freqs, x=awake_event_freqs, ax = axes[i])\n",
    "                    sns.regplot(y=fs_sleep_freqs, x=awake_event_freqs, ax = axes[i])\n",
    "                    axes[i].set_title(category + ' transition event freqs awake vs sleep  ' + '---->' + '  blue = mean ordered | orange = first spike ordered', size = 6)\n",
    "\n",
    "                    axes[i].set_xlabel('awake transition event freq (%)')\n",
    "                    axes[i].set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                # Adjust spacing between subplots\n",
    "                plt.tight_layout()\n",
    "\n",
    "                awake_event_freqs_df.to_csv(chunk_path + 'transit_frequencies_awake_sleep_df.csv', index=False) \n",
    "\n",
    "                SaveFig('transition_events_awake_sleep_frequencies_CATAGORIZED___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                #################################### comparision of reward related transitoions during awake \n",
    "\n",
    "                ### load in reward times \n",
    "\n",
    "                behav_data_path = r'Z:\\projects\\sequence_squad\\organised_data\\animals\\\\'\n",
    "                os.listdir(behav_data_path)\n",
    "                mouse_session_recording = mir\n",
    "\n",
    "                ## set dat_path:\n",
    "                for file_ in os.listdir(behav_data_path):\n",
    "                    if mouse_session_recording.split('_')[0] in file_:\n",
    "                        if mouse_session_recording.split('_')[1] == file_[-1]:\n",
    "                            dat_path = os.path.join(behav_data_path,file_)\n",
    "                for recording in os.listdir(os.path.join(behav_data_path,dat_path)):\n",
    "                    if recording.split('_')[0][-1] == mouse_session_recording.split('_')[-1]:\n",
    "                        dat_path = os.path.join(dat_path,recording)\n",
    "\n",
    "                for file in os.listdir(dat_path + r'\\behav_sync\\\\'):\n",
    "                    if 'task' in file:\n",
    "                        behav_sync = pd.read_csv(dat_path + r'\\behav_sync\\\\' + file + '\\Behav_Ephys_Camera_Sync.csv')\n",
    "\n",
    "\n",
    "                reward_times_bpod = behav_sync.Reward_Times.values\n",
    "                mask = np.isnan(reward_times_bpod)\n",
    "                # Use the mask to filter out NaN values\n",
    "                reward_times = behav_sync.PokeIN_EphysTime.values[~mask]\n",
    "\n",
    "                # account for time window:\n",
    "                params_file = awake_PP_path + awake_file + r'\\trainingData\\\\' + 'params_' + mir + '.json'\n",
    "                with open(params_file, 'r') as file:\n",
    "                    params = json.load(file)\n",
    "                time_span = params['time_span'][0]\n",
    "                time_span_mask = (reward_times > time_span[0])*(reward_times < time_span[-1])\n",
    "                reward_times = reward_times[time_span_mask]\n",
    "                # convert to relative time\n",
    "                reward_times = reward_times - params['time_span'][0][0]\n",
    "\n",
    "                # detemrine seqs that occur within 3s of reward\n",
    "                sorted_seq_times = np.array(sorted(flat_seq_times))\n",
    "                awake_seq_order\n",
    "\n",
    "                reward_related_seqs = []\n",
    "                for r_time in reward_times:\n",
    "                    r_time_window_start = r_time - 2 \n",
    "                    window_mask = (sorted_seq_times >= r_time_window_start) * (sorted_seq_times <= r_time)\n",
    "                    reward_related_seqs += [list(np.array(awake_seq_order)[window_mask])]\n",
    "\n",
    "                reward_event_transitions = {key: 0 for key in list(awake_event_transitions.keys())}\n",
    "                for seq in reward_related_seqs:\n",
    "                    for index,element in enumerate(seq[0:-1]):\n",
    "                        pair = [element,seq[index+1]]\n",
    "                        tup_pair = pair[0],pair[-1]\n",
    "                        if tup_pair in sleep_event_transitions:\n",
    "                            reward_event_transitions[tup_pair] += 1\n",
    "                        else:\n",
    "                            reward_event_transitions[tup_pair] = 1\n",
    "\n",
    "                #normalise to percentage\n",
    "                reward_event_transitions = normalize_counts_to_percentages(reward_event_transitions)\n",
    "\n",
    "                fig,[ax,ax2] = plt.subplots(1, 2,figsize=(20, 5))\n",
    "\n",
    "                plot_event_transitions(reward_event_transitions,ax, '- mean spike ordered')\n",
    "                plot_event_transitions(mean_ordered_sleep_event_transitions,ax, '- first spike ordered')\n",
    "\n",
    "                ax.text(1,23,'sleep transitions',color = 'orange',size = 10)\n",
    "                ax.text(1,22,'reward related awake transitions', color = 'blue',size = 10)\n",
    "                ax.text(0,-5,'sequence order =' + ','.join(list((np.array(seq_order)+1).astype(str))) + '   |   num dominant = ' +  str(num_dominant_seqs), color = 'blue',size = 10)\n",
    "\n",
    "                plot_event_transitions(reward_event_transitions,ax2, '- first spike ordered')\n",
    "                plot_event_transitions(fs_ordered_sleep_event_transitions,ax2, '- first spike ordered')\n",
    "\n",
    "                # Specify the file path where you want to save the dictionary\n",
    "                file_path = chunk_path + 'reward_releated_awake_event_transitions.pickle'\n",
    "                with open(file_path, 'wb') as handle:\n",
    "                    pickle.dump(reward_event_transitions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                SaveFig('event_transitions_reward_related_awake_sleep___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "                #  plot correlation all\n",
    "\n",
    "                fig,[ax,ax2]= plt.subplots(1, 2,figsize=(10, 5))\n",
    "                sns.regplot(y=list(fs_ordered_sleep_event_transitions.values()), x=list(reward_event_transitions.values()), ax = ax)\n",
    "                sns.regplot(y=list(mean_ordered_sleep_event_transitions.values()), x=list(reward_event_transitions.values()), ax = ax2)\n",
    "                ax.set_title('transition event freqs: awake vs sleep (first spike ordered)', size =7)\n",
    "                ax2.set_title('transition event freqs: awake vs sleep (mean spike time ordered)', size =7)\n",
    "                ax.set_xlabel(' awake transition event freq (%)')\n",
    "                ax.set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                SaveFig('transition_events_REWARDEDawake_sleep_frequencies____chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                ### plot correlation for each catagory ####################################################################################\n",
    "\n",
    "                #deal wih the fact that the way I order the sequence messes up the order a bit\n",
    "                if not len(real_order) == num_dominant_seqs:\n",
    "                    dominant = real_order[0:num_dominant_seqs]\n",
    "                    other_ = real_order[num_dominant_seqs::]\n",
    "                else:\n",
    "                    dominant = real_order\n",
    "                    other_ = []\n",
    "\n",
    "                outcome= []\n",
    "                pairs = []\n",
    "                for pair in list(reward_event_transitions.keys()):\n",
    "                    pairs += [list(pair)]\n",
    "                    outcome += [logic_machine_for_pair_catagorisation(list(pair),dominant,other_)]\n",
    "\n",
    "                reward_event_freqs_df = pd.DataFrame({'transition': pairs,'catagory':outcome,'reward_event_transitions':list(reward_event_transitions.values()),'mean_spike_ordered_sleep_frequencies':list(mean_ordered_sleep_event_transitions.values()),'fs_ordered_sleep_frequencies':list(fs_ordered_sleep_event_transitions.values())})\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                # Assuming you have a dataframe named 'df' with columns 'categories', 'datax', and 'datay'\n",
    "\n",
    "                # Group the dataframe by 'categories'\n",
    "                grouped_df = reward_event_freqs_df.groupby('catagory')\n",
    "\n",
    "                # Get the number of unique categories\n",
    "                num_categories = len(grouped_df)\n",
    "\n",
    "                # Create subplots\n",
    "                fig, axes = plt.subplots(num_categories, 1, figsize=(5, 5*num_categories))\n",
    "\n",
    "                # Iterate over each category and subplot\n",
    "                for i, (category, group) in enumerate(grouped_df):\n",
    "                    r_event_freqs = group.reward_event_transitions.values\n",
    "                    mo_sleep_freqs = group.mean_spike_ordered_sleep_frequencies.values\n",
    "                    fs_sleep_freqs = group.fs_ordered_sleep_frequencies.values\n",
    "\n",
    "                    sns.regplot(y=mo_sleep_freqs, x=r_event_freqs, ax = axes[i])\n",
    "                    sns.regplot(y=fs_sleep_freqs, x=r_event_freqs, ax = axes[i])\n",
    "                    axes[i].set_title(category + ' transition event freqs awake vs sleep  ' + '---->' + '  blue = mean ordered | orange = first spike ordered', size = 6)\n",
    "\n",
    "                    axes[i].set_xlabel('reward related awake transition event freq (%)')\n",
    "                    axes[i].set_ylabel('sleep transition event freq (%)')\n",
    "\n",
    "                reward_event_freqs_df.to_csv(chunk_path + 'Rewarded_transit_frequencies_awake_sleep_df.csv', index=False) \n",
    "\n",
    "                # Adjust spacing between subplots\n",
    "                plt.tight_layout()\n",
    "\n",
    "                SaveFig('transition_events_REWARDEDawake_sleep_frequencies_CATAGORIZED___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "\n",
    "\n",
    "                ####################################################################################################################################\n",
    "\n",
    "                ## plot and save out proportions for each sequene in awake and sleep and reward awake and sleep\n",
    "\n",
    "\n",
    "                reward_awake_counts = []\n",
    "                all_awake_counts = []\n",
    "                sleep_counts = []\n",
    "                for i in range(1,7):\n",
    "                    flat = [item for sublist in reward_related_seqs for item in sublist]\n",
    "                    reward_awake_counts += [flat.count(i)]\n",
    "                    all_awake_counts += [awake_seq_order.count(i)]\n",
    "                    sleep_counts += [list(filtered_r_clusters_df.cluster_seq_type.values).count(i)]\n",
    "\n",
    "                reward_awake_props = np.array(reward_awake_counts)/sum(reward_awake_counts)*100\n",
    "                awake_props = np.array(all_awake_counts)/sum(all_awake_counts)*100\n",
    "                sleep_props = np.array(sleep_counts)/sum(sleep_counts)*100\n",
    "\n",
    "                reward_diff = reward_awake_props - awake_props\n",
    "\n",
    "                import scipy\n",
    "\n",
    "                fig,ax= plt.subplots(1, 1,figsize=(5, 5))\n",
    "                sns.regplot(y=sleep_props, x=awake_props, ax = ax)\n",
    "                ax.set_xlabel(' awake single event freq (%)')\n",
    "                ax.set_ylabel('sleep single event freq (%)')\n",
    "                ax.axhline(0,0,ls ='--')\n",
    "\n",
    "                sns.regplot(y=sleep_props, x=reward_awake_props, ax = ax)\n",
    "\n",
    "                ax.text(0,60,'compared to all all awake events', color = 'blue',size = 10)\n",
    "                ax.text(0,55,'compared to reward related awake events', color = 'orange',size = 10)\n",
    "\n",
    "                event_freqs_df = pd.DataFrame({'seq': range(1,7),'awake_frequencies':awake_props,'reward_related_awake_frequencies':reward_awake_props,'sleep_frequencies':sleep_props})\n",
    "\n",
    "                event_freqs_df.to_csv(chunk_path + 'single_event_frequencies_awake_sleep_df.csv', index=False) \n",
    "\n",
    "                SaveFig('SINGLE_EVENT_awake_awakereward_sleep_freqs___chunk'+ str(index_+1) + '.png',chunk_path)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4eb782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c36941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60854bc8-71ef-4ff4-b2f8-ce1158bf1bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3581750-61c0-4f75-9c7e-98f89701892f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
